#+TITLE: R Guide

* TODO LEARN
** TODO ask allan
*** how to add x1-50 columns in a regression 
*** the take first date in a sorted list 
proc sort data=mod1y1; by sitecode date dist2mon;
data mod1y2; set mod1y1; by sitecode date dist2mon;
if first.date;
run;

* R basics
** basic commands/glossary
*** Build in discriptive commnds
`max(x)`
 maximum value in x
 `min(x)`
 minimum value in x
 `sum(x`)
 total of all the values in x
 `mean(x)`
 arithmetic average of the values in x
 `sd(x)
`find the SD of x
 `median(x)`
 median value in x
 `IQR(x)`
 Interquartile range of x
 `range(x)`
 vector of min(x) and max(x)
 `var(x)`
 sample variance of x
 `cor(x, y)`
 correlation between the vectors x and y. The default is Pearson's
correlation coefficient.
 `sort(x)`
 a sorted version of x
 `rank(x)`
 vector of the ranks of the values in x
 `order(x)`
 an integer vector containing the permutation to sort x into ascending
order
 `quantile(x)`
 vector containing the minimum, lower quartile, median, upper quartile,
and maximum of x
 `cumusm(x)`
 vector containing the sum of all of the elements up to that point
 `cumprod(x`)
 vector containing the product of all of the elements up to that point
 `cummax(x)`
 vector of non-decreasing numbers which are the cumulative maxima of the
values in x up to that point
 `cummin(x)`
 vector of non-increasing numbers which are the cumulative minima of the
values in x up to that point
 `pmax(x, y, z)`
 vector, of length equal to the longest of x, y or z, containing the
maximum of x, y or z for the ith position in of x, y or z for the ith
position in each
 `pmin(x, y, z)`
 vector, of length equal to the longest of x, y or z, containing the
minimum of x, y or z for the ith position in each
 `colMeans(x)`
 column means of data frame or matrix x
 `colSums(x)`
 column totals of data frame or matrix x
 `rowMeans(x)`
 row means of data frame or matrix x
 `rowSums(x)`
 row totals of data frame or matrix x

*** Assignment
The assignment operator in R is <- or -> The following are equivalent assignments:
#+begin_src R
x <- 1:10
1:10 ->x
#+end_src

The variable x now represents the vector of integers from 1 to 10. Variables in R are case sensitive, so if you assign something
to x you have not assigned it to X.

*** Operators

Operators
 R's binary and logical operators will look very familiar to
programmers. Note that binary operators work on vectors and matrices as
well as scalars.

[[R_files/img/Image_urTv463hx7mSe0JPDeWo5A_0003.jpg]][[R_files/img/Image_urTv463hx7mSe0JPDeWo5A_0004.jpg]]*
*
 *Examples
*
 x <- c(1:10)
 x[(x>8) | (x<5)]
 # yeilds 1 2 3 4 9 10
 # How it works
 x <- c(1:10)
 x
 1 2 3 4 5 6 7 8 9 10
 x > 8
 F F F F F F F F T T
 x < 5
 T T T T F F F F F F
 x > 8 | x < 5
 T T T T F F F F T T
 x[c(T,T,T,T,F,F,F,F,T,T)]
 1 2 3 4 9 10

*** Rows and Columns
in R to specify rows and columns, you use brackets and in them:

the 'first' position is rows
the 'second' position is columns
leaving a position empty specify the whole column/row
you can use all other R operators like 1:3 to 'select multiple' rows/columns
#+begin_src R
mat[4,2]                                   #display the 4th row and the 2nd column
mat[3,]                                    #display the 3rd row
mat[,2]                                    #display the 2nd column
mat[11:12,5:6]                             #display rows 11-12 and columns 5-6
#+end_src

*** The concatenation function: c()
used to concatenate text/numeric values togheter

#+begin_src r
#examples
vector1 <- c(2,4,6,7,11)
sdata <- c('a', 'b', 'c')
#+end_src

It should be noted that whereas cbind() and rbind() are concatenation
functions that respect dim attributes, the basic c() function does
not, but rather clears numeric objects of all dim and dimnames
attributes. This is occasionally useful in its own right.
*** The colon (:) operator
In R, the colon means "to", as in "1 to 10".

#+BEGIN_SRC R
> 1:10
 [1]  1  2  3  4  5  6  7  8  9 10
> 10:1                                 # And backwards too.
 [1] 10  9  8  7  6  5  4  3  2  1
#+END_SRC

*** sequence-using the seq( ) function
There is another way to produce this same sequence, which is by using the seq( ) function...

#+begin_src R
seq(from=1, to=10, by=1)
[1]  1  2  3  4  5  6  7  8  9 10
#+end_src

The syntax should be self-explanatory. The function has created a regular sequence of integers "from" 1 "to" 10 "by" adding 1 at each step. The seq( ) function is more flexible than the colon operator because the function can be made to step by any amount you want, whereas the colon operator can only step by one...

*** length
length is a numeric value that tells how many elements are in an R object.

For a 'vector', length is the number of rows
while for a 'matrix'-the length is the product of rows and columns

*** dim (dimension)
the command dim (dimension) shows you how "long" (how many cases) are there in the dataset and how many variables

#+BEGIN_SRC R
dim (DATA)
#+END_SRC

*** Head/Tail
show the first/last parts of a vector/matrix
%Tip one can use the n=X option to specify the amount shows%

#+begin_src R
head(VECTOR)
tail(VECTOR,n=10)
#+end_src

*** Bracket notation (manipulate/Subset data in R)
To manipulate data frames in R we can use the bracket notation to access the indices for the observations and the variables
it is easiest to think of the data frame as a rectangle of data where the rows are the observations and the columns are the variables
you can use a vector to subset another vector with the [] option

#+begin_src R
#example 1
aa<-c(1,2,3,4,5,6)
bb<-c(2,3)
cc<-aa[bb]

#example 2
hsb3 <- hsb2.small[, c(1, 7, 8)]
#+end_src
more info: http://www.ats.ucla.edu/stat/r/modules/subsetting.htm

*** Character variables
character vector/values i R uses quotes

#+begin_src R
ex1 <-  c("first","second")
#+end_src

one can also create Boolean vectors such as TRUE/FALSE

#+begin_src R
vec1<-c(TRUE,FALSE,TRUE,TRUE)
#+end_src

*** Vector
vector produces a vector of the given length and mode.
is.vector returns TRUE if x is a vector of the specified mode having no attributes other than names. It returns FALSE otherwise.

*** Matrix
matrix creates a matrix from the given set of values.
as.matrix attempts to turn its argument into a matrix.
is.matrix tests if its argument is a (strict) matrix.

#+begin_src R
mat1<-matrix (VECTOR,nrow=2,ncol=3)
mat2<-matrix (11:18,2,3)
#+end_src

*** attach/detach
@Warning: there is never an actual need to use attach(), using it can lead to confusion or errors@
The attach() function in R can be used to make objects within dataframes accessible in R with fewer keystrokes. As an example:

#+begin_src R
ds = read.csv("http://www.math.smith.edu/r/data/help.csv")
names(ds)
attach(ds)
mean(cesd)
[1] 32.84768
detach(ds)
#+end_src

The search() function can be used to list attached objects and packages. Let's see what is there

#+begin_src R
search()
> search()
 [1] ".GlobalEnv"        "ds"                "tools:RGUI"        "package:stats"
 [5] "package:graphics"  "package:grDevices" "package:utils"     "package:datasets"
 [9] "package:methods"   "Autoloads"         "package:base"

#+end_src

*** info on working dir/objects
#+begin_src R
getwd() # print the current working directory - cwd
ls() # list the objects in the current workspace
setwd(mydirectory) # change to mydirectory
#+end_src

*** names
'names' are Functions to get or set the names of an object.

to show all variables in dataset issue:
#+begin_src R
names(data1)
# remove the names attribute
names(islands) <- NULL
## assign just one name
names(z)[2] <- "b"
#+end_src

%Tip- one can see data structure of a model%
#+begin_src R
names(summary(mod1))
#+end_src

*** Str
Compactly display the internal structure of an R object, a diagnostic
function and an alternative to summary.

#+begin_src R
str(OBJECT)
#+end_src

*** %in%
I found out about the %in% operator too late. It matches all 'i' elements of the `x` vector against the `y` vector, and returns 'TRUE' if `x[i]` is also a member of `y`.

#+begin_src R
3 %in% 1:10
1:10 %in% 3
-3:3 %in% 1:10

(-3:3)[-3:3 %in% 1:10]
(-3:3)[!(-3:3 %in% 1:10)]
#+end_src

*** Table command
To show a specific table of a variable issue at the CLI:
#+begin_src R
table(DATA$Variable )
#+end_src

*** workspace
The workspace is your current R working environment and includes any
user-defined objects (vectors, matrices, data frames, lists, functions)
and all loaded datasets but not the loaded packages (they have to be
loaded every time you start a new R session).
At the end of an R session, you will be asked if you want to save an image of your current
workspace (default directory: MyDocuments). The next time you start R, the saved workspace is automatically reloaded. If instead you want to
keep different workspaces for different projects you can save your workspace anytime with the function

#+begin_src R
save.image("mydirectory")
#+end_src

Workspace files have the extension .RData.
The function to load them is
#+begin_src R
load("mydirectory").
load("myfile.RData")
#+end_src

*** view data in a GUI table
show a table for the data
#+begin_src R
View(DATA)
#+end_src

*** Remove vectors/datasets
To remove a dataset you can type a simple command:
#+begin_src R
rm(DATASET)
#+end_src

*** dummy variables
you can use the 'as.factor' option to insert a variable as a dummy
variable:
#+begin_src R
lm(formula = y ~ x1 + factor(country)
#+end_src
** info on current R session, versions, packages loaded etc
*** basic info
use 
#+begin_src R
sessionInfo()
#+end_src

** R options/settings
*** define how many digits to present in the output
use this command in the console or script

#+BEGIN_SRC r
options(digits=3)
#+END_SRC

where '3' is the number of digits after the dot.

*** .Rprofile
**** location of .rprofile file
on linux
file:/home/zeltak/.Rprofile
**** example R intro file

see code (and explenation below code):
#+BEGIN_EXAMPLE
     local({r <- getOption("repos")
      r["CRAN"] <- "http://cran.revolutionanalytics.com"
      options(repos=r)})
 
options(stringsAsFactors=FALSE)
 
options(max.print=100)
 
options(scipen=10)
 
options(editor="vim")
 
# options(show.signif.stars=FALSE)
 
options(menu.graphics=FALSE)
 
options(prompt="> ")
options(continue="... ")
 
options(width = 80)
 
q <- function (save="no", ...) {
  quit(save=save, ...)
}
 
utils::rc.settings(ipck=TRUE)
 
.First <- function(){
  if(interactive()){
    library(utils)
    timestamp(,prefix=paste("##------ [",getwd(),"] ",sep=""))
 
  }
}
 
.Last <- function(){
  if(interactive()){
    hist_file <- Sys.getenv("R_HISTFILE")
    if(hist_file=="") hist_file <- "~/.RHistory"
    savehistory(hist_file)
  }
}
 
if(Sys.getenv("TERM") == "xterm-256color")
  library("colorout")
 
sshhh <- function(a.package){
  suppressWarnings(suppressPackageStartupMessages(
    library(a.package, character.only=TRUE)))
}
 
auto.loads <-c("dplyr", "ggplot2")
 
if(interactive()){
  invisible(sapply(auto.loads, sshhh))
}
 
.env <- new.env()
 
 
.env$unrowname <- function(x) {
  rownames(x) <- NULL
  x
}
 
.env$unfactor <- function(df){
  id <- sapply(df, is.factor)
  df[id] <- lapply(df[id], as.character)
  df
}
 
attach(.env)
 
message("\n*** Successfully loaded .Rprofile ***\n")
#+END_EXAMPLE

[Lines 1-3]: First, because I don't have a site-wide R configuration script, I set my local CRAN mirror here. My particular choice of mirror is largely arbitrary.

[Line 5]: If stringsAsFactors hasn't bitten you yet, it will.

[Line 9]: Setting 'scipen=10' effectively forces R to never use scientific notation to express very small or large numbers.

[Line 13]: I included the snippet to turn off significance stars because it is a popular choice, but I have it commented out because ever since 1st grade I've used number of stars as a proxy for my worth as a person.

[Line 15]: I don't have time for Tk to load; I'd prefer to use the console, if possible.

[Lines 17-18]: Often, when working in the interactive console I forget a closing brace or paren. When I start a new line, R changes the prompt to "+" to indicate that it is expecting a continuation. Because "+" and ">" are the same width, though, I often don't notice and really screw things up. These two lines make the R REPL mimic the Python REPL by changing the continuation prompt to the wider "...".

[Lines 22-24]: Change the default behavior of "q()" to quit immediately and not save workspace.

[Line 26]: This snippet allows you to tab-complete package names for use in "library()" or "require()" calls. Credit for this one goes to @mikelove.

[Lines 28-34]: There are three main reasons I like to have R save every command I run in the console into a history file.

    Occasionally I come up with a clever way to solve a problem in an interactive session that I may want to remember for later use; instead of it getting lost in the ether, if I save it to a history file, I can look it up later.
    Sometimes I need an alibi for what I was doing at a particular time
    I ran a bunch of commands in the console to perform an analysis not realizing that I would have to repeat this analysis later. I can retrieve the commands from a history file and put it into a script where it belongs.

These lines instruct R to, before anything else, echo a timestamp to the console and to my R history file. The timestamp greatly improves my ability to search through my history for relevant commands.

[Lines 36-42]: These lines instruct R, right before exiting, to write all commands I used in that session to my R command history file. I usually have this set in an environment variable called "R_HISTFILE" on most of my systems, but in case I don't have this defined, I write it to a file in my home directory called .Rhistory.

[Line 44]: Enables the colorized output from R (provided by the colorout package) on appropriate consoles.

[Lines 47-50]: This defines a function that loads a libary into the namespace without any warning or startup messages clobbering my console.

[Line 52]: I often want to autoload the 'dplyr' and 'ggplot2' packages (particularly 'dplyr' as it is now an integral part of my R experience).

[Lines 54-56]: This loads the packages in my "auto.loads" vector if the R session is interactive.

[Lines 58-59]: This creates a new hidden namespace that we can store some functions in. We need to do this in order for these functions to survive a call to "rm(list=ls())" which will remove everything in the current namespace. This is described wonderfully in this blog post.

[Lines 61-64]: This defines a simple function to remove any row names a data.frame might have. This was stolen from Stephen Turner (which was in turn stolen from plyr).

[Lines 66-70]: This defines a function to sanely undo a "factor()" call. This was stolen from Dason Kurkiewicz.


**** using Aliases
I hate to type the full words 'head', 'summary', 'names' every time, so I use aliases.

You can put aliases into your .Rprofile file, but you have to use the full path to the function (e.g. utils::head) otherwise it won't work.

#+BEGIN_EXAMPLE
# aliases
s <- base::summary
h <- utils::head
n <- base::names
#+END_EXAMPLE
*** show numerical format and not scientific notation
use the scipen in R options . it is an integer- A penalty to be applied when deciding to print numeric values in fixed or exponential notation. Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than scipen digits wider.
So if you type:

#+BEGIN_SRC R
options(scipen = 99)
#+END_SRC

In your .Rprofile or at the top of every script you will never get scientific notation.

** Help/fixes
** can get rid of a workspace after restart

if you can get rid of a workspace after restart
 make sure the current workspace (usually /home/user/) dosent have any r
files around or change the workspace manually

** Errors
*** convergence error code = 1
this error when using lme:

#+BEGIN_SRC sh
nlminb problem, convergence error code = 1
message = iteration limit reached without convergence (10)
#+END_SRC
means that the random slope dosnet have significant variance between values.
to solve this either make sure there are more values or consider running the model without mixed model components
₆In example₆ 
i had an issue where using random intercept date and nested regions inside gave me this error. increasing the number of regions solved this issue 

*** Error in na.fail.default

when you get something like this:
 Error in na.fail.default(list(AOD = c(0.092, 0.081, 0.086, 0.085, 0.09,
:
  missing values in object
 Calls: predict ... model.frame.default -> <Anonymous> ->
na.fail.default
 it means that one of the variables used in the model has NA (missing
cases in R).
 you can see that by runing the summary () comand on your data:
 [[R_files/img/Image_PoMzddNlL6CoZV0zHjKh9A_0002.jpg]]
 if any missing cases are present it will show a NA info where the red
box is
 to fix this delete all NA and rerun model
 (id did it with a hack in SAS if contour <0000000000001 then
delete;)...maybe better to look for a way inside R.

*** Error in rep.int(c(1, numeric(n)), n - 1L)

#Error in rep.int(c(1, numeric(n)), n - 1L) :
 # negative length vectors are not allowed
 #Calls: lme ... contrasts -> ctrfn -> .Diag -> array -> as.vector ->
rep.int
 this means that the "list" option in lme should be removed since there
is no random slope anymore (due to [[javascript:;][previous convergance
error]])
 the solution is to re-write the lme formula:
 library(nlme)
 smooth\_A2007\_yearly = lme(predicted ~ MPM ,
    random = ~1|UID2,
 data= GAM\_A2007 )

*** Error in smooth.construct.tp.smooth.spec

#Error in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots) :
 # A term has fewer unique covariate combinations than specified maximum
degrees of freedom
 #Calls: gam ... smooth.construct -> smooth.construct.tp.smooth.spec ->
.C
 This means a problem with the degress of fredoom in the smoothing. the
solution is to add a option (k=XX) to lower the degress of freedom in
the problematic smooth stage:
 fit2\_1 = gam(resid ~ s(x,y,k=9),
 data= A2005\_bimon1 )

*** replacement has 47633 rows, data has 47640 (in AOD models)

Error in `$<-.data.frame`(`*tmp*`, "newpred", value =
c(8.55884623164173, :
   replacement has 47633 rows, data has 47640
 Calls: $<- -> $<-.data.frame
 solution use first the dim command to find out the length of both the
original dataset and own to attach.
 the use the table command for the model variables to see which one has
a problem:
 for example:

------------------------------------------------------------------------------------------------------------------
 table(GAM\_A2008$bimon)
                1 2 3 4
              424 7980 11757 13363
 4.00000000000001 5 6
                7 12633 1476

------------------------------------------------------------------------------------------------------------------
 This shows the in the bimon variable (goes from 1-6) there are 7
variables that got a 4.000001 value. these have to be droped or recoded.

*** nlminb problem, convergence error code = 1, message = false
convergence (8)

This is a common problem, for which solutions are poorly documented.
 I have found useful is to use a different optimizing algorithm. You can
do this by:

control=lmeControl(opt = "optim")

 IE
 #rerun the lme on the predictions including the spatial spline (smooth)
 Final\_pred\_2001 = lme(newpred ~ mpm ,
    random = list(guid= ~1 + mpm ),control=lmeControl(opt = "optim")
,data= GAM\_T2001 )
 Other options:
            1. Have you tried fitting simpler models, in the hopes of
 getting something that converges without complaint, then use 'update'
to
 try more complicated models? It sometimes works, though often not.
            2. Also, have you tried something like 'lme(...,
 control=lmeControl(returnObject=TRUE))'
            3. Finally, have you tried something like 'lme(...,
 control=lmeControl(singular.ok=TRUE))' OR 'lme(...,
 control=lmeControl(singular.ok=TRUE, returnObject=TRUE))'? I'm not
 sure, but I believe this may work when "returnObject=TRUE" does not.
*** if you have dates issues under linux!!
check your locale conf and make sure its not Hebrew/other but only English!!
** Help in R

Once R is installed, there is a comprehensive built-in help system. At
the program's command prompt you can use any of the following:
 help.start() # general help
 help(foo) # help about function foo
 ?foo # same thing
 apropos("foo") # list all functions containing string foo
 example(foo) # show an example of function foo
 # search for foo in help manuals and archived mailing lists
 RSiteSearch("foo")
 # get vignettes on using installed packages
 vignette() # show available vingettes
 vignette("foo") # show specific vignette

** R support online

to post to the R mailing list (stat.ethz.ch) email this address:
 r-help@r-project.org
 also one can try #R on freenode

** Packages
*** Update R version and all packages

1) If you have a customized Rprofile.site file , save a copy outside of R.
2) Launch your current version of R and issue the following statements
oldip <- installed.packages()[,1]
save(oldip, file="path/installedPackages.Rdata")
where path is a directory outside of R.
3) Download and install the newer version of R.
4) If you saved a customized version of the Rprofile.site file in step
1, copy it into the new installation.
5) Launch the new version of R, and issue the following statements
load("path/installedPackages.Rdata")
newip <- installed.packages()[,1]
for(i in setdiff(oldip, newip))
install.packages(i)
where path is the location specified in step 2.
6 Delete the old installation (optional).
*** general

Packages
 Packages are collections of R functions, data, and compiled code in a
well-defined format. The directory where packages are stored is called
the library.
  R comes with a standard set of packages. Others are available for
download and installation. Once installed, they have to be loaded into
the session to be used.
 .libPaths() # get library location
 library() # see all packages installed
 search() # see packages currently loaded
 *Adding Packages*
 You can expand the types of analyses you do be adding other packages. A
complete list of contributed packages is available from CRAN.
 *Follow these steps:*
    1. Download and install a package (you only need to do this once).
    2. To use the package, invoke the library(package) command to load
it into the current session. (You need to do this once in each session,
unless you customize your environment to automatically load it each
time.)
 *On MS Windows:*
    1. Choose Install Packages from the Packages menu.
    2. Select a CRAN Mirror. (e.g. Norway)
    3. Select a package. (e.g. boot)
    4. Then use the library(package) function to load it for use. (e.g.
library(boot))
 *On Linux:*
    1. Download the package of interest as a compressed file.
    2. At the command prompt, install it using
       R CMD INSTALL [options] [l-lib] pkgs
    3. Use the library(package) function within R to load it for use in
the session.

*** load packages manually into rkward

To load packages into Rkward:
 under windows:
 1)manually download the binary packages in zip format
 2)extract either to a user specifed location (u can define this in
Rkwrad) or extract to:
  c:\Documents and Settings\EKLOOG\My Documents\RKWard\R\library\

*** manually install R packages

use the *install.packages*option and path to the file
 Example:
 install.packages("c:/Users/ekloog/Downloads/MODIS\_0.3-19.zip")

*** install from source

in linux:
 clone git repo of

https://github.com/chaitanyav/weatherdata

 when outside the main folder issue:

R CMD build weatherdata

 To check the package

R CMD check weatherdata\_1.0.tar.gz

 make sure at this point that the XML package is installed as its a
dependency
 To install the package

R CMD install weatherdata\_1.0.tar.gz
*** install package from specific repo
#+begin_src R
install.packages("Matrix", repos = "http://cran.rstudio.com/", type="source")
#+end_src
*** install and load multiple R packages 

#+begin_src R
# ipak function: install and load multiple R packages.
# check to see if packages are installed. Install them if they are not, then load them into the R session.
 
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]

if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)

sapply(pkg, require, character.only = TRUE)
}
 
# usage
packages <- c("ggplot2", "plyr","lme4", "FNN","data.table","reshape","reshape2","sqldf","sp","gtools","doBy","Hmisc","gdata","car")

ipak(packages)
#+end_src
*** Update packages
To update all your installed packages to the latest versions available:

#+BEGIN_SRC R
update.packages()
#+END_SRC

*** Define library path
I generally try to keep all of my packages in one library, but if you want to add a library why not append the new library to the existing library:

#+BEGIN_SRC R
.libPaths( c( .libPaths(), "~/Downloads") )
#+END_SRC

Then I get:
#+BEGIN_EXAMPLE
> .libPaths()
[1] "/Library/Frameworks/R.framework/Versions/2.15/Resources/library"
[2] "/Users/davidwinsemius/Downloads"  
#+END_EXAMPLE

** extract info
*** from summary
In general, use:
 *names(OBJECT)*
 and
 *names(summary(OBJECT))*
 to see what objects are contained in those lists. Then you can explore
those objects and you will often find what you need.
 IE:
 Rcvd <- (glmmPQL(count ~
ns(date,df=35)+pmnew\_l0+pmnewmayear+temp\_f\_l0+med\_inc+Avg\_per\_mi+Avg\_pctnoh+Avg\_p\_A65,
random = ~ 1 |guid, family = poisson, data = cvd))
 names(summary(Rcvd))
 summary(Rcvd)$corFixed

** R from Console
*** launching a script
you may either use:
#+BEGIN_EXAMPLE
source("/home/jim/psych/adoldrug/partyuse1.R")
#+END_EXAMPLE
from within R or
on the system command line
#+BEGIN_EXAMPLE
R CMD BATCH /home/jim/psych/adoldrug/partyuse1.R
#+END_EXAMPLE
you could use the [[#j5SxSQ8Hyr3IhW0ZV92f7Q][sink]] function to store
the output

*** launching a script with console output
you may either use:
#+begin_src R
source('myscript.R', echo = TRUE) 
#+end_src
%Tip you could also use the [[#j5SxSQ8Hyr3IhW0ZV92f7Q][sink]] function to store the output%
*** Clear R console
press ?C-l?
*** Kill R
*** interrupt a running code in R with a keyboard command
use ?C-c?
Although depending on what the process is doing it might not take right away.

If youre on a unix based system, one thing I do is ?C-z? to go back to the command line prompt and then issue a 'kill' to the process ID.

*** batch using rscript
you can run the script itself as an executable by having the first line start with a shebang, which is #!, followed by the path to the place where Rscript is installed. ₆In example₆ 

#+BEGIN_SRC sh
#!/usr/bin/Rscript
source("/media/NAS/Uni/org/files/Uni/Projects/code/P046.Israel_MAIAC/CNNEW/CS03_CreateDB_2006.r")
#+END_SRC
$Note-The path to Rscript listed in your shebang line needs to be in your PATH variable,$

 
** head command
*** use head with a subset of data  

#+BEGIN_SRC R
head(closestaodse[!is.na(closestmean),])
head(closestaodse[closestnobs > 1,])
#+END_SRC

** Memory
*** function to check all object sizes
use this function:


#+BEGIN_SRC sh
.ls.objects <- function (pos = 1, pattern, order.by,
                         decreasing=FALSE, head=FALSE, n=5) {
  napply <- function(names, fn) sapply(names, function(x)
    fn(get(x, pos = pos)))
  names <- ls(pos = pos, pattern = pattern)
  obj.class <- napply(names, function(x) as.character(class(x))[1])
  obj.mode <- napply(names, mode)
  obj.type <- ifelse(is.na(obj.class), obj.mode, obj.class)
  obj.prettysize <- napply(names, function(x) {
    capture.output(print(object.size(x), units = "auto")) })
  obj.size <- napply(names, object.size)
  obj.dim <- t(napply(names, function(x)
    as.numeric(dim(x))[1:2]))
  vec <- is.na(obj.dim)[, 1] & (obj.type != "function")
  obj.dim[vec, 1] <- napply(names, length)[vec]
  out <- data.frame(obj.type, obj.size, obj.prettysize, obj.dim)
  names(out) <- c("Type", "Size", "PrettySize", "Rows", "Columns")
  if (!missing(order.by))
    out <- out[order(out[[order.by]], decreasing=decreasing), ]
  if (head)
    out <- head(out, n)
  out
}
# shorthand
lsos <- function(..., n=10) {
  .ls.objects(..., order.by="Size", decreasing=TRUE, head=TRUE, n=n)
}

#run the function
lsos()

#+END_SRC
*** Check current memory (gc)
use the garbage collection option
#+begin_src R
gc()
#to get more details
gc(verbose=T)
#+end_src

** notifications
*** make R beep/play a sound at the end of a script
use the alarm function- alarm()
'OR' better yet if using linux:
try combining system with some auditive/visual notification, e.g.

#+BEGIN_SRC sh
system("aplay -t wav /usr/share/sounds/phone.wav") # for auditive bell (an I mean it literary)
system("zenity --title=\"R script info\" --text=\"Script has finished with zero exit status\" --info") # for GTK dialog
#+END_SRC

You can check zenity manual if you prefer alert in, say, notification area... But, with system function, you can do pretty much anything: send an email, run some other script, reboot the machine, sudo rm -rf *.*, etc. anything... and I mean it.

But this stands only IF you're running GNU/Linux (or UNIX) distribution, otherwise, stick to Windows specific commands, though in that case, I can't give you much info...
** History
*** show history of commands window
#+begin_src R
 history(Inf)
#+end_src
** NA (Missing data)
*** Types of NA's in R
#+begin_src R
NAs <- list(NA, NA_integer_, NA_real_, NA_character_, NA_complex_)
data.frame(contantName = sapply(NAs, deparse), 
           class       = sapply(NAs, class),
           type        = sapply(NAs, typeof))

#     contantName     class      type
# 1            NA   logical   logical
# 2   NA_integer_   integer   integer
# 3      NA_real_   numeric    double
# 4 NA_character_ character character
# 5   NA_complex_   complex   complex
#+end_src

* Data Input/output
** Import
*** import csv files

To read a CSV (comma seperated) file, with variable names at the top we
use the command read.csv:
 data1 <- read.csv("/PATH/TO/FILE/FILE.csv", header=T)
 data1-new R object created
 read.csv-R command
 example:
 GAM\_T2001 = read.csv("/home/zeltak/2)PostDoc/Database/AOD\_NE/R/lib/to
SAS/$aodmc\_final.csv", header=T)
 *options:*
 sep=","
 row.names="id"
 importing only subsets:
 #imports  rows 20000001:40000000
 data1 <-  read.csv("/PATH/TO/FILE/FILE.csv", header=T,
nrow=20000000,skip=20000000)                   

*** import from dbf

To read a dbf file, with variable names at the top we use the command
read.dbf. You need to load the library "foreign" to use it.

-----------------------------------------------------------------------------------------------------------------------
 library(foreign)
 data1 <- read.dbf("/PATH/TO/FILE/FILE.dbf")

-----------------------------------------------------------------------------------------------------------------------
 library(foreign) - name of library
 data1-new R object created
 read.dbf-R command
 data1 <-
read.dbf("/home/zeltak/2)PostDoc/Database/AOD\_NE/spss/lib/from
SAS/samp\_f\_a2003.dbf")

*** Import from Excel
The best way to read an Excel file is to export it to a comma delimited file but if you really need to use this:

#+begin_src R
#import XLS

require(XLConnect) # library for importing from excel
importdat <- readWorksheet(loadWorkbook(paste(path.import, filenames[i], sep = "")), sheet = 1)

#+end_src

*** import from Spss

For an SPSS file, you need to have *a package loaded called 'foreign'*.
 The you just type:
 data1 <- read.spss("data6\_1a.sav")
 data1-new R object created
 read.spss-R command
 Method 2: using the Hmisc package
 1) save SPSS dataset in trasport format:
 *in spss syntax:*
 get file='c:\mydata.sav'.
 export outfile='c:\mydata.por'.
 *in R
*
 library(Hmisc)
 mydata <- spss.get("c:/mydata.por", use.value.labels=TRUE)
 # last option converts value labels to R factors

*** import tab delimted

To read a tab delimited file, with variable names at the top, we'll
assume that your data look like:
 HASSLES ANX
 38 10
 10 12
 60 21
 90 16
 88 27
 96 30
 1 9
 41 7
 86 32
 59 11
 ...
 We use the command read.table, and type:
 data1 <- read.table("data6\_1a.dat", header = TRUE)
 If we go through that a bit at a time, we'll see what it means.
 data1 is the name of the data object that we are creating in R.
Effectively, R can have lots of datasets that it is working with at the
same time, and it needs to know what to call each one. Because we aren't
very imaginative, we are going to call it data1.
 <- is what we would call the assignment operator, if we wanted to use
computery talk It's like using the = sign. (Except that the problem with
the equals sign is that it is ambiguous. If we write x = 4 does that
mean "put the value 4 into the box labelled x", or does it mean "is x
equal to 4"? In R, for the first, we would write x <- 4, and for the
second, x == 4).
 Next we have our file name, in speech marks. We changed the directory
that R would look in, so that we didn't have to type in the whole path
(e.g. "c:\my documents\statistics\examples\R\data6\_1a.dat").
 Finally, we put header = TRUE, which means that it is true that the
variable names are included in the header at the top of the file.

*** importing from SAS

#+BEGIN_SRC R
#in SAS
# save SAS dataset in trasport format
libname out xport 'c:/mydata.xpt';
data out.mydata;
set sasuser.mydata;
run;

# in R
library(Hmisc)
mydata <- sasxport.get("c:/mydata.xpt")
# character variables are converted to R factors 
#+END_SRC

*** import from text delimted

You can import data from delimited text files using read.table() , a
function that reads a file in table format and saves it as a data frame.
 mydataframe <- read.table(file, header=logical\_value, sep="delimiter",
row.names="name")
 where file is a delimited ASCII file , header is a logical value
indicating whether the first row contains variable names (TRUE or
FALSE), sep specifies the delimiter separating data values, and
row.names is an optional parameter specifying one or more variables to
represent row identifiers.
 For example, the statement
 grades <- read.table("studentgrades.csv", header=TRUE, sep=",",
row.names="STUDENTID")
 reads a comma-delimited file named studentgrades.csv from the current
working directory, gets the variable names from the first line of the
file, specifies the variable STUDENTID as the row identifier, and saves
the results as a data frame named grades. Note that the sep parameter
allows you to import files that use a symbol other than a comma to
delimit the data values. You could read tab-delimited files with
sep="\t". The default is sep="", which denotes one or more spaces, tabs,
new lines, or carriage returns. By default, character variables are
converted to factors. This behavior may not always be desirable (for
example, a variable containing respondents' comments). You can suppress
this behavior in a number of ways. Including the option stringsAs
Factors=FALSE will turn this behavior off for all character variables.
Alternatively, you can use the colClasses option to specify a class (for
example, logical, numeric, character, factor) for each column. The
read.table() function has many additional options for fine-tuning the
data import. See help(read.table) for details.

*** Import from txt

use:
 DATA <- read.table("/PATH/TO/DATA.txt", header=FALSE)
 IE:
 grades <-
read.table("c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.4.Resources/DATA/hpbl/hpbl/hpbl\_2000.txt",
header=FALSE)
  
 When using the read.table() family of functions to input external data
into data frames, specify the colClasses and nrows options explicitly,
set comment. char = "", and specify "NULL" for columns that aren't
needed. This will decrease memory usage and speed up processing
considerably. When reading external data into a matrix, use the scan()
function instead.

*** import binary files

[[R_files/attach/extract.binary.r][Attachment #02 (extract.binary.r)]]
 The code needed to read binary data into R is relatively easy. However,
reading the data in correctly requires that you are either already
familiar with your data or possess a comprehensive description of the
data structure.
 In the binary data file, information is stored in groups of binary
digits. Each binary digit is a zero or one and eight binary digits
grouped together is a byte. In order to successfully read binary data,
you must know how pieces of information have been parsed into binary.
For example, if your data consists of integers, how may bytes should you
interpret as representative of one integer in your data? Or if your data
contains both positive and negative numbers, how can you distinguish the
two? How many pieces of information do you expect to find in the binary
data?
 Ideally, you know the answers to these questions before starting to
read in the binary file. If you do not, you can explore the read in
options in R. To get started, we establish a connection to a file and
indicate that we will be using the connection to read in binary data. We
do this with the file command, providing first the pathname, and the
"rb" for "reading binary". For more details, see help(file) in R.
 to.read =
file("[[http://www.ats.ucla.edu/stat/r/faq/bintest.dat][http://www.ats.ucla.edu/stat/r/faq/bintest.dat]]","rb")
 Next, we use the readBin command to begin. If we think the file
contains integers, we can start by reading in the first integer and
hoping that the size of the integer does not require further
specifications. Different platforms store binary data in different ways,
and which end of a string of binary values represents the greatest
values or smallest values is a difference that can yield very different
results from the same set of binary values. This characteristic is
called the "endian". The binary files in the examples on this page were
written using a PC, which suggests they are little-endian. When reading
in binary data that may or may not have been written on a different
platform, indicating an endian can be crucial. For example, without
adding endian = "little" to the command below while running R on a Mac,
the command reads the first integer as 16777216.
 Pasted from
<[[http://www.ats.ucla.edu/stat/r/faq/read_binary.htm][http://www.ats.ucla.edu/stat/r/faq/read\_binary.htm]]>
 *In example:* read Modis lat/long binary files (32 bit float) *(see
attachment)*
 #These files are plain binary files, one file for each tile which
include lon and lat in 32-bits float numbers.
 #The dimension is 600x600km2.
 library(Hmisc)
 file2read <-
file("c:/Users/ekloog/Documents/tmp/miac/MAIACAOT.h00v00.20021861825.hdf.latlon",
"rb")
 bin = readBin(file2read, double(), n = (600\^2) * 2, size = 4)
 head(bin)
 # from Hmisc
 describe(bin)
 x <- matrix(bin, ncol = 600, nrow = 600, byrow = F)
 y <- matrix(bin[-c(1:(600\^2))], ncol = 600, nrow = 600, byrow = F)
 library(reshape)
 library(ggplot2)
 #pull out the first row
 new <- data.frame(x = x[1,], y = y[1,])
 head(new)
 ###################
 # HDF
 gdalDrivers()

*** Import with Fread (data.table package)

#+begin_src r
library(data.table)
T2003<- fread("f:/Uni/Projects/AN004_mod2pred/T2003_m2_pred_mpm.csv")
#+end_src

** Export

*** Export csv

to export to CSV:
 write.csv(OBJECT NAME, "/PATH/TO/FILE/FILE.csv")
 write.csv(aodmc\_2001,
"/home/zeltak/2)PostDoc/Database/AOD\_NE/R/lib/to
SAS/aodmcF\_T2001\_aodmc.csv")

*** export dbf

To write (export) a dbf file you need to load the library "foreign" to
use it.

------------------------------------------------------------------------------------------------------------------
 library(foreign)
 write.dbf(OBJECT,"/PATH/TO/FILE/FILE.dbf")

------------------------------------------------------------------------------------------------------------------
 library(foreign)
 write.dbf (F\_T2000\_All,
"/home/zeltak/2)PostDoc/Database/AOD\_NE/R/lib/import\_sas/AOD\_cleaned/final.t2000all.dbf")

*** export Excel

The *write.xlsx()*function in the xlsx package can be used to save an R
data frame to an Excel 2007 workbook. The format is:
 *library(xlsx)
 write.xlsx(x, outfile, col.Names=TRUE, row.names=TRUE,
            sheetName="Sheet 1", append=FALSE)
*
 For example, the statements
 library(xlsx)
 write.xlsx(mydata, "mydata.xlsx")
 export the data frame *mydata* to a worksheet (Sheet 1 by default) in
an Excel workbook named mydata.xlsx in the current working directory. By
default, the variable names in the dataset are used to create column
headings in the spreadsheet and row
 names are placed in the first column of the spreadsheet. If mydata.xlsx
already exists, it is overwritten.

*** export part of output to a table

R can export parts of the output to a table
 here is an example of how to export correlations from 10 separate
correlation tests into 1 table:
 1) create an empty numeric vector:
 cortable2001\_pop0<- numeric(10)
 2)run the correlations
 3)issue :
 attributes(Vector\_NAME)
 IE:
 #attributes(pop\_T2001)
 to get a list of all objects in the vector
 in this case the we are interested in the 'estimate' which is the R
value
 3)after each correlation test finishes export the *part* you want that
corresponds to the placment in the empty vector (in this case the
$estimate )
 cortable2001\_ses0[*1*] <- Cor\_T2001\_p1$estimate
 4)export it to a table:
 write.table(cortable2001\_ses0, file = "C:/Documents and
Settings/EKLOOG/My Documents/Postdoc/~work/D.Predicted PM
analysis/8.Mixed model ses\_pop analysis/d.step 2 in r/T2001\_ses0.csv",
sep = ",", col.names = NA)

*** export SAS

The *write.foreign()*function in the*foreign package* can be used to
export a data frame to an external statistical application. Two files
are created: a free format text file containing the data, and a code
file containing instructions for readingthe data into the external
statistical application. The format is:
 *write.foreign(dataframe, datafile, codefile, package=package)*
 For example, the code for SAS

write.foreign(data1,"C:/test.txt","C:/test.sas",package="SAS",dataname="data1",validvarname="V7")
 For spss:
 *
*write.foreign(mydata, "mydata.txt", "mycode.sps", package="SPSS")*
*

*** export Spss

*SPSS
*
 # write out text datafile and
 # an SPSS program to read it
 library(foreign)
 write.foreign(mydata, "c:/mydata.txt", "c:/mydata.sps", package="SPSS")

*** Export Tab Delimited

You can use the write.table() function to output an R object to a
delimited text file. The format is:
 *write.table(x, outfile, sep=delimiter, quote=TRUE, na="NA")*
 where x is the object and outfile is the target file. For example, the
statement:
 write.table(mydata, "mydata.txt", sep=",")
 would save the dataset mydata to a comma-delimited file named
mydata.txt in the current working directory. Include a path (for
example, "c:/myprojects/mydata.txt") to save the output file elsewhere.
 Replacing sep="," with sep="\t" would save the data in a tab-delimited
file. By default, strings are enclosed in quotes ("")
 and missing values are written as NA.
  

*** output terminal to text (sink)

you can output all terminal commands to a text file using sink
 sink("PATH/TO/FILE" , append=TRUE, split=TRUE)
 IE:

sink("c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.1.Projetcs/3.1.10.Medicare\_NE/3.1.10.5.Results/2.new
model/out.txt" , append=TRUE, split=TRUE)

** Text/graphic Input/Output
*** Text output using sink 
**** start sink output 
#+BEGIN_EXAMPLE
sink(file = NULL, append = FALSE, type = c("output", "message"),
     split = FALSE)
#+END_EXAMPLE

Arguments
'file' a writable connection or a character string naming the file to write to, or NULL to stop sink-ing.
'append' logical If TRUE, output will be appended to file; otherwise, it will overwrite the contents of file.
'type' character Either the output stream or the messages stream.
'split' logical: if TRUE, output will be sent to the new sink and to the current output stream, like the Unix program tee.

₆In example₆ 

#+BEGIN_SRC R
sink("/home/zeltak/smb4k/ZUNISYN/ZUraid/Uni/Projects/P031_MIAC_PM/3.Work/2.Gather_data/FN000_RWORKDIR/sink_mod3_2009.txt", type = c("output", "message"))
#+END_SRC

**** turn off sinc text output and output to console again
simply issue:
#+BEGIN_SRC R
sink()
#+END_SRC

*** pipe R output using pipe on linux CLI
as an alternative to sink you can use the following on linux

#+BEGIN_SRC sh
R | tee PATH/TO/mylog.txt
#+END_SRC

*** Graphic output
**** type of output

these are the default output types you can use in R
 *Note: for more comples output (such as SVG etc..external packages are
needed)*
 pdf("filename.pdf")
 win.metafile("filename.wmf")
 png("filename.png")
 jpeg("filename.jpg")
 bmp("filename.bmp")
 postscript("filename.ps")
 *example:*
 pdf("c:\\Users\\ekloog\\Documents\\tmp\\rgraph.pdf")
 hist(allresp\_o$pmnewmayear)
 hist(allresp$pmnewmayear)
 dev.off()

**** Turn off grapic output

simply issue:
 dev.off()

** mISC
*** create a list from files in directory

us the file.list command:
 *IE:*
 files <- list.files(path =
"c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.1.Projetcs/3.1.2.MIA\_PM\_MODELSV3/3.1.1.4.Work/3.Analysis/AN008\_mod3\_corr/",pattern=".csv$",full.names
= TRUE)
 *Arguments*
 path    a character vector of full path names; the default corresponds
to the working directory getwd(). Missing values will be ignored.
 pattern    an optional regular expression. Only file names which match
the regular expression will be returned.
 all.files    a logical value. If FALSE, only the names of visible files
are returned. If TRUE, all file names will be returned.
 full.names    a logical value. If TRUE, the directory path is prepended
to the file names. If FALSE, only the file names are returned.
 recursive    logical. Should the listing recurse into directories?
 ignore.case    logical. Should pattern-matching be case-insensitive?

* R Data Types (R objects)
** Intro

This diversity of structures provides the R language with a great deal
of flexibility in dealing with data.
 The data types or modes that R can handle include numeric, character,
logical(TRUE/FALSE), complex (imaginary numbers), and raw (bytes). In R,
PatientID, AdmDate, and Age would be numeric variables, whereas Diabetes
and Status would be character variables. Additionally, you'll need to
tell R that PatientID is a case identifier, that AdmDate contains dates,
and that Diabetes and Status are nominal

** Data structures
*** Intro

[[R_files/img/Image_AgHmLINTzVNnE4asBa4QLg_0002.png]]
 In R, an object is anything that can be assigned to a variable. This
includes constants, data structures, functions, and even graphs. Objects
have a *mode* (which describes how the object is stored) and a *class*
(which tells generic functions like print how to handle it).
 A *data frame* is a structure in R that holds data and is similar to
the datasets found in standard statistical packages (for example, SAS,
SPSS, and Stata). The *columns are variables* and the *rows are
observations*. You can have variables of different types (for example,
numeric, character) in the same data frame. Data frames are the main
structures you'll use to store datasets.
 *Factors* are nominal or ordinal variables. They're stored and treated
specially in R.

*** vectors

Vectors are one-dimensional arrays that can hold numeric data, character
data, or logical data.
 The combine function c() is used to form the vector. Here are examples
of each type of vector:
 a <- c(1, 2, 5, 3, 6, -2, 4)
 b <- c("one", "two", "three")
 c <- c(TRUE, TRUE, TRUE, FALSE, TRUE, FALSE)
 Here, *a* is numeric vector, *b* is a character vector, and *c* is a
logical vector.
 *Note* that the data in a vector must only be one type or mode
(numeric, character, or logical). You can't mix modes in the same
vector.
 *NOTE* *Scalars are one-element vectors*. Examples include
 f <- 3, g <- "US" and h <- TRUE.
 They're used to hold constants.
 You can refer to elements of a vector using a numeric vector of
positions within brackets.
 For example:
 a[c(2, 4)]
 refers to the 2nd and 4th element of vector *a*.
 Here are additional examples:
 [[R_files/img/Image_6KBA2VNrM0fNQwJf7gUVNQ_0002.png]]
 The *colon operator* used in the last statement is used to generate*a
sequence of numbers*.
  For example, a <- c(2:6) is equivalent to a <- c(2, 3, 4, 5, 6).

*** Matrices

*Matrices*
 A matrix is a two-dimensional array where each element has the same
mode (numeric, character, or logical). Matrices are created with the
matrix function . The general format is:
 myymatrix <- matrix(vector, nrow=number\_of\_rows,
ncol=number\_of\_columns, byrow=logical\_value, dimnames=list(
char\_vector\_rownames, char\_vector\_colnames))
 where vector contains the elements for the matrix, nrow and ncol
specify the row and column dimensions, and dimnames contains optional
row and column labels stored in character vectors. The option byrow
indicates whether the matrix should be filled in by row (byrow=TRUE) or
by column (byrow=FALSE). The default is by column. The following listing
demonstrates the matrix function
 [[R_files/img/Image_DMrqIcPrrEnynw7CBX3cOg_0003.png]]
 [[R_files/img/Image_DMrqIcPrrEnynw7CBX3cOg_0004.png]]
 First a 2 x 5 matrix is created containing numbers 1 to 10. By default,
the matrix is filled by column. Then the elements in the 2nd row are
selected, followed by the ele- ments in the 2nd column. Next, the
element in the 1st row and 4th column is selected. Finally, the elements
in the 1st row and the 4th and 5th columns are selected. Matrices are
two-dimensional and, like vectors, can contain only one data type.
 When there are more than two dimensions, you'll use arrays. When there
are multiple modes of data, you'll use data frames.

*** arrays

Arrays are similar to matrices but can have more than two dimensions.
They're created with an array function of the following form:
 myarray <- array(vector, dimensions, dimnames)
 where vector contains the data for the array, dimensions is a numeric
vector giving the maximal index for each dimension, and dimnames is an
optional list of dimension labels. The following listing gives an
example of creating a three-dimensional (2x3x4) array of numbers.
 Like matrices, they must be a single mode.
 [[R_files/img/Image_754ha0iQpomqc1hC8EWiNg_0003.png]]

*** Dataframes

*Dataframes*
 A dataframe is more general than a matrix, in that *different columns
can have different modes* (numeric, character, factor, etc.). This is
similar to *SAS and SPSS datasets*.
 Data frames are the most common data structure you'll deal with in R.
 if there are *multiple modes of data, you can't contain this data in a
matrix*. In this case, a data frame would be the structure of choice. A
data frame is created with the *data.frame()* function :
 mydata <- data.frame(col1, col2, col3,...)
 where col1, col2, col3, ... are column vectors of any type (such as
character, numeric, or logical). Names for each column can be provided
with the names function. The following listing makes this clear.
 Each column must have only one mode, but you can put columns of
different modes together to form the data frame.
 There are a variety of ways to identify the elements of a dataframe .
 myframe[3:5] # columns 3,4,5 of dataframe
 myframe[10:15,2:6] #present rows 10-15 for variables 2-6
 myframe[c("ID","Age")] # columns ID and Age from dataframe
 myframe$X1 # variable x1 in the dataframe

*** factors

variables can be described as *nominal, ordinal, or continuous*.
 *Nominal*variables are categorical, without an implied order. Diabetes
(Type1, Type2) is an example of a nominal variable.
 *Ordinal* variables imply order *but not amount*. Status (poor,
improved, excellent) is a good example of an ordinal variable.
 *Continuous* variables can take on any value within some range, and
both *order and amount* are implied.
 *Categorical (nominal) and ordered categorical (ordinal)* variables in
R are called *factors*.
 The function factor() stores the categorical values as a vector of
integers in the range [1... k] (where k is the number of unique values
in the nominal variable), and an internal vector of character strings
(the original values) mapped to these integers.
 For example, assume that you have the vector diabetes <- c("Type1",
"Type2", "Type1", "Type1") The statement
 diabetes <- factor(diabetes)
 stores this vector as (1, 2, 1, 1) and associates it with 1=Type1 and
2=Type2 internally (the assignment is alphabetical).
 Any analyses performed on the vector diabetes will treat the variable
as nominal and select the statistical methods appropriate for this level
of measurement.
 For vectors representing *ordinal variables*, you add the parameter
*ordered=TRUE* to
 the *factor()*function .
 Given the vector status <- c("Poor", "Improved", "Excellent", "Poor")
 the statement
 status <- factor(status, ordered=TRUE)
 will encode the vector as (3, 2, 1, 3) and associate these values
internally as 1=Excellent, 2=Improved, and 3=Poor.
 Additionally, any analyses performed on this vector will treat the
variable as ordinal and select the statistical methods appropriately.
 By default, factor levels for character vectors are created in
alphabetical order. For ordered factors, the alphabetical default is
rarely sufficient. You can override the default by specifying a levels
option. For example
 status <- factor(status, order=TRUE, levels=c("Poor", "Improved",
"Excellent"))
 would assign the levels as 1=Poor, 2=Improved, 3=Excellent.
  Be sure that the specified levels match your actual data values. Any
data values not in the list will be set to missing. The following
listing demonstrates how specifying factors and ordered factors impact
data analyses.
 [[R_files/img/Image_Xy4gWAhs814y7d4pGHKsDw_0003.png]]

*** Lists
**** intro
A list is a generic vector containing other objects.

For example, the following variable x is a list containing copies of three vectors n, s, b, and a numeric value 3.

#+BEGIN_SRC R
 n = c(2, 3, 5) 
 s = c("aa", "bb", "cc", "dd", "ee") 
 b = c(TRUE, FALSE, TRUE, FALSE, FALSE) 
 x = list(n, s, b, 3)   # x contains copies of n, s, b
#+END_SRC

'List Slicing'

We retrieve a list slice with the single square bracket "[]" operator. The following is a slice containing the second member of x, which is a copy of s.
#+BEGIN_SRC R
x[2] 
[[1]] 
[1] "aa" "bb" "cc" "dd" "ee"
#+END_SRC

With an index vector, we can retrieve a slice with multiple members. Here a slice containing the second and fourth members of x.

#+BEGIN_SRC R
x[c(2, 4)] 
[[1]] 
[1] "aa" "bb" "cc" "dd" "ee" 
 
[[2]] 
[1] 3
#+END_SRC

'Member Reference'

In order to reference a list member directly, we have to use the double square bracket "[[]]" operator. The following object x[[2]] is the second member of x. In other words, x[[2]] is a copy of s, but is not a slice containing s or its copy.
#+BEGIN_SRC R
x[[2]] 
[1] "aa" "bb" "cc" "dd" "ee"
#+END_SRC

We can modify its content directly.
#+BEGIN_SRC R
x[[2]][1] = "ta" 
x[[2]] 
[1] "ta" "bb" "cc" "dd" "ee" 
 s 
[1] "aa" "bb" "cc" "dd" "ee"   # s is unaffected 
#+END_SRC
**** Named list members
We can assign names to list members, and reference them by names instead of numeric indexes.

For example, in the following, v is a list of two members, named "bob" and "john".

#+BEGIN_SRC R
v = list(bob=c(2, 3, 5), john=c("aa", "bb")) 
v 
$bob 
[1] 2 3 5 
 
$john 
[1] "aa" "bb"
#+END_SRC

'List Slicing'

We retrieve a list slice with the single square bracket "[]" operator. Here is a list slice containing a member of v named "bob".

#+BEGIN_SRC R
v["bob"] 
$bob 
[1] 2 3 5
#+END_SRC

With an index vector, we can retrieve a slice with multiple members. Here is a list slice with both members of v. Notice how they are reversed from their original positions in v.

#+BEGIN_SRC R
v[c("john", "bob")] 
$john 
[1] "aa" "bb" 
 
$bob 
[1] 2 3 5

#+END_SRC

'Member Reference'

In order to reference a list member directly, we have to use the double square bracket "[[]]" operator. The following references a member of v by name.

#+BEGIN_SRC R
v[["bob"]]
[1] 2 3 5
#+END_SRC

A named list member can also be referenced directly with the "$" operator in lieu of the double square bracket operator.

#+BEGIN_SRC R
v$bob 
[1] 2 3 5
#+END_SRC

'Search Path Attachment'

We can attach a list to the R search path and access its members without explicitly mentioning the list. It should to be detached for cleanup.

#+BEGIN_SRC R
attach(v) 
bob 
[1] 2 3 5 
detach(v) 
#+END_SRC


*** local data frame
**** base
Local data frame is simply a wrapper for a data frame that prints nicely
tbl_df creates a “local data frame”

#+BEGIN_SRC R
# convert to local data frame
flights <- tbl_df(hflights)

# printing only shows 10 rows and as many columns as can fit on your screen
flights
#+END_SRC

** Object Modes
*** check which class a VARIABLE is

use the class command:
 class(T2008allbimon$date)

*** Logical

*Logical*
 elements are logical constants such as T for TRUE and F for FALSE.
 Missing logical constants are indicated by a special data value NA.
When a logical object is used where the context requires numeric values,
the elements are automatically coerced by substituting 1 for TRUE and 0
for FALSE. The mode functions are logical(), is.logical(), and
as.logical().

*** Null

Null - an empty object. The mode functions are is.null(), and as.null().
There is no null() function to create null objects.

*** Numeric data

*Numeric*- elements are simple numeric values.
 Missing numeric values are indicated by a special data value NA. An
element of a numeric vector set to NA has no valid numeric value and
require the prespecification of special handling by most functions. The
mode functions are numeric(), is.numeric(), and as.numeric(). Numeric
values are stored in one of three storage.mode categories may be changed
with the storage.mode() function. Double precision is the default
storage.mode.

*** Strings

a string variable is a variable with characters such as names, adresses
etc..

** workspace
*** clear specific items from workspace using 'gdata' package 
#+BEGIN_SRC r
library(gdata)
keep(aa) #will not remove but show you how the workspace will look
keep(aa, sure=TRUE) #will actually remove the objects and keep the 'aa'
keep(aa,bb sure=TRUE) #to keep 2 or more objects just specify each one
#+END_SRC

*** clear all workspace

to clear the workspace:

rm(list = ls(all = TRUE))

*** remove specific object (rm)
to remove a specific object use:
#+BEGIN_SRC R
rm(object)
#or for multi
rm(p2003,p2004,p2005,p2006,p2007,p2008)
#+END_SRC

*** list the objects in the current workspace

#+BEGIN_SRC sh
ls() # list the objects in the current workspace
#+END_SRC

Pasted from
<[[http://www.statmethods.net/interface/workspace.html][http://www.statmethods.net/interface/workspace.html]]>

*** saving

#+BEGIN_SRC R
# save the workspace to the file .RData in the cwd 
save.image()
#+END_SRC

save specific objects to a file, if you don't specify the path, the cwd is assumed: 

#+BEGIN_SRC R
save(object list,file="myfile.RData")
save(list=c("x", "y", "z"), file="xyz.rda")
#+END_SRC

*** Save RDS
use save and read RDS to save and open a single R object and import it as another object name
#+BEGIN_EXAMPLE
saveRDS(mod, "mymodel.rds")
mod2 <- readRDS("mymodel.rds")

#+END_EXAMPLE
*** Load Rds
#+begin_src R
am2.lu.nd.pb<- readRDS("/home/zeltak/ZH_tmp/pa.rds")
#+end_src

*** save path
#+begin_src R
save("pm", "dat", file = paste0(path.data, "pre_mod1_", Sys.Date(), ".Rdata"))
#+end_src

*** load a workspace into the current session

if you don't specify the path, the cwd is assumed

load("myfile.RData")

 Pasted from
<[[http://www.statmethods.net/interface/workspace.html][http://www.statmethods.net/interface/workspace.html]]>
*** get object size
#+begin_src R
print(object.size(mod3grid), units = "MB") 
#+end_src

** output
*** disable scientifc notations
run this

#+BEGIN_EXAMPLE
options(scipen=999)
#+END_EXAMPLE

this can also be written to your .rprofile to be permanent
* Data Management
** Operators/functions/Math
*** Arithmetic operators

[[R_files/img/Image_kTPm5vo7exVxohhSa98PUw_0003.png]]

*** logical operators

[[R_files/img/Image_OyxJAju3sVVuTKafqqAz6g_0003.png]]

*** Mathematical functions

[[R_files/img/Image_SM1Z23eNFKMrFcQGgNgF8w_0003.png]]

*** Statistical functions

[[R_files/img/Image_AtvHZv1ot3M1WsPzE7C4zA_0005.png]]
 [[R_files/img/Image_AtvHZv1ot3M1WsPzE7C4zA_0006.png]]

*** Standarize data (scale)
By default, the scale() function standardizes the specified columns of a matrix or data frame to a mean of 0 and a standard deviation of 1:
newdata <- scale(mydata)
To standardize each column to an arbitrary mean and standard deviation, you can use code similar to the following:

newdata <- scale(mydata)*SD + M

where M is the desired mean and SD is the desired standard deviation.
Using the scale() function on non-numeric columns will produce an error. To standardize a specific column rather than an entire matrix or data frame, you can use code such as

newdata <- transform(mydata, myvar = scale(myvar)*10+50)

This code standardizes the variable myvar to a mean of 50 and standard deviation of 10.

examples

#+BEGIN_SRC R
pm25.m1[,elev.s:= scale(elev)]
pm25.m1[,tden.s:= scale(tden)]
pm25.m1[,pden.s:= scale(pden)]
#+END_SRC



*** Character functions

[[R_files/img/Image_CaSfF5a6dSMOp0dbbA0Q3Q_0003.png]]
 Note that the functions grep(), sub(), and strsplit() can search for a
text string
 (fixed=TRUE) or a regular expression (fixed=FALSE) (FALSE is the
default). Regular expressions provide a clear and concise syntax for
matching a pattern of text. For ex ample, the regular expression
 \^[hc]?at
 matches any string that starts with 0 or one occurrences of h or c,
followed by at. The expression therefore matches hat, cat, and at, but
not bat.

*** Seed

**** setting the seed for random number generation

setting the seed for random number generation
 Each time you generate pseudo-random deviates, a different seed, and
therefore different results, are produced. To make your results
reproducible, you can specify the seed explicitly, using the set.seed()
function.
 An example is given in the next listing. Here, the runif() function is
used to generate pseudo-random numbers from a uni-
 form distribution on the interval 0 to 1.
 [[R_files/img/Image_O2ejh7nbVzX4nXkyKgvcWA_0003.png]]

*** Misc useful functions

[[R_files/img/Image_vWX1hDmWQ86vjSPxjlkgfA_0003.png]]

*** row means/sums

[[R_files/img/Image_aTBqfsWEwWi7HEQxNUownA_0003.png]]
 colSums (x, na.rm = FALSE, dims = 1)
 rowSums (x, na.rm = FALSE, dims = 1)
 colMeans(x, na.rm = FALSE, dims = 1)
 rowMeans(x, na.rm = FALSE, dims = 1)
 rowMeans(x, na.rm = TRUE)
 example:
 cvtable$mean<- rowMeans(cvtable[,2:10])
 *where:*
 we only want the mean of variables (columns) 2-10
 *In example:*
 F\_T2002\_All$meankin<-rowMeans(F\_T2002\_All[,18:19])
*** create a log variable 

#+begin_src R
m1_2003$logroad<-log(m1_2003$Mjrrdden_1 +.1)
#+end_src

** Aggregating and Reshaping Data
*** Basic
R provides a number of powerful methods for aggregating and reshaping data. When you aggregate data, you replace groups of observations with summary statistics based on those observations. When you reshape data, you alter the structure (rows and columns) determining how the data is organized.
*** Aggragete
**** Data Aggregation in R: plyr, sqldf and data.table

http://www.psychwire.co.uk/2011/04/data-aggregation-in-r-plyr-sqldf-and-data-table/

-plyr-

Let’s begin with plyr. The power of plyr comes from the fact that it splits up data, runs a function on the split-up data, and then sticks it all back together. It has a wide variety of useful aggregation functions, but here I’m going to use ddply. This function gives as it’s output a dataframe and gives as output another dataframe. The plyr functions are written in the syntax of XYply where X is the input object type and Y is the output object type. In this case, both ds of ddply stand for dataframe. Let’s look at some initial code:

ddply(full_list, c("Subject","Class"), function(df)mean(df$RT))

This is fine, and gives us mean DPS values for each class and spec. But there’s a problem. The “mean” column is labelled V1, which isn’t that helpful, especially if we have multiple columns computed (i.e., ending up with V1, V2, V3 makes it hard to remember which column is which). So let’s get the column labelled:

ddply(full_list, c("Subject","Class"), function(df) return(c(AVERAGE=mean(df$RT))))

Great! Now let’s add some more columns to output:

ddply(full_list, c("Subject","Class"), function(df) return(c(AVERAGE=mean(df$RT),

MEDIAN=median(df$RT),SE=sqrt(var(df$RT)/length(df$RT)))))

This then gives us the target aggregated table pictured above.

-sqldf-

Next up is sqldf. The name gives is away slightly: it’s a library for running SQL statements on data frames. SQL stands for Structured Query Language, with data stored on tables in a database. There are a number of SQL database types, which are all reasonably similar, and sqldf uses as default the incredibly popular SQLite. To get the target aggregated data using this, it’s a case of running a simple query:

sqldf("SELECT SUBJECT, CLASS, AVG(RT) AS AVERAGE, MEDIAN(RT) AS MEDIAN,

SQRT((VARIANCE(RT)/COUNT(RT))) AS SE

FROM full_list

GROUP BY SUBJECT, CLASS")

Note that to get the number of rows involved, we need to use COUNT rather than LENGTH. Easy!

-data.table-

The last library to look at here is data.table. This has the benefit of being considered the roadrunner of aggregation functions. It’s damn fast! This can be achieved as follows:

dps_dt = data.table(full_list)
dps_dt[,list(AVERAGE=.Internal(mean(RT)), MEDIAN=median(RT),

SE= sqrt(var(RT)/length(RT))),by=list(Subject,Class)]

Note that the first line takes our data.frame called full_list and casts it as a data.table object type. Here, two lists are used to do two things: (1) create the column names and (2) group the data by class and spec. The first list call sets up the column names and the calculations that need to be run. The second list gets fed to the by function which then aggregates by class and spec.

**** 'aggregate using ddply'
#+begin_src R
aggf<- ddply(m1_2003, c("SiteCode"), function(df) return(c(barpm=mean(df$PM25),barpred=mean(df$predicted))))
#+end_src   
**** basic

*Aggregating Data*
 It is relatively easy to collapse data in R using one or more*BY variables* and a defined function (mean,sum etc..). The format is:

'aggregate(x, by, FUN)'

where x is the data object to be collapsed, by is a list of variables
that will be crossed to form the new observations, and FUN is the scalar
function used to calculate summary statistics that will make up the new
observation values
*NOTE: it is much better to attach the dataset and then detach it for easier operation and file names*
 *example code:*
aggdata <-aggregate(mtcars, by=list(cyl),FUN=mean, na.rm=TRUE)
When using the aggregate() function, the by variables must be in a list
(even if there is only one).

**** aggregate by 2 variables

1)basic example for 2 variable aggregate
 [[R_files/img/Image_P4U8ow9UO4l0atrmLjT32A_0002.png]]
 2)example using a 'model format' :
 aggpmx<- aggregate(pm$xpm ~ pm$sitecode,FUN=mean, na.rm=TRUE)
 for 2 or more variables:

AGGDATA<- aggregate(cbind(DATA$AGGVAR1,DATA$AGGVAR2)~DATA$CLASSVAR1,FUN=mean, na.rm=TRUE)


where:
DATA$AGGVAR1-the variable(s) to be aggregated by the Fun (mean sum etc)
 DATA$CLASSVAR1-the variable to aggregate by (the class variable)
 aggpmx<- aggregate(cbind(T2001allbimon$pm\_mod3,T2001allbimon$mixpred)
~ T2001allbimon$guid,FUN=mean, na.rm=TRUE)

3)example with separate aggregates and the using the merge function to
combine:
 attach(T2001c)
 agg1<- aggregate(mfvalue ~ sitecode,FUN=mean, na.rm=TRUE)
 agg2<- aggregate(predicted ~ sitecode,FUN=mean, na.rm=TRUE)
 aggf <- merge(agg1,agg2,by="sitecode") # merge two dataframes by
sitecode
 where:
 sitecode- is the aggregated variable

**** quick rename aggregated variables

use this:
 *names(DATASET) <- c("NEW VAR 1 NAME", "NEW VAR 2 NAME")*
 Example:
 names(cagg) <- c("guid", "count")
**** aggregate data to find number of observations per variable (the number of unique values as an additional column)
#+begin_src R
library("plyr")
data_frame <- data.table(ddply(pm2011, .(SiteCode), transform, n = length(SiteCode)))
#+end_src
**** proc summary eqivalant
#+begin_src R
#in SAS
proc summary data=dat nway;
  class a b;
  var x y;
  output out=smry mean(x)=xmean mean(y)=ymean var(y)=yvar;
run;

#with plyr in R:

smry <- ddply(dat, .(a, b), summarise, xmean=mean(x), ymean=mean(y), yvar=var(y))
#+end_src

*** Reshape Package

[[R_files/attach/melt.pdf][Attachment #03 (melt.pdf)]]
 Reshape Package (see also attached pdf)
 Basically, you "melt" data so that each row is a unique id-variable
combination. Then you "cast" the melted data into any shape you would
like. Here is a very simple example.
 *mydata
*
 *id    time x1    x2*
 1    1    5    6
 1    2    3    5
 2    1    6    1
 2    2    2    4
  
 # example of melt function
 library(reshape)
 mdata <- melt(mydata, id=c("id","time"))
 *newdata
*
 *id time variable    value*
 1    1    x1    5
 1    2    x1    3
 2    1    x1    6
 2    2    x1    2
 1    1    x2    6
 1    2    x2    5
 2    1    x2    1
 2    2    x2    4
 # cast the melted data
 # cast(data, formula, function)
 subjmeans <- cast(mdata, id~variable, mean)
 timemeans <- cast(mdata, time~variable, mean)
 *subj means
*
 *id    x1    x2*
 1    4    5.5
 2    4    2.5
 *timemeans
 time x1    x2*
 1    5.5    3.5
 2    2.5    4.5
 There is much more that you can do with the melt( ) and cast( )
functions. See the documentation for more details.
 you can use fun.aggregate = mean also

**** Basic

Basically, you'll "melt" data so that each row is a unique ID-variable
combination. Then you'll "cast" the melted data into any shape you
desire. During the cast, you can aggregate the data with any function
you wish.The dataset you'll be working with is shown here in table 5.8:
 [[R_files/img/Image_64ditW3E27VwG9X2eP08jQ_0002.png]]
 In this dataset, the measurements are the values in the last two
columns (5, 6, 3, 5, 6, 1, 2, and 4). Each measurement is uniquely
 identified by a combination of ID variables (in this case ID, Time, and
whether the measurement is on X1 or X2). For example,
 the measured value 5 in the first row is uniquely identified by knowing
that it's from observation (ID) 1, at Time 1, and on
 variable X1.

**** Melting data

When you melt a dataset, you restructure it into a format where each
measured vari-
 able is in its own row, along with the ID variables needed to uniquely
identify it. If you
 melt the data from table 5.8, using the following code
 library(reshape)
 md <- melt(mydata, id=(c("id", "time")))
 you end up with the structure shown in table 5.9.
 [[R_files/img/Image_hHSdBwwzt17G066M8c4g0w_0003.png]]
 Note that you must specify the variables needed to uniquely identify
each measurement (ID and Time) and that
 the variable indicating the measurement variable names (X1 or X2) is
created for
 you automatically. Now that you have your data in a melted form, you
can recast it into any shape, using
 the *cast()*function.

**** Casting data

The cast() function starts with melted data and reshapes it using a
formula that
 you provide and an (optional) function used to aggregate the data. The
format is
 *newdata <- cast(md, formula, FUN)*
  
 where md is the melted data, formula describes the desired end result,
and FUN is the
 (optional) aggregating function. The formula takes the form
 *rowvar1 + rowvar2 + ... ~ colvar1 + colvar2 + ...*
 In this formula, rowvar1 + rowvar2 + ... define the set of crossed
variables that define the rows, and colvar1 + colvar2 + ... define the
set of crossed variables that define the columns. See the examples in
figure 5.1.
 [[R_files/img/Image_Kp45EEyZp6nZA8XaX8BRsQ_0003.png]]
 Because the formulas on the right side (d, e, and f) don't include a
function, the data is reshaped. In contrast, the examples on the left
side (a, b, and c) specify the mean as an aggregating function. Thus the
data are not only reshaped but aggregated as well. For example, (a)
gives the means on X1 and X2 averaged over time for each observation.
Example (b) gives the mean scores of X1 and X2 at Time 1 and Time 2,
averaged over observations. In (c) you have the mean score for each
observation at
 Time 1 and Time 2, averaged over X1 and X2. As you can see, the
flexibility provided by the melt() and cast() functions is
 amazing. There are many times when you'll have to reshape or aggregate
your data prior to analysis. For example, you'll typically need to place
your data in what's called

*** Transpose

**** Basic

The transpose (reversing rows and columns) is perhaps the simplest
method of reshaping a dataset.
 Use the *t()* function to transpose a matrix or a data frame. In the
latter case, row names become variable (column) names.
 An example is presented:
 [[R_files/img/Image_Jbrhox7YxpYrqLecCqHFtw_0002.png]]
 IE:
 x<-t(DATASET)
** Creating new dataset/variables/rows
*** Intro

Use the assignment operator <- to create new variables. A wide array of
operators and functions are available here.
 # Three examples for doing the same computations
 mydata$sum <- mydata$x1 + mydata$x2
 mydata$mean <- (mydata$x1 + mydata$x2)/2
 attach(mydata)
 mydata$sum <- x1 + x2
 mydata$mean <- (x1 + x2)/2
 detach(mydata)
 mydata <- transform( mydata,
 sum = x1 + x2,
 mean = (x1 + x2)/2
 )
 or
 data\_10$predicted = newpred2000
 where:
 data\_10 is the file name
 $ is the sign for variable name
 predicted is the new variable name
 and = newpred2000 means it takes its values from the newpred2000 file

*** create a new empty vector

create a new empty vector
 Basic way to create an empty numeric vector issue :
 cortable$2001\_pop0<- numeric(10)
 where:
 cortable$2001\_pop0 > name of new vector
 numeric > type of vector
 10 > length of vector
 *Alternative options:*
 *with operators:*
 mydata$sumx <- mydata$x1 + mydata$x2
 mydata$meanx <- (mydata$x1 + mydata$x2)/2
 *using attaching:*
 attach(mydata)
 mydata$sumx <- x1 + x2
 mydata$meanx <- (x1 + x2)/2
 detach(mydata)
 *with transform*
 mydata <- transform(mydata,
                     sumx = x1 + x2,
                     meanx = (x1 + x2)/2)

*** create empty dataset

examples for creating new tables:
 cvtable <- data.frame(res=numeric(10),mod3=numeric(10))
 where:
 10-length of variable
 numeric-type of variable
 dd <- data.frame(age=numeric(0),sex=character(0))
 or for multiple variables
 cvtable <- data.frame(r2000=numeric(15),r2001=numeric(15))
 *another example:
*
 #create CV table
 ICAM\_restable <-
data.frame(lag=character(7),beta=numeric(7),se=numeric(7),pc=numeric(7),L\_CI=numeric(7),H\_CI=numeric(7),sig=numeric(7),ciw=numeric(7))

*** create a dataframe from variables

Usually you will obtain a dataframe by importing it from SAS, SPSS,
Excel, Stata, a database, or an ASCII file. To create it interactively,
you can do something like the following.
 # create a dataframe from scratch
 age <- c(25, 30, 56)
 gender <- c("male", "female", "male")
 weight <- c(160, 110, 220)
 *mydata <- data.frame(age,gender,weight)*
 You can also use R's built in spreadsheet to enter the data
interactively, as in the following example.
 # enter data using editor
 mydata <- data.frame(age=numeric(0), gender=character(0),
weight=numeric(0))
 mydata <- edit(mydata)
 # note that without the assignment in the line above,
 # the edits are not saved!

*** enter character row names

to add names to existing rows in a data frame use this:
 DATASETNAME$VARIABLENAME <- C("ROW 1 NAME", "ROW 2 NAME","ROW 3 NAME")
 IE:
 ICAM\_restable$lag <- c("lag24h", "lag003", "lagweek", "lag2week",
"lag3week", "lagmonth", "lagyear")

*** add rows to an existing data frame

to add rows to an exisitng data frame create a new dataframe with
exactly the same variable number and names but with the *needed rows* to
add:
 *Existing dataset:*
 cvtable <- data.frame(type=character(17),
r2000=numeric(17),r2001=numeric(17),r2002=numeric(17),r2003=numeric(17),r2004=numeric(17),r2005=numeric(17),r2006=numeric(17),r2007=numeric(17),r2008=numeric(17),r2009=numeric(17),r2010=numeric(17))
 cvtable$type <- c("it\_1",
"it\_2","it\_3","it\_4","it\_5","it\_6","it\_7","it\_8","it\_9","it\_10","R\_preloc","spat\_pre","tem\_pre","R","R2","spatial","temporal")
 *NEW dataset:*
 newtable <- data.frame(type=character(5),
r2000=numeric(5),r2001=numeric(5),r2002=numeric(5),r2003=numeric(5),r2004=numeric(5),r2005=numeric(5),r2006=numeric(5),r2007=numeric(5),r2008=numeric(5),r2009=numeric(5),r2010=numeric(5))
 newtable$type <- c("winter", "summer","lowpop","highpop","extra")
 then just *rbind* the two:
 cvtable3 <- rbind(cvtable,newtable)
*** generate random numbers
This looks like the same exercise as the last one, but now we only want whole numbers, not fractional values. For that, we use the sample function:

#+BEGIN_SRC sh
x3 <- sample(1:10, 1)
#+END_SRC

The first argument is a vector of valid numbers to generate (here, the numbers 1 to 10), and the second argument indicates one number should be returned. If we want to generate more than one random number, we have to add an additional argument to indicate that repeats are allowed:

#+BEGIN_SRC sh
x4 <- sample(1:10, 5, replace=T)
#+END_SRC

₆In example₆ 

#+BEGIN_SRC sh
m1_2008$rxx<-sample(1:4, 5623, replace=T)
#+END_SRC
where 1:4 is the numbers (levels) to be used, 5623 is the dimension of the dataset

** Deleting Data
*** Delete rows

use this:
 x <- x[-n,]
 where n is the row (case) number
 IE:
 zinc <- read.csv("c:/2.Gather\_data/export to R/orig\_data\_raw.csv",
header=T)
 zinc <- zinc[-*771*,]
 *771-*is the row number in the table (dataset)

*** delete duplicate cases
to delete duplicate cases

#+BEGIN_SRC R
DATA<-subset(DATA,!duplicated(DATA$VARIABLE))
#example:
metstat2<-subset(metstat,!duplicated(metstat$STN))
#for multiple
tss<-ndviX[!duplicated(ndviX[,c("ndviid","m")]),]
#+END_SRC

*** Excluding (DROPPING-deleting) Variables

1.Excluding single Variable

#+BEGIN_SRC sh
dataframe$VAR <- NULL 
#+END_SRC

completely removes the variable.


2.exclude multiple variables
 #*exclude* variables (this excludes *VAR1* and *VAR2* from newdata)
 DELLIST <- names(DATASETNAME) %in% c("*VAR1*", *"VAR2*")
 newdata <- DATASETNAME[*!*DELLIST]
 altentate way to exclude multiple variables
 # exclude all variable but the 66 to 76 variables
 newdata <- OLDDATA[c(66:76)]
 3.Include multiple variables
 #*include* variables (this only includes *VAR1* and *VAR2* in newdata)
 DELLIST <- names(DATASETNAME) %in% c("*VAR1*", *"VAR2*")
 newdata <- DATASETNAME[DELLIST]
 *examples:
*
 DELLIST <- names(grid\_2003) %in% c("OBJECTID.x", "Id.x", "X.x", "Y.x")
 grid\_2003 <- grid\_2003[!DELLIST]

*** workspace/object deletions

Delete temporary objects and objects that are no longer needed. The
calll:
 *rm(list=ls())*
 will remove all objects from memory, providing a clean slate.
 Specific objects can be removed with
 *rm(object)*

** Missing Data
*** delete missing cases from dataset based on a specific variables:

to delete missing cases from dataset based on a specific variable use:
#+BEGIN_SRC R
NEWDATA <-  OLDDATA[complete.cases(OLDDATA$VARIABLEUSED),]

#delete missing cases in predictions
T2000_merged <- T2000_merged[complete.cases(T2000_merged$pred),]

#+END_SRC


*** delete all missing data from all dataset (na.omit)

The function na.omit() returns the object with listwise deletion of
missing values.
 # create new dataset without missing data
 newdata <- na.omit(mydata)
 *NOTE:Carefull this could wipe out all DB..each cases with even 1
missing variable data will be wiped!!)*
 newdata = na.omit(data\_10)
 where:
 newdata is the new datafile created
 na is the omiting parameters na=not avilabe (missing cases)
 omit is the command to delete the cases
 data\_10 is the source database
 To delete cases from a specific variable issue
 *to use it with a specific variable*
 newdata = na.omit(data\_10$predicted)
 where:
 newdata is the new datafile created
 na is the omiting parameters na=not avilabe (missing cases)
 omit is the command to delete the cases
 data\_10 is the source databas
 predicted is the variable used name

*** exclude missing values from analysis

Excluding Missing Values from Analyses
 Arithmetic functions on missing values yield missing values.
 x <- c(1,2,NA,3)
 mean(x) # returns NA
 mean(x, na.rm=TRUE) # returns 2
 The function complete.cases() returns a logical vector indicating which
cases are complete.
 # list rows of data that have missing values
 mydata[!complete.cases(mydata),]

*** Recoding Values to Missing

Recoding Values to Missing
#+begin_src r
# recode 99 to missing for variable v1
# select rows where v1 is 99 and recode column v1
mydata[mydata$v1==99,"v1"] <- NA
#+end_src

*** Testing for Missing Values (is.na)

Testing for Missing Values
 is.na(x) # returns TRUE of x is missing
 y <- c(1,2,3,NA)
 is.na(y) # returns a vector (F F F T)

*** delete missing values from specific variables

#+begin_src r
mod1 <- raw1[!is.na(raw1$meanpbl+raw1$avewsp) & !is.na(raw1$rhmean) & raw1$mon != "SJA", ]
#+end_src
** columns and rows
*** How to add values to a specific matrix row-column
If m is your matrix, then

#+BEGIN_SRC R
m = matrix(0, 2, 4)
m[2,3] = m[2,3] + 10
m
     [,1] [,2] [,3] [,4]
[1,]    0    0    0    0
[2,]    0    0   10    0
#+END_SRC

*** calculate a mean across a row 
use the rowMwans function
@note this syntax seems to only work on data.frame objetcs@

#+BEGIN_SRC R
#calculate means for all rows in column 14,15
#also dont failon NA
y$tst<-rowMeans(y[,14:15], na.rm=T)
#+END_SRC

** Recode variables
*** Recode into breaks

use the 'cut' command:
#+begin_src r
ff1$popfac <- as.factor(cut(ff1$pop06,breaks=c(0,500,1000,2000,10000),include.lowest = TRUE))
#+end_src

*** Recode missing (9,99 etc) to NA (r sys default)

To recode missing (9,99 etc) to NA (r sys default), You must recode the value (IE 99) for the Variable to missing with code

#+BEGIN_EXAMPLE
DATAFRAME$VAR[DATAFRAME$VAR == MISS_VALUE] <- NA
#+END_EXAMPLE

₆In example₆ 

#+BEGIN_EXAMPLE
leadership$age[leadership$age == 99] <- NA
#+END_EXAMPLE

The statement variable[condition] <- expression will only make the assignment  when condition is TRUE.

*** Recoding a continuous variable into categorical variable

to recode continous data into numeric

#+BEGIN_SRC R
mpmg$bimon[mpmg$m==1] <- 1
mpmg$bimon[mpmg$m==2] <- 1
mpmg$bimon[mpmg$m==3] <- 2
mpmg$bimon[mpmg$m==4] <- 2
mpmg$bimon[mpmg$m==5] <- 3
mpmg$bimon[mpmg$m==6] <- 3
mpmg$bimon[mpmg$m==7] <- 4
mpmg$bimon[mpmg$m==8] <- 4
mpmg$bimon[mpmg$m==9] <- 5
mpmg$bimon[mpmg$m==10] <- 5
mpmg$bimon[mpmg$m==11] <- 6
mpmg$bimon[mpmg$m==12] <- 7
#+END_SRC

convert into factor

#+BEGIN_SRC R
#Mark those whose control measurement is <7 as "low", and those with >=7 as "high":
data$category[data$control< 7] <- "low"
data$category[data$control>=7] <- "high"
# Convert the column to a factor
data$category <- factor(data$category)
# subject sex control cond1 cond2 scode category
#       1   M     7.9  12.3  10.7    g1     high
#       2   F     6.3  10.6  11.1    g2      low
#       3   F     9.5  13.1  13.8    g2     high
#       4   M    11.5  13.4  12.9    g1     high
#+END_SRC

another example: create 4 MOTHEREDUC categories

#+BEGIN_SRC R
attach(bw)
*bw$medu[MOTHEREDUC <= 8] <- "nohs"*
bw$medu[MOTHEREDUC > 8 & MOTHEREDUC <= 12] <- "hs"
bw$medu[MOTHEREDUC > 12 & MOTHEREDUC <= 16] <- "col"
bw$medu[MOTHEREDUC >= 16] <- "grad"
detach(bw)
#+END_SRC

*** 'Recode discrete variables (Using car pcakage)'

#+BEGIN_SRC R
library(car)
GAM_T2004x$bimon<-recode(GAM_T2004x$m,"1=1;2=1;3=2;4=2;5=3;6=3;7=4;8=4;9=5;10=5;11=6;12=6")
#+END_SRC
*** Recoding a categorical variable
The easiest way is to use revalue() or mapvalues() from the plyr package. 
This will code M as 1 and F as 2, and put it in a new column. Note that these functions preserves the type: if the input is a factor, the output will be a factor; and if the input is a character vector, the output will be a character vector.

#+BEGIN_SRC R
# The following two lines are equivalent:
data$scode <- revalue(data$sex, c("M"="1", "F"="2"))
data$scode <- mapvalues(data$sex, from = c("M", "F"), to = c("1", "2"))
# subject sex control cond1 cond2 scode
#       1   M     7.9  12.3  10.7     1
#       2   F     6.3  10.6  11.1     2
#       3   F     9.5  13.1  13.8     2
#       4   M    11.5  13.4  12.9     1
#+END_SRC

** Replace

*** replaced numeric data

replace(x, list, values) #remember to assign this to some object i.e., x
<- replace(x,x==-9,NA) #similar to the operation x[x==-9] <- NA
 Pasted from
<[[http://www.personality-project.org/R/r.commands.html][http://www.personality-project.org/R/r.commands.html]]>
 *example:*
 here we replace all negative elevation numbers to '0':
 points200$elev<- replace(points200$elev, points200$elev<= 0 ,0)

** Rename variables

*** with reshape package

First a package named "reshape" needs to be add from the repositories
 library(reshape)
 mydata <- rename(mydata, c(oldname="newname"))
 # you can re-enter all the variable names in order
 # changing the ones you need to change.the limitation
 # is that you need to enter all of them!
 names(mydata) <- c("x1","age","y", "ses")
 *Example:
*

------------------------------------------------------------------------------------------------------------------
 library(reshape)
 mydata <- rename(mydata, c(oldname="newname"))

------------------------------------------------------------------------------------------------------------------
 for example:

------------------------------------------------------------------------------------------------------------------
 library(reshape)
 aodmc\_2000 <- rename(aodmc\_2000, c(aod="AOD"))

------------------------------------------------------------------------------------------------------------------
 *#to rename multiple variables at once*
 T2009 <- rename(T2009,c(V6="AOD",V1="c",V2="m",V3="d",V4="x",V5="y") )

*** with gregmisc package

use:
 rename.vars(data, from="", to="", info=TRUE)
 IE:
 aggmetx2 <- rename.vars(aggmetx, from="x", to="STN", info=FALSE)
 to rename multiple variables at once use the following code:
 metxy <- rename.vars(metxy, c("LONG\_DD","LAT\_DD"), c("xmet","ymet"))

*** rename columns

For matrices use:
 colnames((aggf) <- c("SITECODE", "barpm", "barpred")
 for *already defined dataframes*:
 names(aggf) <- c("SITECODE", "barpm", "barpred")

*** rename using a GUI (fix)

You can use the statement
 fix(leadership)
 to invoke an interactive editor, click on the variable names, and
rename them in the dialogs that are presented
 [[R_files/img/Image_R2BywVFvCSlOTPf7V7ZwVA_0003.png]]

*** using the names() function

you can rename variables via the names() function . For example:
 names(leadership)[2] <- "testDate"
 would rename date to testDate
 In a similar fashion:
 names(leadership)[6:10] <- c("item1", "item2", "item3", "item4",
"item5")
 would rename q1 through q5 to item1 through item5.

** Dates
*** Basic

Dates are typically entered into R as *character strings and then
translated into date variables* that *are stored numerically*. The
function *as.Date()* is used to make this translation. The syntax is
*as.Date(x, "input\_format")* , where x is the character data and
input\_format gives the appropriate format for reading the date
 [[R_files/img/Image_7db9ae9Qz4wQZm0915NJ1A_0005.png]]
 [[R_files/img/Image_7db9ae9Qz4wQZm0915NJ1A_0006.png]]
 The *default format* for inputting dates is yyyy-mm-dd. The statement
 mydates <- as.Date(c("2007-06-22", "2004-02-13"))
 converts the character data to dates using this default format. In
contrast,
 strDates <- c("01/05/1965", "08/16/1975")
 dates <- as.Date(strDates, "%m/%d/%Y")
 reads the data using a mm/dd/yyyy format.
 use case
 *if the date is in SAS date9 format then use something like this:*
 test2 <- as.Date(test, format = "%d%b%Y")
 *or if its in a format common like this 08/25/2000:*
 mb2$rdate <- as.Date(mb1$date,format='%m/%d/%Y')
 NOTE: make sure to add special signs (like '/' or '-' if present in the
orig date field and to use to correct symbol (%m vs %b) etc

*** convert from SAS/other date into R date

#+begin_src r
mod1$day <- as.Date(strptime(mod1$DATE, "%m/%d/%y"))
#+end_src

*** advanced date variable table

#+BEGIN_EXAMPLE
 %a, %A Abbreviated and full weekday name.
 %b, %B Abbreviated and full month name.
 %d Day of the month (01---31).
 %H Hours (00---23).
 %I Hours (01---12).
 %j Day of year (001---366).
 %m Month (01---12).
 %M Minute (00---59).
 %p AM/PM indicator.
 %S Second as decimal number (00---61).
 %U Week (00---53); the first Sunday as day 1 of week 1.
 %w Weekday (0--6, Sunday is 0).
 %W Week (00---53); the first Monday as day 1 of week 1.
 %y Year without century (00---99)
 %Y Year with century.
 %z (output only.) Offset from Greenwich; -0800 is 8 hours west of.
 %Z (output only.) Time zone as a character string (empty if not
available
#+END_EXAMPLE

*** convert a string to a date example

You can use the as.Date( ) function to convert character data to dates.
The format is as.Date(x, "format"), where *x* is the character data
(vector) and *format* gives the appropriate format the *x* data is in.
 d <- as.Date(*DATE*, format="*%m/%d/%Y*")
 *example:*
 # convert date info in original format format 'mm/dd/yyyy'
 dates <- as.Date(*DATE*, "*%m/%d/%Y*")
 *my examples:*
 aggmpmreg2003\_r3$date <- as.Date(aggmpmreg2003\_r3$*DATE*,
"*%Y/%m/%d*")
 or if the date data is arranged differently
 mod2pred2003reg\_r3$date <- as.Date(mod2pred2003reg\_r3$*DATE*,
"*%m/%d/%y*")
 *NOTE: the The default format in R is yyyy-mm-dd
*

*** for var in collection:
You can use the*format(x, format="output\_format")* function to output
dates in a
 specified format, and to extract portions of dates:
 > today <- Sys.Date()
 > format(today, format="%B %d %Y")
 [1] "December 01 2010"
 > format(today, format="%A")
 [1] "Wednesday"
 The format() function takes an argument (a date in this case) and
applies an output format.

*** Extract day of the year

#+begin_src r
mod1$dayofyr <- as.numeric(format(mod1$day, "%j"))
#+end_src

*** Extract Parts of a Date Object

to extract the weekday, month or quarter, or the Julian time (days since
some origin) you can use built in R functions:
 ## S3 method for class 'Date'
 months(x)
 IE:
 mod2all2003$m <- months(mod2all2003$date)
 *Other examples:*
 weekdays(x, abbreviate)
 ## S3 method for class 'POSIXt'
 weekdays(x, abbreviate = FALSE)
 ## S3 method for class 'Date'
 weekdays(x, abbreviate = FALSE)
 months(x, abbreviate)
 ## S3 method for class 'POSIXt'
 months(x, abbreviate = FALSE)
 ## S3 method for class 'Date'
 months(x, abbreviate = FALSE)
 quarters(x, abbreviate)
 ## S3 method for class 'POSIXt'
 quarters(x, ...)
 ## S3 method for class 'Date'
 quarters(x, ...)
 julian(x, ...)
 ## S3 method for class 'POSIXt'
 julian(x, origin = as.POSIXct("1970-01-01", tz="GMT"), ...)
 ## S3 method for class 'Date'
 julian(x, origin = as.Date("1970-01-01"), ...)
 weekdays and months return a character vector of names in the locale in
use.
 quarters returns a character vector of "Q1" to "Q4".
 julian returns the number of days (possibly fractional) since the
origin, with the origin as a "origin" attribute.

*** date calculations

*internal commands:*
 Sys.Date( ) returns today's date.
 Date() returns the current date and time.
 When R stores dates internally, they're represented as the number of
days since January 1, 1970, with negative values for earlier dates. That
means you can perform arithmetic operations on them. For example:
 startdate <- as.Date("2004-02-13")
 enddate <- as.Date("2011-01-22")
  
 days <- enddate - startdate
 results in the Time difference of 2535 days and it displays the number
of days between February 13, 2004 and January 22, 2011.
 Finally, you can also use the function *difftime()* to calculate a time
interval and express it as seconds, minutes, hours, days, or weeks.
Let's assume that I was born on October 12, 1956. How old am I?
 > today <- Sys.Date()
 > dob <- as.Date("1956-10-12")
 > difftime(today, dob, units="weeks")
 Time difference of 2825 weeks

*** convert a date Character

*Date to Character*
 You can convert dates to character data using the as.Character( )
function.
 # convert dates to character data
 strDates <- as.character(dates)
 The conversion allows you to apply a range of character functions to
the data values (subsetting, replacement, concatenation, etc.).

*** Lubridate package

Lubridate is an R package that makes it easier to work with dates and
times.
 Pasted from
<[[http://www.r-statistics.com/][http://www.r-statistics.com/]]>

*** subset by date range
1. make sure the date field is [[#7db9ae9Qz4wQZm0915NJ1A][converted to the standard R]] date
2. issue the followiing command

#+BEGIN_SRC R
NEWDATA <-subset(FULLDATA, as.Date(DATEFIELD) >= 'DATERANGE' & as.Date(DATEFIELD) <= 'DATERANGE')

#example
mb4 <-subset(mb3, as.Date(rdate) >= '2003-09-02' & as.Date(rdate) <= '2004-09-04')
#+END_SRC
*** Create bimon

#+BEGIN_SRC R
library(car)
GAM_T2004x$bimon<-recode(GAM_T2004x$m,"1=1;2=1;3=2;4=2;5=3;6=3;7=4;8=4;9=5;10=5;11=6;12=6")
#+END_SRC
*** subset by years
use the following code:
#+begin_src r
mod1_2003 <-subset(mod1, year == 2003)
mod1_2004 <-subset(mod1, year == 2004)
mod1_2005 <-subset(mod1, year == 2005)
mod1_2006 <-subset(mod1, year == 2006)
mod1_2007 <-subset(mod1, year == 2007)
mod1_2008 <-subset(mod1, year == 2008)
mod1_2009 <-subset(mod1, year == 2009)
mod1_2010 <-subset(mod1, year == 2010)
mod1_2011 <-subset(mod1, year == 2011)
#+end_src
*** create a date range/date time series
**** simple date sequence
#+begin_src R
bd <- as.Date("2007-05-20")
ed <- as.Date("2010-06-13")
seqd <- seq(bd, ed, by="1 day")
#+end_src

**** to create a date range based on start and end points use`
#+begin_src R
days_2000<-seq.Date(from = as.Date("2000-01-01"), to = as.Date("2000-12-31"), 1)
#+end_src
where the 1 at the end of the file specifies the increment , thats is increment by 1 day
**** to create a date range from a file containing dates

#+begin_src R
#extract date range
dayrange <- DATA[, range(date)]
#create date range
alldays <- seq.Date(from = as.Date(dayrange[1]), to = as.Date(dayrange[2]), 1)
#+end_src
*** create a full grid-day combo
#+begin_src R
#to create a date range based on start and end points use
days_2006<-seq.Date(from = as.Date("2006-01-01"), to = as.Date("2006-12-31"), 1)
#create date range
mod3grid <- data.table(expand.grid(guid = basegrid[, unique(guid)], day = days_2006))
#+end_src

where guid is the id variable to expand from dataset basegrid and days_2006 is the date range
*** Padding a Time Series 
#+BEGIN_SRC R
raw.data <- read.delim("timefill.csv", header=T, sep=",")

# Convert the time column to a date column.
# Accessing a column is done by using the '$' sign
# like so: raw.data$time.
raw.data$time <- as.Date(raw.data$time)

# sort the data by time. The [*,] selects all rows that 
# match the specified condition - in this case an order function
# applied to the time column.
sorted.data <- raw.data[order(raw.data$time),]

# Find the length of the dataset
data.length <- length(sorted.data$time)

# Find min and max. Because the data is sorted, this will be 
# the first and last element.
time.min <- sorted.data$time[1]
time.max <- sorted.data$time[data.length]

# generate a time sequence with 1 month intervals to fill in
# missing dates
all.dates <- seq(time.min, time.max, by="month")

# Convert all dates to a data frame. Note that we're putting 
# the new dates into a column called "time" just like the 
# original column. This will allow us to merge the data.
all.dates.frame <- data.frame(list(time=all.dates))

# Merge the two datasets: the full dates and original data
merged.data <- merge(all.dates.frame, sorted.data, all=T)

# The above merge set the new observations to NA.
# To replace those with a 0, we must first find all the rows
# and then assign 0 to them.
merged.data$observations[which(is.na(merged.data$observations))] <- 0
#+END_SRC
 
*** adding days to date
#+begin_src R
as.Date("2001-01-01") + 45
#+end_src

** Type conversions
*** basic

*Data Type Conversion*
 Type conversions in R work as you would expect. For example, adding a
character string to a numeric vector converts all the elements in the
vector to character. IE: Use as.foo to explicitly convert it.
 *as.numeric(), as.character(), as.factor(), as.vector(), as.matrix(),
as.data.frame()*
 [[R_files/img/Image_Ozbl1bP2hzHlrmemA836Pw_0003.png]]
 Functions of the form *is.*datatype()return TRUE or FALSE, whereas
*as.*datatype() converts the argument to that type.

*** convert from character to numeric (as.numeric)

use the as.numeric command:
 zinc$rec <- as.numeric(zinc$Study)

** Sorting with internal order command
*** sorting with sort command

Sorting Data
 To sort a dataframe in R, use the order( ) function. By default,
sorting is ASCENDING.
 Prepend the sorting variable by a minus sign to indicate DESCENDING
order.
 Here are some examples:
 # sorting examples using the mtcars dataset
 data(mtcars)
 # sort by mpg
 newdata <- mtcars[order(mpg),]
 # sort by mpg and cyl
 newdata <- mtcars[order(mpg, cyl),]
 #sort by mpg (ascending) and cyl (descending)
 newdata <- mtcars[order(mpg, -cyl),]
 *Example:*
 T2001c <- T2001c[order(T2001c$sitecode),]
 or for to sort by 2 variables:
 pmguidlumet <- pmguidlumet[order(pmguidlumet$guid,pmguidlumet$date),]

** Merging/appending Data
*** intro

**

Merge: Merge *adds variables* to a dataset. Merging two datasets require
that both have */at least/one variable in common* (either string or
numeric). If string make sure the categories have the same spelling
(i.e. country names, etc.). Explore each dataset separately before
merging. Make sure to use all possible common variables (for example, if
merging two panel datasets you will need country and years).
 *Append:* Append *adds cases/observations* to a dataset.
 Appending two datasets require that both have /exactly/the same number
of variables with /exactly/the same name. If using categorical data make
sure the categories on both datasets refer to /exactly/the same thing
(i.e. 1 "Agree", 2"Disagree", 3 "DK" on both).
 If datasets do not have the same number of variables you can either
drop or create them so both match.

*** Merge (Adding Columns)

**** basic

*Adding Columns*
 To merge two dataframes (datasets) horizontally, use the merge
function. In most cases, you join two dataframes by one or more common
key variables (i.e.,*an inner join*).
 Note: you need to [[#HcUzOY5wv0T3I7p0Qwp32A][sort]] the datasets by the
key variables first and then merge
 #sort
 DATA <- DATA[order(VAR),]
 DATA2 <- DATA2[order(VAR),]
 # merge two dataframes by VAR
 MERGED\_DATA <- merge(DATA,DATA2,by="VAR")
 another example of merging by 2 variables
 # merge two dataframes by ID and Country
 MERGED\_DATA <- merge(DATA,DATA2,by=c("VAR1","VAR2"))
  
 After the merge, column names for columns from the first table *have
the .x suffix*added, and *from the second the .y suffix*.
 the merge table only contains merged rows. If there are elements in the
common column of one table, but not the other, that partial data will
not be included in the merged table.

**** complex Join

*Joins Types (see [[#2UiapiqIesIbsHMW6CQCEw][HERE]] for a visual
example):*
 a full *inner join* (IE [[#l4n8Q9wnydky7nPoQ2wL4Q][Basic join]] -all
records from both tables) can be created with the "all" keyword, but
that can be used for other joins:
 **

Outer join: merge(df1, df2, all=TRUE)

 **

Left outer: merge(df1, df2, all.x=TRUE)

 **

Right outer: merge(df1, df2, all.y=TRUE)
 *Example for left join (if a in SAS):*
 zipyw<-merge(nezip,yaw,by=c("zipcode"),all.x=TRUE)
 this is equivalent to the sas command:
 data DATA3;
 merge DATA1(in=a) DATA2 (in=b) ;
   by BY1;
   if a;
 run;

**** different Join types

[[R_files/img/Image_2UiapiqIesIbsHMW6CQCEw_0004.png]][[R_files/img/Image_2UiapiqIesIbsHMW6CQCEw_0005.png]][[R_files/img/Image_2UiapiqIesIbsHMW6CQCEw_0006.png]]

**** join by variables with different names

You can use the by.x and by.y parameters if the matching variables have
different names in the different data frames.
 IE:
 MERGED\_DATA<- merge(DATA,DATA2, by.x = "VARNAME1", by.y = "VARNAME2")
 Pasted from
<[[http://stackoverflow.com/questions/1299871/how-to-join-data-frames-in-r-inner-outer-left-right][http://stackoverflow.com/questions/1299871/how-to-join-data-frames-in-r-inner-outer-left-right]]>

**** join (plyr)

Join (part of the plyr package) , like merge, is designed for the types
of problems where you would use a sql join.
 join(x, y, by = intersect(names(x), names(y)), type ="left", match =
"all")
 *where:*
 x    data frame
 y    data frame
 by    character vector of variable names to join by
 type    type of join: left (default), right, inner or full. See details
for more information.
 match    how should duplicate ids be matched? Either match just the
"first" matching row, or match "all" matching rows.
 *The four join types return:*
 inner: only rows with matching keys in both x and y
 left: all rows in x, adding matching columns from y
 right: all rows in y, adding matching columns from x
 full: all rows in x with matching columns in y, then the rows of y that
don't match x.
 Examples
 MERGED\_DATA<-join(ne,yw,by=c("zipcode"))
**** merge datasets with duplicates

#+begin_src R
tst<- merge(x, y[!duplicated(y[,c("aodid","day")]),], by=c("aodid","day"))                               
#+end_src

*** Append (adding rows)

**** Adding Rows (rbind)

Adding Rows
 To join two dataframes (datasets) vertically, use the rbind function.
 The two dataframes must have the same variables, but they do not have
to be in the same order.

total <- rbind(dataframeA, dataframeB)

 If dataframeA has variables that dataframeB does not, then either:
    1. Delete the extra variables in dataframeA or
    2. Create the additional variables in dataframeB and set them to NA
(missing)
 before joining them with rbind.
 IE:
 #merge datas into 1 set
 met09\_10 <- rbind(data1,data2,data3,data4,data5,data6)

**** cbind- joining two matrices or data frames horizontally

*Description*
 Take a sequence of vector, matrix or data frames arguments and combine
by columns or rows, respectively.
 IE:
 total <- cbind(A, B)
 This function will horizontally concatenate the objects A and B. For
the function to work properly,
 each object has to have the same number of rows and be sorted in the
same order.
 NOTE:its better to attach (and then detach) the object before using
cbind IE:
 attach(data\_means)
 disc\_raw
<-cbind(Zn.appm,SD.Zn.a,Fe.appm,SD.Fe.a,Fe.appm,SD.Fe.a,N.a,Zn.eppm,SD.Zn.e,Fe.eppm,SD.Fe.e,N.e)
 detach(data\_means)

*** concentrate data

To concentrate data from various objects/files use the "c" command
 Newfile <- *c*(obj1, obj2, ....)
 for example:

------------------------------------------------------------------------------------------------------------------
 ALL\_pred <- c(Xpred\_1, Xpred\_2, Xpred\_3, Xpred\_4, Xpred\_5,
Xpred\_6)

------------------------------------------------------------------------------------------------------------------

** Subsetting Data
*** Simple subset (subset function)

@NOTE: for equel you MUSR use the '==' sign (double equal) for the subset to work@

1.For simple subsets use either the subset function:

#+begin_src r
NEWDATA<- subset(DATA, VARIABLE OPERATOR VALUE)
#example
mod1_2003 <-subset(mod1, year == 2003)
#+end_src
Or internal which command:

#+begin_src r
NEWDATA <- OLDDATA[ which(OLDDATA$VAR== 2003) , ]
#example
mod1_2003 <- mod1[ which(mod1$year== 2003) , ]
#+end_src 
*** complex subsetting
In the following example, we select all rows that have a value of age greater than or equal to 20 or age less then 10.**We keep the ID and
Weight columns

#+begin_src R
newdata <- subset(mydata, age >= 20 | age < 10, select=c(ID, Weight)

#Select all men over the age of 25 and we keep variables weight through income (weight, income and all columns between them).

newdata <- subset(mydata, sex==*"m"* & age > 25,select=*weight:income*)

#+end_src

!NOTE:use "" for character variables use the & sign to add both conditions or the | sign to use OR!

There is no limit to how many logical statements may be combined to achieve the subsetting that is desired. The data frame x.sub1 contains
only the observations for which the values of the variable y is greater than 2 and for which the variable V1 is greater than 0.6.

x.sub1 <- subset(x.df, y > 2 & V1 > 0.6)

Pasted from
<[[http://www.statmethods.net/management/subset.html][http://www.statmethods.net/management/subset.html]]>

*** subsetting by rows

to select specific rows in a dataset just define the range of rows you
want and leave the columns empty:
 *NEWDATA <- ORIGDATA[ROWY:ROWX,]
*
 *example:*
 ICAM_diab_g <- ICAM_diab[11:16,]
 Where:
 11:16 - tells R to subset from row 11 to row 16

*** 'subset dataset using a vector/variable'

use this format if you have a common key variable:

#+BEGIN_EXAMPLE
dataset[(dataset$key %in% mylist),]
#or the opposite to exclude the data
dataset[!(dataset$key %in% mylist),]
#+END_EXAMPLE

The following code allows you to subset observations in a dataset using a column of the data frame or using a single vector.

#+BEGIN_EXAMPLE
basegridX<- basegrid[basegrid$guid %in% sa$guid, ]        
#+END_EXAMPLE

where: 'basegrid' is our dataset
'guid' is the key variable we use to subset in data
'sa$guid' is the external data that include only the subset we want ('guid')


In example:

#+BEGIN_SRC R
tst<- day2011[!(day2011$guid %in% x.sub2$guid),]
#+END_SRC

`alternative`:
another way is to manually create a list first and then exclude using an index

#+BEGIN_SRC R
list<-c(1:31,index)
a2<-subset(data.set,select=list) 
#+END_SRC

*** Selecting (Keeping) Variables

*Selecting (Keeping) Variables*
 example:
 newdata <- leadership[, c(6:10)]
 *or for non consecutive variables:*
 day2001 <- F\_T2001\_All[, c(4:5,7,9,17:19)]
 Note: that we are selecting columns by position IE above columns 4 to 5
then 7,9 etc
 selects variables q1, q2, q3, q4, and q5 from the leadership data frame
and saves them to the data frame newdata . Leaving the row indices blank
(,) selects all the rows by default.
 The statements
 myvars <- c("q1", "q2", "q3", "q4", "q5")
 newdata <-leadership[myvars]
 accomplish the same variable selection. Here, variable names (in
quotes) have been entered as column indices, thereby selecting the same
columns. Finally, you could've used
 myvars <- paste("q", 1:5, sep="")
 newdata <- leadership[myvars]
 This example uses the paste() function to create the same character
vector as in the previous example.
 *Other examples:
*
 # select variables v1, v2, v3
 myvars <- c("v1", "v2", "v3")
 newdata <- mydata[myvars]
 # another method
 myvars <- paste("v", 1:3, sep="")
 newdata <- mydata[myvars]
 # select 1st and 5th thru 10th variables
 newdata <- mydata[c(1,5:10)]

*** Excluding (DROPPING-deleting) Variables

1.Excluding single Variable
 dataframe$VAR <- NULL completely removes the variable.
 2.exclude multiple variables
 #*exclude* variables (this excludes *VAR1* and *VAR2* from newdata)
 DELLIST <- names(DATASETNAME) %in% c("*VAR1*", *"VAR2*")
 newdata <- DATASETNAME[*!*DELLIST]
 altentate way to exclude multiple variables
 # exclude all variable but the 66 to 76 variables
 newdata <- OLDDATA[c(66:76)]
 3.Include multiple variables
 #*include* variables (this only includes *VAR1* and *VAR2* in newdata)
 DELLIST <- names(DATASETNAME) %in% c("*VAR1*", *"VAR2*")
 newdata <- DATASETNAME[DELLIST]
 *examples:
*
 DELLIST <- names(grid\_2003) %in% c("OBJECTID.x", "Id.x", "X.x", "Y.x")
 grid\_2003 <- grid\_2003[!DELLIST]

*** Random Samples

*Random Samples*
 Use the sample( ) function to take a random sample of size n from a
dataset.
 # take a random sample of size 50 from a dataset mydata
 # sample without replacement
 mysample <- mydata[sample(1:nrow(mydata), 50,
    replace=FALSE),]
 *to define a sample by size (number) use:*
 NEW\_sample <- DATA[sample(nrow(DATA), Number), ]
 for example
 ten\_per\_T2000 <- F\_T2000\_All[sample(nrow(F\_T2000\_All), 200), ]

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
 *to define a sample by % use:*
 ten\_per\_T2000 <- F\_T2000\_All[sample(nrow(F\_T2000\_All),
0.1*nrow(F\_T2000\_All)), ]

*** Selecting Observerations

*Selecting Observerations*
 # first 5 observerations
 newdata <- mydata[1:5,]
 # based on variable values
 newdata <- mydata[ which(mydata$gender=='F'& mydata$age > 65), ]
 # or
 attach(newdata)
 newdata <- mydata[ which(gender=='F' & age > 65),]
 detach(newdata)

*** subsetting data in a model
**** lm

#+begin_src R
p.model3<-lm(bp1sys~ age+as.factor(booze) , na.action=na.omit, data=hanes1b,subset=(sex==0))
 #+end_src
where we subset only for the sex=0 group

**** glm

example:
 try\_inc\_l0\_low\_inc <- (glmmPQL(count ~
ns(date,df=35)+temp\_f\_l0+pmnew\_l0+pmnewmayear+pmnewmayear+Avg\_pctnoh+Avg\_per\_mi+Avg\_p\_A65+ran,
random = ~ 1 | guid, family = poisson,*subset(ts0004lag,
ts0004lag$inc\_bin\_m == 1)*, data = ts0004lag))

**** lme

*In LME*
 use this:
 subset=VAR==4,
 IE:
 out.model\_zinc <- lme( Znppm ~ CO , na.action=na.omit,
 random = ~1 | Paircount, subset=crop_type==4,
 data = zinc)

**** gam

example:
 bp.model.ps<-gam(resid\_pm~s(totrdlth\_m,fx=FALSE,k=10,bs='cr')+delev+
 dpopden+durban+dist\_point,sp=1,
 na.action=na.omit,data=res\_all,subset=(totrdlth\_m<=9000))

*** create a subset of only NA data
extract only missing values
#+BEGIN_SRC R
tst<-m2_2008[is.na(m2_2008$predicted)]
#+END_SRC
or
#+BEGIN_SRC R
data1 <- !is.na(age)&!is.na(bmi)
#+END_SRC
where:
! is the non selected
is.na is the values of missing
so !is.na is take all non missing

*** create a subset of variables

#+begin_src R
xx<- names(data1) %in% c("pmnewma2","fintempma2","med_income","p_ospace")
yy <- data1[xx]
#+end_src
 
*** Create subset of data and delete missing

1. Create subset of data and delete missing
 #create a indicator vector with the number(position in dataframe) of
variables you want to keep

 ind<-c(2,3,4,5,12,13,14,15,20)

 #using that indicator vector subset the data from the original
dataframe(hanes1b) to the new subsetted one (hanes)
 hanes<-hanes1b[ind]
 #use the na.omit command to clear all missing values
 hanes<-na.omit(hanes)
 #see results
 dim(hanes)
 names(\hanes)

*** subset by date

1. make sure the date field is [[#7db9ae9Qz4wQZm0915NJ1A][converted to
the standard R]] date
 2. issue the followiing command
 *NEWDATA <-subset(FULLDATA, as.Date(DATEFIELD) >= 'DATERANGE' &
as.Date(DATEFIELD) <= 'DATERANGE')*
 IE:
 mb4 <-subset(mb3, as.Date(rdate) >= '2003-09-02' & as.Date(rdate) <=
'2004-09-04')

*** Create unique dataset
#+begin_src R
grid <- unique(ndvi, by="ndviid")
#+end_src
** character/string variables manipulation
*** combine 2 string (text) variables

#+BEGIN_SRC emacs-lisp :results none
test$date<-paste(test$month,test$year,sep="/")
#+END_SRC

*** concentrate parts of time variables to date

to concentrate variables containing parts of dates use:
 T2009sr$date <- paste(T2009sr$d,T2009sr$m,T2009sr$c, sep="/")

*** extract part of a text variable

use the `substr` function

#+BEGIN_SRC R
#extract the 5th and 6th character from start
test$month<- substr(test$V1, 5, 6)
#extact the starting character until the 4th but as a numeric variable
test$year<- as.numeric(substr(test$V1, 1, 4))
#+END_SRC
*** remove whitespace from string variables
Probably the best way is to handle the trailing whitespaces when you read your data file. If you use read.csv or read.table you can set the parameter
`strip.white=TRUE`

If you want to clean strings afterwards you one of these functions:

#+BEGIN_EXAMPLE
# returns string w/o leading whitespace
trim.leading <- function (x)  sub("^\\s+", "", x)

# returns string w/o trailing whitespace
trim.trailing <- function (x) sub("\\s+$", "", x)

# returns string w/o leading or trailing whitespace
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
#+END_EXAMPLE

To use one of these functions on myDummy$country:

#+BEGIN_SRC R
myDummy$country <- trim(myDummy$country)

#To 'show' the whitespace you could use:

paste(myDummy$country)
#+END_SRC


 
** Functions

*** Character Functions

[[R_files/img/Image_2ESMMUL5R1Gy33GwZp53PQ_0002.jpg]]

*** Numeric Functions

Function Description:

#+BEGIN_SRC R
abs(x) absolute value
sqrt(x) square root
ceiling(x) ceiling(3.475) is 4
floor(x) floor(3.475) is 3
trunc(x) trunc(5.99) is 5
round(x, digits=n) round(3.475, digits=2) is 3.48
signif(x, digits=n) signif(3.475, digits=2) is 3.5
cos(x), sin(x), tan(x) also acos(x), cosh(x), acosh(x), etc.
log(x) natural logarithm
log10(x) common logarithm
exp(x) e\^x
#+END_SRC

*** repeats and sequences

[[R_files/img/Image_OrrXIErI5QCR0MHSot5cLw_0002.jpg]]

*** Statistical Functions

﻿Other useful statistical functions are provided in the following table.
Each has the option na.rm to strip missing values before calculations.
Otherwise the presence of missing values will lead to a missing result.
Object can be a numeric vector or dataframe.
 [[R_files/img/Image_VxnHeZQAbWHn8w4MIaAyTg_0002.jpg]]

*** Statistical Probability Functions

﻿Statistical Probability Functions
 [[R_files/img/Image_5X9kvCTRA2g2DdYRMDgSUw_0002.jpg]]

** IF-Else (Control Structures)

*** creare a new vector of recoded data

Jan=ifelse(DATA$Month==1,1,0)
 creates a Jan object where if the month is 1 then it gets 1 otherwise 0

*** create NA from data

Temp<-ifelse(temp==0,NA,temp)
 this will make all temp values of '0' become NA (missing)

*** Intro

*﻿Control Structures*
 R has the standard control structures you would expect. expr can be
multiple (compound) statements by enclosing them in braces { }. It is
more efficient to use built-in functions rather than control structures
whenever possible.
 *
 if-else*
 if (cond) expr
 if (cond) expr1 else expr2
 *for*
 for (var in seq) expr
 *while*
 while (cond) expr
 *switch*
 switch(expr, ...)
 *
 ifelse*
 ifelse(test,yes,no)
 *
 Examples*
 # transpose of a matrix
 # a poor alternative to built-in t() function
 mytrans <- function(x) {
 if (!is.matrix(x)) {
 warning("argument is not a matrix: returning NA")
 return(NA\_real\_)
 }
 y <- matrix(1, nrow=ncol(x), ncol=nrow(x))
 for (i in 1:nrow(x)) {
 for (j in 1:ncol(x)) {
 y[j,i] <- x[i,j]
 }
 }
 return(y)
 }
 # try it
 z <- matrix(1:10, nrow=5, ncol=2)
 tz <- mytrans(z)

** levels

*** Changing the levels of a factor

Changing the levels of a factor can be done using the levels function:
 #change the levels of a factor
 levels(veteran$celltype) <- c("s","sc","a","l")

*** find out how many levels a variable has

to find out how many levels a variable has issue:
 sapply(DATA, nlevels)

*** intro

Levels - an integer vector that specifies the permitted levels of a
category object. These may be altered with the levels() function.

*** relevel-Reorder Levels of Factor

Description
 The levels of a factor are re-ordered so that the level specified by
ref is first and the others are moved down. This is useful for
contr.treatment contrasts which take the first level as the reference.
 Usage
 relevel(x, ref, ...)
 Arguments
 x An unordered factor.
 ref The reference level.
 ... Additional arguments for future methods.
 example:
 warpbreaks$tension <- relevel(warpbreaks$tension, ref="M")

** spliting datasets

*** split into 90-10% cross validation sets

to split a data-set for cross validationv into 10% and 90% sets do the
following:
1)create this functions:

#+begin_src R
splitdf <- function(dataframe, seed=NULL) {
if (!is.null(seed)) set.seed(seed)
index <- 1:nrow(dataframe)
trainindex <- sample(index, trunc(length(index)/10))
trainset <- dataframe[trainindex, ]
testset <- dataframe[-trainindex, ]
list(trainset=trainset,testset=testset)
}
#+end_src

where: the `10` is the number to split by (10 is 10%, 2 is 50% etc..)

2)run the function (apply the function) with your dataset:
#+BEGIN_SRC sh
#apply the function
splits_s1 <- splitdf(m1_2003)
#+END_SRC

$Note you can add a seed option (seed=) and put a number to get a less random result$
3) this returns a list - two data frames called 'trainset' and 'testset'
4) you can now convert the list to the datasets

#+begin_src R
mod1d_10_s1 <- splits_s1$trainset
mod1d_90_s1 <- splits_s1$testset
#+end_src
** Sequences
*** create a ID variable
#+begin_src R
mydata$id<-seq(1,9,by=1)
#+end_src
where, it starts at '1' goes up to '9' and increments by 1 value each time. one can also used the dim of dataset to create a sequence and more. see examples:

#+begin_src R
d1<-dim(mydata)f
mydata$id<-seq(1,d1[1],by=1)
#where we extract the length from the data dimensions
#+end_src

** Using with( ) and by( )

*Using with( ) and by( )*
 There are two functions that can help write simpler and more efficient
code.
 *With*
 The with( ) function applys an expression to a dataset. It is similar
to DATA= in SAS.
 # with(data, expression)
 # example applying a t-test to dataframe mydata
 with(mydata, t.test(y1,y2)
 *By*
 The by( ) function applys a function to each level of a factor or
factors. It is similar to BY processing in SAS.
 # by(data, factorlist, function)
 # example apply a t-test separately for men and women
 by(mydata, gender, t.test(y1,y2))

** Series
*** create a full gird/date series
to create a grid per day series (like mod3 in the PM models) use
something like this:

#+begin_src r
alldays <- seq.Date(from = as.Date("2003-01-01"), to = as.Date("2012-12-31"), 1)
fakeallaodid <- 1:10500
test <- expand.grid(alldays, fakeallaodid)
#+end_src
where alldays is the date range
and fakeallaodid is the variable you want to populate the dates for
finally the last line puts them all together
%Tip:if you want to add X,Y you can merge back by ID%

** conversions

*** convert from farenhight to celsius

convert from farenhight to celsius:
 NEWVAR

=(5/9)*(VARIABLE-32)
** Expanding data
*** create a full grid with every day
#+begin_src R
#create full grid
mod3grid <- data.table(expand.grid(GRIDVAR = DATA[, unique(GRIDVAR)], NAME_NEW_DATE_VAR = RANGE_DATE_DATA))
#+end_src

₆In example₆ 

#+begin_src R
#extract date range
dayrange <- wtst[, range(day)]
#create date range
alldays <- seq.Date(from = as.Date(dayrange[1]), to = as.Date(dayrange[2]), 1)
#create full grid
mod3grid <- data.table(expand.grid(guid = grid[, unique(guid)], day = alldays))
dim(mod3grid)
#+end_src
** Sampling
*** create a sample from a list
#+begin_src R
sample(DF, 3)
#+end_src

** rounding
*** round up variable
#+BEGIN_SRC R
paper_table$bw.crude.270<-round(paper_table$bw.crude.270,4)
#+END_SRC

* Control flow (functions/loops/macros)
** Basic
statement is a single R statement or a compound statement (a group of statements enclosed in curly braces { } and separated by semicolons)
`cond` is an expression that resolves to true or false.
`expr` is a statement that evaluates to a number or character string.
`seq` is a sequence of numbers or character strings

** Repetition and looping
*** basics

Looping constructs repetitively execute a statement or series of
statements until a condition isn't true. These include the for and while
structures.

*** For
The `for` loop executes a statement repetitively until a variable's
value is no longer contained in the sequence seq. The syntax is

#+BEGIN_EXAMPLE
for(i in values){
  ... do something ...
}
#+END_EXAMPLE

This for loop consists of the following parts:

The keyword `for`, followed by parentheses.

An identifier between the parentheses. In this example, we use `i`, but that can be any object name you like.

The keyword `in`, which follows the identifier.

A vector with values to loop over. In this example code, we use the object values, but that again can be any vector you have available.

A code block between braces (`{`) that has to be carried out for every value in the object values.

In the code block, you can use the identifier. Each time R loops through the code, R assigns the next value in the vector with values to the identifier.

*** While

WHILE
 A *while* loop executes a statement repetitively until the condition is
no longer true. The syntax is
 *while (cond) statement*
 In a second example, the code
 i <- 10
 while (i > 0) {print("Hello"); i <- i - 1}
 once again prints the word Hello 10 times. Make sure that the
statements inside the
 brackets modify the while condition so that sooner or later it's no
longer true---other wise the loop will never end!
 In the previous example, the statement
 i <- i - 1
 subtracts 1 from object i on each loop, so that after the tenth loop
it's no longer larger than 0. If you instead added 1 on each loop, R
would never stop saying Hello. This is why while loops can be more
dangerous than other looping constructs.
 Looping in R can be inefficient and time consuming when you're
processing the rows or columns of large datasets. Whenever possible,
it's better to use R's builtin numerical and character functions in
conjunction with the apply family of functions.

*** loop examples

Create a loop for a number series:
 #loop will go from 2000-2008
 for (i in 2000:2008) print(i)
 create a loop from a vector containing values:
 #create vector
 modis\_years <- c(2000,2001,2002,2003,2004,2005,2006,2007,2008)
 #use that vector as loop
 for (i in modis\_years) print(i)
 *look at the following loop example:*
 for (*i* in *1*: ncol(t.mat))*{*
 for (*j* in *1*: ncol(s.mat))*{*
 st.mat <- cbind (st.mat, t.mat[,i]*s.mat[,j])
 *}
 }
 where:
 {* srarts the loop*
 }*ends the loop
 this example states the for every *i* (every column) from *1* to all
number of columns in the t.mat dataset

** Conditional execution
*** if-else

In conditional execution, a statement or statements are only executed if
a specified condition is met. These constructs include if-else, ifelse,
and switch.
 IF-ELSE
 The *if-else* control structure executes a statement if a given
condition is true. Optionally, a different statement is executed if the
condition is false. The syntax is
 if (cond) statement
 if (cond) statement1 else statement2
 *Here are examples:
*
 if (is.character(grade)) grade <- as.factor(grade)
 if (!is.factor(grade)) grade <- as.factor(grade) else print("Grade
already
 is a factor")
 In the first instance, if grade is a character vector, it's converted
into a factor. In the second instance, one of two statements is
executed. If grade isn't a factor (note the ! symbol), it's turned into
one. If it is a factor, then the message is printed.

*** ifelse

IFELSE
 The ifelse construct is a compact and vectorized version of the if-else
construct . The syntax is
 *ifelse(cond, statement1, statement2)*
 The first statement is executed if cond is TRUE. If cond is FALSE, the
second statement is executed. Here are examples:
 ifelse(score > 0.5, print("Passed"), print("Failed"))
 outcome <- ifelse (score > 0.5, "Passed", "Failed")
 Use ifelse when you want to take a binary action or when you want to
input and out-
 put vectors from the construct.

*** switch

SWITCH
 *switch* chooses statements based on the value of an expression. The
syntax is
 *switch(expr, ...)*
 where ... represents statements tied to the possible outcome values of
expr. It's easiest
 to understand how switch works by looking at the example in the
following listing.
 [[R_files/img/Image_soOFWEh1vZj3aSQBCnxXfA_0002.png]]

** User-written functions
*** Intro

One of R greatest strengths is the users ability to add functions. In
fact, many of the
 functions in R are functions of existing functions. The structure of a
function looks
 like this:
 myfunction <- function(arg1, arg2, ... ){
 statements
 return(object)
 }
 Objects in the function *are local to the function*. The object
returned can be any data type, from scalar to list.

* Descriptive Statistics
** Quantitative variables
*** Basic discriptives
**** internal commands
***** Summary

*summary*
 The summary() function provides the minimum, maximum, quartiles, and
the mean
 for numerical variables and frequencies for factors and logical
vectors:
 # mean,median,25th and 75th quartiles,min,max
 summary(mydata)
 [[R_files/img/Image_r2az5xIGMUyQohTPznwNgA_0002.png]]

***** fivenum

*fivenum
*
 The function fivenum() returns Tukey's five-number summary (minimum,
lower-hinge, median, upper-hinge,
 and maximum). *
*# Tukey min,lower-hinge, median,upper-hinge,max
 fivenum(x)

***** sapply

*sapply( )
*
 One method of obtaining descriptive statistics is to use the sapply( )
function with a specified summary statistic.
 *This only works for one statistic function at a time!! (mean,SD etc)
*
 For the*sapply()* function, the format is
 *sapply(x, FUN, options)*
 where x is your data frame (or matrix) and FUN is an arbitrary
function. If options are
 present, they're passed to FUN. Typical functions that you can plug in
here are mean,
 sd, var, min, max, median, length, range, and quantile.
 # get means for variables in dataframe mydata
 # excluding missing values
 sapply(mydata, mean, na.rm=TRUE)
 Possible functions used in sapply include mean, sd, var, min, max, med,
range, and quantile.

***** summary subseted

summary(nas1$bc_pred[nas1$Year == 1995])
**** external packages
***** Describe (Hmisc)
****** base
The describe() function in the Hmisc package returns the number of variables and observations, the number of missing and unique values, the mean, quantiles, and the five highest and lowest values.
 
#+BEGIN_SRC R
library(Hmisc)
describe(mydata)
# n, nmiss, unique, mean, 5,10,25,50,75,90,95th percentiles
# 5 lowest and 5 highest scores
#+END_SRC
****** limit digits in output 
use the dig=switch
#+BEGIN_SRC R
describe(bda$sga,dig=2)
#+END_SRC
****** describe only a subset of the data1
#+BEGIN_SRC R
describe(bd$birthw[bd$mrn.n == 4])
#+END_SRC
****** Get percents in dichotomous variable (0,1)

#+BEGIN_SRC R
#$Note$ that you may need to manually run is at a factor to make hmisc know its not continous
Hmisc::describe(as.factor(bd$lungd))
#+END_SRC
results:
#+BEGIN_EXAMPLE
as.factor(bd$lungd) 
      n missing  unique 
 452316    1342       2 

0 (437753, 97%), 1 (14563, 3%) 
#+END_EXAMPLE

****** exclude missing
#+BEGIN_SRC R
 exclude.missing=TRUE
#+END_SRC

***** pastecs
gives the following:

#+BEGIN_EXAMPLE
# nbr.val, nbr.null, nbr.na, min max, range, sum,
# median, mean, SE.mean, CI.mean, var, std.dev, coef.var 
#+END_EXAMPLE

example:
use `stat.desc`
#+BEGIN_SRC R
stat.desc(bd$ta270)
          nbr.val          nbr.null            nbr.na               min               max             range               sum            median 
 453658.000000000       0.000000000       0.000000000      -0.419630007       8.376250511       8.795880518 1844219.041496962       4.122560722 
             mean           SE.mean      CI.mean.0.95               var           std.dev          coef.var 
      4.065218825       0.001191507       0.002335318       0.644053584       0.802529491       0.197413602 
 
#+END_SRC

***** psych package

# item name ,item number, nvalid, mean, sd,
# median, mad, min, max, skew, kurtosis, se


#+BEGIN_SRC R
library(psych)
describe(bd$ta270)
  vars      n mean  sd median trimmed  mad   min  max range  skew kurtosis se
1    1 453658 4.07 0.8   4.12    4.13 0.74 -0.42 8.38   8.8 -1.15     2.43  0
#+END_SRC


$Note$ see [[*Solve%20the%20masking%20problmes][Solve the masking problmes]] to address same name function in packages

***** Solve the masking problmes

The packages psych and Hmisc both provide functions named 'describe()'. Simply put, the package last loaded takes precedence.
If psych is loaded after Hmisc, a message is printed indicating that the describe() function in Hmisc is masked by the function in psych. When you type in the describe() function and R searches for it, R comes to the psych package first and executes it. If you want the Hmisc version instead, you can type

#+BEGIN_EXAMPLE
Hmisc::describe(mt)
#+END_EXAMPLE

The function is still there. You have to give R more information to find it.

**** zeltak example of full EPI discriptives

#+BEGIN_SRC R
Hmisc::describe(bd$birthw)
describe(bd$cig_pre)
describe(bd$cig_preg)
describe(bd$tden)
describe(bd$med_income)
describe(bd$p_ospace)  
describe(bd$cig_pre)
describe(bd$gender)  
describe(bd$prev_400)
describe(bd$diab)
describe(bd$hyper) 
describe(bd$lungd)
describe(bd$diab_other)
describe(bd$prevpret)
stat.desc(bd$kess)  
describe(bd$mrn.n)
Hmisc::describe(as.factor(bd$edu_group))
describe(bd$byob)     
describe(bd$parity)
Hmisc::describe(as.factor((bd$mrn.n))
describe(bd$ges_calc)
describe(bd$elev)
describe(bd$parity)

Hmisc::describe(as.factor(bd$diab_other))
describe(bd$birthw[bd$diab_other == 1])

Hmisc::describe(as.factor(bd$diab))
describe(bd$birthw[bd$diab == 1])

Hmisc::describe(as.factor(bd$prev_400))
describe(bd$birthw[bd$prev_400 == 1])

Hmisc::describe(as.factor(bd$prevpret))
describe(bd$birthw[bd$prevpret == 1])

Hmisc::describe(as.factor(bd$hyper))
describe(bd$birthw[bd$hyper == 1])

Hmisc::describe(as.factor(bd$lungd))
describe(bd$birthw[bd$lungd == 1])
#+END_SRC
 
*** Descriptive statistics by group
**** use the aggregate()

You can use the *aggregate()*function to obtain descriptive statistics
by group, as shown in the following listing:
 [[R_files/img/Image_zxet55Lywh2XQRtTnzenVw_0003.png]]
 *Note:* the use of
 list(am=mtcars$am)
 If you had used list(mtcars$am), the am column would have been labeled
Group.1 rather than am. You use the assignment to pro-
 vide a more useful column label. If you have more than one grouping
variable, you can use code like:
 by=list(name1=groupvar1, name2=groupvar2, ... , groupvarN).

**** Doby package
***** Base
The doBy package provides much of the functionality of SAS PROC SUMMARY.
The `summaryBy()` function in the doBy package has the format

#+BEGIN_EXAMPLE
summaryBy(formula, data=dataframe, FUN=function)
where the*formula* takes the form:

var1 + var2 + var3 + ... + varN ~ groupvar1 + groupvar2 + ... +groupvarN

Variables on the left of the ~ are the numeric variables to be analyzed
Variables on the right are categorical grouping variables
#+END_EXAMPLE

The function can be any [[#6WidyD3pF97CtIhGJoBD1g][built-in]] or user created R function.
It is possible use a vector of predefined functions. A typical usage will be by invoking a list of predefined functions:

#+BEGIN_SRC R
library(doBy)
summaryBy(uptake~Plant, data=CO2, FUN=c(mean,var,median))
#+END_SRC

***** 'Calculate counts per ID variable'				:fav:
#+BEGIN_SRC R
library(doBy)
tden$count<-1
summaryBy(count~guid, data=tden, FUN=sum)
#+END_SRC

***** using functions:

library(doBy)
# produces mean and SD for birth weight and mean30 for each level of new_race

#+BEGIN_EXAMPLE
summaryBy( birthw+mean30~ new\_race, data = bw,FUN = function(x) { c(m = mean(x), s = sd(x)) } )

#+END_EXAMPLE

also it can be written like this with defining the function first:

#+BEGIN_EXAMPLE
myfun1 <- function(x){c(m=mean(x), v=var(x))}
summaryBy(conc+uptake~Plant, data=CO2,
FUN=myfun1)
#+END_EXAMPLE

**** using psych

The describe.by() function contained in the psych package provides the
same descriptive statistics as describe, stratified by one or more
grouping variables, as you can see in the following listing:
 [[R_files/img/Image_krQjuTlWhWYb1i0LIMlx0Q_0003.png]]
 Unlike the previous example, the describe.by() function doesn't allow
you to specify an arbitrary function, so it's less generally applicable.
If there's more than one grouping variable, you can write them as
list(groupvar1, groupvar2, ... , groupvarN). But this will only work if
there are no empty cells when the grouping variables are crossed
** Frequncies
*** using ddply
**** get frequnecy of observations per monitor
#+begin_src R
$Note$-this uses data.table syntax
#group by station
table_temp<-ddply(na.omit(temp[,c("Temp","stn","c"),with=F]),.(stn,c),nrow)


#group by station and year
table_temp<-ddply(na.omit(temp[,c("Temp","stn","c"),with=F]),.(stn,c),nrow)
#+end_src

$Note there is a Data.table way of doing it, look:$

 
*** using Hmisc
you can also use [[*Get%20percents%20in%20dichotomous%20variable%20(0,1)][Hmisc]] to get simple freq like percents. 
** Counts
*** count number of occurances per variable
#+begin_src R
count(df,"VAR")
#+end_src

** Categorical variables

*** Basic

*Generating Frequency Tables*
R provides many methods for creating frequency and contingency tables.
[[R_files/img/Image_wo5vv0tDDExxafUfvb8bpw_0003.png]]

In the following examples, assume that A, B, and C represent
categorical variables.

*** One way tables

You can generate simple frequency counts using the table() function.
TABLE <- with(DATASET,table(CAT_VAR))

NOTE: The table() function ignores missing values (NAs) by default. To
include NA as a valid category in the frequency counts, include the
table option useNA="ifany".*
 IE:
mytable <- with(F_T2000_All, table(reg_id))

#+BEGIN_SRC R
mytable <- with(Arthritis, table(Improved))
prop.table(mytable)
prop.table(mytable)*100
#+END_SRC

*** two-way tables

For two-way tables, the format for the *table()* function is
 *mytable <- table(A, B)*
 *
 NOTE: The table() function ignores missing values (NAs) by default. To
include NA as a valid category in the frequency counts, include the
table option useNA="ifany".*
 where A is the row variable, and B is the column variable.
Alternatively, the *xtabs()* function allows you to create a contingency
table using formula style input. The format is
 *mytable <- xtabs(~ A + B, data=mydata)*
 where mydata is a matrix or data frame. In general, the variables to be
cross-classified appear on the right of the formula (that is, to the
right of the ~) separated by + signs. If a variable is included on the
left side of the formula, it's assumed to be a vector of frequencies
(useful if the data have already been tabulated).
 [[R_files/img/Image_uBxiC9fZqIOqOlPmtq3Crg_0005.png]]
 You can generate marginal frequencies and proportions using the
margin.table() and prop.table() functions, respectively. For row sums
and row proportions
 [[R_files/img/Image_uBxiC9fZqIOqOlPmtq3Crg_0006.png]]
 The index (1) refers to the first variable in the table() statement.
Looking at the table, you can see that 51 percent of treated individuals
had marked improvement, compared to 16 percent of those receiving a
placebo.For column sums and column proportions
 Here, the index (2) refers to the second variable in the table()
statement. Cell proportions are obtained with this statement:
 > prop.table(mytable)
 Improved
 Treatment None Some Marked
 Placebo 0.3452 0.0833 0.0833
 Treated 0.1548 0.0833 0.2500
 You can use the addmargins() function to add marginal sums to these
tables. For
 example, the following code adds a sum row and column:
 > addmargins(mytable)
 Improved
 Treatment None Some Marked Sum
 Placebo 29 7 7 43
 Treated 13 7 21 41
 Sum 42 14 28 84
 > addmargins(prop.table(mytable))
 Improved
 Treatment None Some Marked Sum
 Placebo 0.3452 0.0833 0.0833 0.5119
 Treated 0.1548 0.0833 0.2500 0.4881
 Sum 0.5000 0.1667 0.3333 1.0000
 When using addmargins(), the default is to create sum margins for all
variables in a
 table. In contrast:
 > addmargins(prop.table(mytable, 1), 2)
 Improved
 Treatment None Some Marked Sum
 Placebo 0.674 0.163 0.163 1.000
 Treated 0.317 0.171 0.512 1.000
 adds a sum column alone. Similarly,
 > addmargins(prop.table(mytable, 2), 1)
 Improved
 Treatment None Some Marked
 Placebo 0.690 0.500 0.250
 Treated 0.310 0.500 0.750
 Sum 1.000 1.000 1.000
 adds a sum row. In the table, you see that 25 percent of those patients
with marked
 improvement received a placebo.

*** gmodels

A third method for creating two-way tables is the CrossTable() function
in the gmodels package. The CrossTable() function produces two-way
tables modeled after PROC FREQ in SAS
 *# 1-Way Cross Tabulation*
 to get simple precentages use just one variable:
 CrossTable(DATA$VARIABLE)
 *# 2-Way Cross Tabulation
*library(gmodels)
 CrossTable(DATA$ROWVAR, DATA$COLVAR)
 [[R_files/img/Image_dQ1YnuXtN79ICJRSA1LgYw_0003.png]]
 The CrossTable() function has options to report percentages (row,
column, cell); specify decimal places; produce chi-square, Fisher, and
McNemar tests of independence; report expected and residual values
(Pearson, standardized, adjusted standardized); include missing values
as valid; annotate with row and column titles; and format as SAS or SPSS
style output. See help(CrossTable) for details. to get frequencies the
easiest way to do it is use the gmodels package

*** Count unique values in R

the best way is to use the table function
 lets look at an example:
 #generate dummy data
 dummyData = rep(c(1,2, 2, 2), 25)
 > table(dummyData)
 dummyData
 1 2
 25 75
 #or another presentation of the same data
 > as.data.frame(table(dummyData))
 dummyData Freq
 1 1 25
 2 2 75
 Pasted from
<[[http://stackoverflow.com/questions/4215154/count-unique-values-in-r][http://stackoverflow.com/questions/4215154/count-unique-values-in-r]]>

** IQR
*** get basic IQR of variable

issue the internal command:
 IQR(x) #Interquartile range of x
 IE:
 IQR(DATA$VARIABLE)

** create a discripitive stat table for multiple years

*see this example code:*
 #create table
 cvtable <- data.frame(type=character(23),
r2000=numeric(23),r2001=numeric(23),r2002=numeric(23),r2003=numeric(23),r2004=numeric(23),r2005=numeric(23),r2006=numeric(23),r2007=numeric(23),r2008=numeric(23))
 cvtable$type <-
c("elevmin","elevmax","popmin","popmax","openmin","openmax","dismin","distmax","pmin","pmax","amin","amax","vismib","vismax","windmin","windmax","hummin","hummax","tempmin","tempmax","aodmin","aodmax","")
 #import mod1 DB (PM-AOD)
 F\_T2000\_All <-
read.csv("c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.1.Projetcs/3.1.2.MIA\_PM\_MODELS/3.1.2.4.Work/3.Analysis/mod1/T2000.csv",
header=T)
 cvtable$r2000[1] <-min(F\_T2000\_All$ELEV)
 cvtable$r2000[2] <-max(F\_T2000\_All$ELEV)
 cvtable$r2000[3] <-min(F\_T2000\_All$pop\_sqkm)
 cvtable$r2000[4] <-max(F\_T2000\_All$pop\_sqkm)
 cvtable$r2000[5] <-min(F\_T2000\_All$p\_open)
 cvtable$r2000[6] <-max(F\_T2000\_All$p\_open)
 cvtable$r2000[7] <-min(F\_T2000\_All$A1\_dist\_km)
 cvtable$r2000[8] <-max(F\_T2000\_All$A1\_dist\_km)
 cvtable$r2000[9] <-min(F\_T2000\_All$point\_pm)
 cvtable$r2000[10] <-max(F\_T2000\_All$point\_pm)
 cvtable$r2000[11] <-min(F\_T2000\_All$area\_pm )
 cvtable$r2000[12] <-max(F\_T2000\_All$area\_pm )
 cvtable$r2000[13] <-min(F\_T2000\_All$VISIB )
 cvtable$r2000[14] <-max(F\_T2000\_All$VISIB )
 cvtable$r2000[15] <-min(F\_T2000\_All$WDSP)
 cvtable$r2000[16] <-max(F\_T2000\_All$WDSP)
 cvtable$r2000[17] <-min(F\_T2000\_All$ah\_gm3 )
 cvtable$r2000[18] <-max(F\_T2000\_All$ah\_gm3 )
 cvtable$r2000[19] <-min(F\_T2000\_All$TEMP )
 cvtable$r2000[20] <-max(F\_T2000\_All$TEMP )
 cvtable$r2000[21] <-min(F\_T2000\_All$AOD )
 cvtable$r2000[22] <-max(F\_T2000\_All$AOD )

* Basic Statistics
** Tests of Independence

*Tests of Independence*
 *
 Chi-Square Test*
 For 2-way tables you can use chisq.test(mytable) to test independence
of the row and column variable. By default, the p-value is calculated
from the asymptotic chi-squared distribution of the test statistic.
Optionally, the p-value can be derived via Monte Carlo simultation.
 *Fisher Exact Test*
 fisher.test(x) provides an exact test of independence. x is a two
dimensional contingency table in matrix form.
 *Mantel-Haenszel test*
 Use the mantelhaen.test(x) function to perform a
Cochran-Mantel-Haenszel chi-squared test of the null hypothesis that two
nominal variables are conditionally independent in each stratum,
assuming that there is no three-way interaction. x is a 3 dimensional
contingency table, where the last dimension refers to the strata.
 *Loglinear Models*
 You can use the loglm( ) function in the MASS package to produce
log-linear models. For example, let's assume we have a 3-way contingency
table based on variables A, B, and C.
 library(MASS)
 mytable <- xtabs(~A+B+C, data=mydata)
 We can perform the following tests:
 Mutual Independence: A, B, and C are pairwise independent.
loglm(~A+B+C, mytable)
 Partial Independence: A is partially independent of B and C (i.e., A is
independent of the composite variable BC).
 loglin(~A+B+C+B*C, mytable)
 Conditional Independence: A is independent of B, given C.
 loglm(~A+B+C+A*C+B*C, mytable)No Three-Way Interaction
 loglm(~A+B+C+A*B+A*C+B*C, mytable)
 Martin Theus and Stephan Lauer have written an excellent article on
Visualizing Loglinear Models, using mosaic plots. There is also great
tutorial example by Kevin Quinn on analyzing loglinear models via glm.

*** chi-square test of independence

You can apply the function chisq.test() to a two-way table in order to
produce a chi-square test of independence of the row and column
variables. See example:
 [[R_files/img/Image_Z8iH6lexlXspixk98Qo3VA_0003.png]]

*** Fisher's exact test

You can produce a Fisher's exact test via the fisher.test() function.
Fisher's exact test evaluates the null hypothesis of independence of
rows and columns in a contingency table with fixed marginals. The format
is fisher.test(mytable), where mytable is a two-way table. Here's an
example:
 > mytable <- xtabs(~Treatment+Improved, data=Arthritis)
 > fisher.test(mytable)
 Fisher's Exact Test for Count Data
 data: mytable
 p-value = 0.001393
 alternative hypothesis: two.sided
 In contrast to many statistical packages, the fisher.test() function
can be applied
 to any two-way table with two or more rows and columns, not a 2x2
table.

*** cochran--mantel--haenszel test

The mantelhaen.test() function provides a Cochran--Mantel--Haenszel
chi-square
 test of the null hypothesis that two nominal variables are
conditionally independent
 in each stratum of a third variable. The following code tests the
hypothesis that Treat-
 ment and Improved variables are independent within each level Sex. The
test assumes
 that there's no three-way (Treatment x Improved x Sex) interaction.
 > mytable <- xtabs(~Treatment+Improved+Sex, data=Arthritis)
 > mantelhaen.test(mytable)
 Cochran-Mantel-Haenszel test
 data: mytable
 Cochran-Mantel-Haenszel M\^2 = 14.6, df = 2, p-value = 0.0006647
 The results suggest that the treatment received and the improvement
reported aren't
 independent within each level of sex (that is, treated individuals
improved more than
 those receiving placebos when controlling for sex).

*** Measures of association

The significance tests in the previous section evaluated whether or not
sufficient evidence existed to reject a null hypothesis of independence
between variables. If you can reject the null hypothesis, your interest
turns naturally to measures of association in or-
 der to gauge the strength of the relationships present. The
assocstats() function in the vcd package can be used to calculate the
phi coefficient, contingency coefficient, and Cramer's V for a two-way
table. An example is given in the following listing
 [[R_files/img/Image_iwcuxkrtaXaDL4TDcKUvRA_0003.png]]\\
 In general, larger magnitudes indicated stronger associations. The vcd
package also provides a kappa() function that can calculate Cohen's
kappa and weighted kappa for a confusion matrix (for example, the degree
of agreement between two judges classify-
 ing a set of objects into categories).

** Kurtosis
*** check using the e1071 pacakge
the kurtosis is a measure of the peakedness of the data
distribution. Negative kurtosis would indicates a flat data
distribution, which is said to be platykurtic. Positive kurtosis would
indicates a peaked distribution, which is said to be
leptokurtic. Incidentally, the normal distribution has zero kurtosis,
and is said to be mesokurtic.
We apply the function kurtosis from the e1071 package to compute the kurtosis of eruptions. As the package is not in the core R library, it has to be installed and loaded into the R workspace.

#+begin_src r
> library(e1071)                    # load e1071
> duration = faithful$eruptions     # eruption durations
> kurtosis(duration)                # apply the kurtosis function
[1] -1.5116
#+end_src

The kurtosis of eruption duration is -1.5116, which indicates that eruption duration distribution is platykurtic. This is consistent with the fact that its histogram is not bell-shaped.

** Correlations

*** basics

Correlation coefficients are used to *describe relationships among
quantitative variables*. The sign *± indicates the direction of the
relationship* (positive or inverse) and the *magnitude indicates the
strength of the relationship* (ranging from 0 for no relationship to 1
for a perfectly predictable relationship).
 R can produce a variety of correlation coefficients, including Pearson,
Spearman, Kendall, partial, polychoric, and polyserial. Let's look at
each in turn.
 *Pearson, spearman, and kendall correlations*
 The *Pearson product* moment correlation assesses the degree of linear
relationship*between two quantitative variables*.
 *Spearman's* Rank Order correlation coefficient assesses the degree of
relationship between *two rank-ordered variables*.
 *Kendall's* Tau is also a *nonparametric measure of rank correlation*.
 The *cor()* function produces all three correlation coefficients,
whereas the*cov()* function provides covariances.
 *cor(x, use= , method= )* OR *cov(x, use= , method= )
*
 *NOTE:
 1. The default in R is that all variables with missing data are removed
from statistical computation. if you require them add na.action command
 2. both cor and cov will not work on the whole dataset if one variable
is character*
 *
 [[R_files/img/Image_SB44Q8qYgrxIhDtxb5e6ZQ_0003.png]]
*
 *example:*
 cor(data\_10$mfvalue,data\_10$predicted)
 where:
 cor is the command
 data\_10 is the data name
 $mfvalue is the variable name

*** exclude missing cases

you can exclude missing cases with the use = "complete" command
 for example
 cvtable3$r2008[20]
<-(cor(T2008\_merged\_lowpop$MFVALUE,T2008\_merged\_lowpop$OApred,use =
"complete")

*** quickly get R2

since the cor give a R result to get R2 simple square it:
 cvtable3$r2008[20]
<-(cor(T2008\_merged\_lowpop$MFVALUE,T2008\_merged\_lowpop$OApred,use =
"complete"))\^2

*** Testing correlations for significance

Once you've generated correlation coefficients, how do you test them for
statistical significance?
 The typical null hypothesis is no relationship (that is, the
correlation in the population is 0). You can use the cor.test() function
to test an individual Pearson,Spearman, and Kendall correlation
coefficient. A simplified format is:
 *cor.test(x, y, alternative = , method = )*
 where x and y are the variables to be correlated, alternative specifies
a two-tailed or one tailed test ("two.side", "less", or "greater") and
method specifies the type of correlation ("pearson", "kendall", or
"spearman") to compute.
 Use alternative="less" when the research hypothesis is that the
population correlation is less than 0. Use alternative="greater" when
the research hypothesis is that the population correlation is greater
than 0. By default, alternative="two.side" (population correlation
 isn't equal to 0) is assumed. See the following listing for an example.
 [[R_files/img/Image_dGGpsBv8ncDG5OD9RxkCyA_0003.png]]

** T-test

*** overview

The most common activity in research is the *comparison of two groups*.
Do patients receiving a new drug show greater improvement than patients
using an existing medication? Does one manufacturing process produce
fewer defects than another?
 Which of two teaching methods is most cost-effective? If your*outcome
variable is categorical*, you can use tables/correlations .T-tests
*focus on group comparisons, where the outcome variable is continuous
and assumed to be distributed normally*.
 What do you do if you want to compare more than two groups? If you can
assume that the data are independently sampled from normal populations,
you can*use analysis of variance (ANOVA)*. ANOVA is a comprehensive
methodology that covers many experi-
 mental and quasi-experimental designs.

*** Independent t-test

Are you more likely to be imprisoned if you commit a crime in the South?
The comparison of interest is Southern versus non-Southern states and
the dependent variable. is the probability of incarceration. A two-group
independent t-test can be used to test
 the hypothesis that the two population means are equal. Here, you
assume that the two groups are independent and that the data are sampled
from normal populations. The format is either
 *t.test(y ~ x, data)*
 where y is numeric and x is a dichotomous variable, or
 *t.test(y1, y2)
*
 where y1 and y2 are numeric vectors (the outcome variable for each
group). The optional data argument refers to a matrix or data frame
containing the variables. In contrast to most statistical packages, the
default test assumes unequal variance and
 applies the Welsh degrees of freedom modification. You can add a
var.equal=TRUE option to specify equal variances and a pooled variance
estimate. By default, a two tailed alternative is assumed (that is, the
means differ but the direction isn't specified).
 You can add the option alternative="less" or alternative="greater" to
specify a directional test.
 In the following code, you compare Southern (group 1) and non-Southern
(group 0) states on the probability of imprisonment using a two-tailed
test without the assumption of equal variances:
 > library(MASS)
 > t.test(Prob ~ So, data=UScrime)
 Welch Two Sample t-test
 data: Prob by So
 t = -3.8954, df = 24.925, p-value = 0.0006506
 alternative hypothesis: true difference in means is not equal to 0
 95 percent confidence interval:
 -0.03852569 -0.01187439
 sample estimates:
 mean in group 0 mean in group 1
 0.03851265 0.06371269
 You can reject the hypothesis that Southern states and non-Southern
states have equal probabilities of imprisonment (p < .001).
 *NOTE* Because the outcome variable is a proportion, you might try to
transform it to normality before carrying out the t-test. In the current
case, all reasonable transformations of the outcome variable (Y/1-Y,
log(Y/1-Y), arcsin(Y), arcsin(sqrt(Y)) would've led to the same
conclusions.

*** Dependent t-test

As a second example, you might ask if unemployment rate for younger
males (14--24) is greater than for older males (35--39). In this case,
*the two groups aren't independent*. You wouldn't expect the
unemployment rate for younger and older males in
 Alabama to be unrelated. *When observations in the two groups are
related, you have a dependent groups design*. Pre-post or repeated
measures designs also produce dependent groups. A dependent t-test
assumes that the difference between groups is normally distributed. In
this case, the format is
 *t.test(y1, y2, paired=TRUE)*
 where y1 and y2 are the numeric vectors for the two dependent groups.
The results
 are as follows:
 > library(MASS)
 > sapply(UScrime[c("U1","U2")], function(x)(c(mean=mean(x),sd=sd(x))))
 U1 U2
 mean 95.5 33.98
 sd 18.0 8.45
 > with(UScrime, t.test(U1, U2, paired=TRUE))
 Paired t-test
 data: U1 and U2
 t = 32.4066, df = 46, p-value < 2.2e-16
 alternative hypothesis: true difference in means is not equal to 0
 95 percent confidence interval:
 57.67003 65.30870
 sample estimates:
 mean of the differences
 61.48936
 The mean difference (61.5) is large enough to warrant rejection of the
hypothesis that the mean unemployment rate for older and younger males
is the same. Younger males have a higher rate. In fact, the probability
of obtaining a sample difference this large if
 the population means are equal is less than 0.00000000000000022 (that
is, 2.2e--16).

** Chi-squared tests

*** overview

[[R_files/img/Image_RGHskMFYkmg3WsVbMTybdA_0002.png]]

*** basic chi test

To perform the Chi-squared test you type something like the following:

chisq.test(your.data)

 Pearson's Chi-squared test
 data: your.data
 X-squared = 121.5774, df = 8, p-value < 2.2e-16
 This gives you a basic result but you will want more than that in order
to interpret the statistic.
 the test produces more data than is displayed, to see what you have to
work with type:

names(chisq.test(your.data))

 [1] "statistic" "parameter" "p.value" "method" "data.name" "observed"
 [7] "expected" "residuals"
 This shows us that there are other data that we can call upon to help
us. It is cumbersome to run the test each time to it would be better to
assign the chi-squared test result to a variable. It's a good habit to
get into when using R and means that you can use the results in further
calculations.
 In this instance we might try:

your.chi = chisq.test(your.data)

names(your.chi)

 To see the observed values (i.e. the original data) type:

your.chi$observed


 To see the expected values type:

your.chi$expected


 To see the residuals type:

your.chi$residuals


 The residuals calculated are the Pearson residuals i.e. (observed -
expected) / sqrt(expected). You can examine these and easiy pick out
which are the most important associations (and the direction).
 You do not actually need to type the full command to see the components
of the chi-squared test. After the $ sign you can type a short version
and as long as it is unique it will be intepreted e.g.

your.chi$obs
 your.chi$exp
 your.chi$res

 R will produce the desired table. If you wish to extract a single value
from one of these tables then you can do that by appending an extra part
e.g.

your.chi$res["row.name", "col.name"]

 In other words add a square bracket and type in the row and column
headings (in quotes) that define the value you wish.

** Nonparametric statistics

*** Basic

If you're unable to *meet the parametric assumptions of a t-test or
ANOVA*, you can turn to *nonparametric approaches* when the outcome
variables are severely skewed or ordinal in nature.

*** Wilcoxon rank sum test (Mann--Whitney U test)

If the two groups are independent, you can use the Wilcoxon rank sum
test (more popularly known as the Mann--Whitney U test) to assess
whether the observations are sampled from the same probability
distribution (that is, whether the probability of
 obtaining higher scores is greater in one population than the other).
The format is either:
 *wilcox.test(y ~ x, data)*
 where y is numeric and x is a dichotomous variable, or
 *wilcox.test(y1, y2)*
 where y1 and y2 are the outcome variables for each group. The optional
data argument refers to a matrix or data frame containing the variables.
The default is a two-tailed test. You can add the option exact to
produce an exact test, and alternative="less"
 or alternative="greater" to specify a directional test.
 If you apply the Mann--Whitney U test to the question of incarceration
rates from
 the previous section, you'll get these results:
 > with(UScrime, by(Prob, So, median))
 So: 0
 [1] 0.0382
 --------------------
 So: 1
 [1] 0.0556
 > wilcox.test(Prob ~ So, data=UScrime)
 Wilcoxon rank sum test
 data: Prob by So
 W = 81, p-value = 8.488e-05
 alternative hypothesis: true location shift is not equal to 0
 Again, you can reject the hypothesis that incarceration rates are the
same in Southern and non-Southern states (p < .001).

*** Wilcoxon signed rank test (for sependent samples)

The Wilcoxon signed rank test provides a nonparametric alternative to
the dependent sample t-test. It's appropriate in situations where the
groups are paired and the assumption of normality is unwarranted. The
format is identical to the Mann--Whitney U test, but you add the
*paired=TRUE* option. Let's apply it to the unemployment question from
the previous section:
 > sapply(UScrime[c("U1","U2")], median)
 U1 U2
 92 34
 > with(UScrime, wilcox.test(U1, U2, paired=TRUE))
 Wilcoxon signed rank test with continuity correction
 data: U1 and U2
 V = 1128, p-value = 2.464e-09
 alternative hypothesis: true location shift is not equal to 0
 Again, you'd reach the same conclusion reached with the paired t-test.
In this case, the parametric t-tests and their nonparametric equivalents
reach the same conclusions. When the assumptions for the t-tests are
reasonable, the parametric tests will be more powerful (more likely to
find a difference if it exists). The nonparametric tests are more
appropriate when the assumptions are grossly
 unreasonable (for example, rank ordered data).

*** Comparing more than two groups

If you can't meet the assumptions of ANOVA designs (normal samples), you
can use nonparametric methods to evaluate group differences. *If the
groups are independent, a Kruskal--Wallis* test will provide you with a
useful approach. If *the groups are dependent (IE, repeated measures or
randomized block design), the Friedman test is more appropriate*. The
format for the Kruskal--Wallis test is:
 *kruskal.test(y ~ A, data)*
 where y is a numeric outcome variable and A is a grouping variable with
two or more levels (if there are two levels, it's equivalent to the
Mann--Whitney U test). For the Friedman test, the format is
 *friedman.test(y ~ A | B, data)*
 where y is the numeric outcome variable, A is a grouping variable, and
B is a blocking variable that identifies matched observations. In both
cases, data is an option argument specifying a matrix or data frame
containing the variables.
 Let's apply the Kruskal--Wallis test to the illiteracy question. First,
you'll have to add the region designations to the dataset. These are
contained in the dataset state. region distributed with the base
installation of R.
 states <- as.data.frame(cbind(state.region, state.x77))
 > kruskal.test(Illiteracy ~ state.region, data=states)
 Kruskal-Wallis rank sum test
 data: states$Illiteracy by states$state.region
 Kruskal-Wallis chi-squared = 22.7, df = 3, p-value = 4.726e-05
 The significance test suggests that the illiteracy rate isn't the same
in each of the four regions of the country (p <.001). Although you can
reject the null hypothesis of no difference, the test doesn't tell you
which regions differ significantly from each other. To answer this
question, you could compare groups two at a time using the Mann--Whitney
U test. A more elegant approach is to apply a simultaneous multiple
comparisons procedure that makes alll pairwise comparisons, while
controlling the type I error rate (the probability of finding a
difference that isn't there).
 The npmc package provides the nonparametric multiple comparisons you
need:
 [[R_files/img/Image_UF8tPRsGiTyJO5l7vCBELg_0003.png]]

** ANOVA/MANOVA

*** ANOVA

**** Evaluate Model Effects

Evaluate Model Effects
 WARNING: R provides Type I sequential SS, not the default Type III
marginal SS reported by SAS and SPSS. In a nonorthogonal design with
more than one term on the right hand side of the equation order will
matter (i.e., A+B and B+A will produce different results)! We will need
use the drop1( ) function to produce the familiar Type III results. It
will compare each term with the full model. Alternatively, we can use
anova(fit.model1, fit.model2) to compare nested models directly.
 summary(fit) # display Type I ANOVA table
 drop1(fit,~.,test="F") # type III SS and F Tests
 Nonparametric and resampling alternatives are available.

**** Fit a Model

*ANOVA*
 If you have been analyzing ANOVA designs in traditional statistical
packages, you are likely to find R's approach less coherent and
user-friendly. A good online presentation on ANOVA in R is available
from Katholieke Universiteit Leuven.
 *
 Fit a Model*
 In the following examples lower case letters are numeric variables and
upper case letters are factors.
 # One Way Anova (Completely Randomized Design)
 fit <- aov(y ~ A, data=mydataframe)
 # Randomized Block Design (B is the blocking factor)
 fit <- aov(y ~ A + B, data=mydataframe)
 # Two Way Factorial Design
 fit <- aov(y ~ A + B + A:B, data=mydataframe)
 fit <- aov(y ~ A*B, data=mydataframe) # same thing
 # Analysis of Covariance
 fit <- aov(y ~ A + x, data=mydataframe)
 For within subjects designs, the dataframe has to be rearranged so that
each measurement on a subject is a separate observation. See R and
Analysis of Variance.
 # One Within Factor
 fit <- aov(y~A+Error(Subject/A),data=mydataframe)
 # Two Within Factors W1 W2, Two Between Factors B1 B2
 fit <- aov(y~(W1*W2*B1*B2)+Error(Subject/(W1*W2))+(B1*B2),
 data=mydataframe)

**** Look at Diagnostic Plots

 2. Look at Diagnostic Plots
 Diagnostic plots provide checks for heteroscedasticity, normality, and
influential observerations.layout(matrix(c(1,2,3,4),2,2)) # optional
layout
 plot(fit) # diagnostic plots

**** Multiple Comparisons

Multiple Comparisons
 You can get Tukey HSD tests using the function below. By default, it
calculates post hoc comparisons on each factor in the model. You can
specify specific factors as an option. Again, remember that results are
based on Type I SS!
 # Tukey Honestly Significant Differences
 TukeyHSD(fit) # where fit comes from aov()

**** Visualizing Results

Visualizing Results
 Use box plots and line plots to visualize group differences. There are
also two functions specifically designed for visualizing mean
differences in ANOVA layouts. interaction.plot( ) in the base stats
package produces plots for two-way interactions. plotmeans( ) in the
gplots package produces mean plots for single factors, and includes
confidence intervals.
 # Two-way Interaction Plot
 attach(mtcars)
 gears <- factor(gears)
 cyl <- factor(cyl)
 interaction.plot(cyl, gear, mpg, type="b", col=c(1:3),
 leg.bty="o", leg.bg="beige", lwd=2, pch=c(18,24,22),
 xlab="Number of Cylinders",
 ylab="Mean Miles Per Gallon",
 main="Interaction Plot")
 [[R_files/img/Image_Ke3i4rlTM7ErqRHnp6yTBw_0005.jpg]]
 # Plot Means with Error Bars
 library(gplots)
 attach(mtcars)
 cyl <- factor(cyl)
 plotmeans(mpg~cyl,xlab="Number of Cylinders",
 ylab="Miles Per Gallon, main="Mean Plot\nwith
9[[R_files/img/Image_Ke3i4rlTM7ErqRHnp6yTBw_0006.jpg]]5% CI")

*** Assessing Classical Test Assumptions

*Assessing Classical Test Assumptions*
 In classical parametric procedures we often assume normality and
constant variance for the model error term. Methods of exploring these
assumptions in an ANOVA/ANCOVA/MANOVA framework are discussed here.
Regression diagnostics are covered under multiple linear regression.
 *
 Outliers*
 Since outliers can severly affect normality and homogeneity of
variance, methods for detecting disparate observerations are described
first.
 The aq.plot( ) function in the mvoutlier package allows you to identfy
multivariate outliers by plotting the ordered squared robust Mahalanobis
distances of the observations against the empirical distribution
function of the MD2i. Input consists of a matrix or dataframe. The
function produces 4 graphs and returns a boolean vector identifying the
outliers.
 # Detect Outliers in the MTCARS Data
 library(mvoutlier)
 outliers <-
 aq.plot(mtcars[c("mpg","disp","hp","drat","wt","qsec")])
 outliers # show list of outliers
 *Univariate Normality*
 You can evaluate the normality of a variable using a Q-Q plot.
 # Q-Q Plot for variable MPG
 attach(mtcars)
 qqnorm(mpg)
 qqline(mpg)
 Significant departures from the line suggest violations of normality.
 You can also perform a Shapiro-Wilk test of normality with the
shapiro.test(x) function, where x is a numeric vector. Additional
functions for testing normality are available in nortest package.
 Multivariate Normality
 MANOVA assumes multivariate normality. The function mshapiro.test( ) in
the mvnormtest package produces the Shapiro-Wilk test for multivariate
normality. Input must be a numeric matrix.
 # Test Multivariate Normality
 mshapiro.test(M)
 If we have p x 1 multivariate normal random vector x vector
 then the squared Mahalanobis distance between x and μ is going to be
chi-square distributed with p degrees of freedom. We can use this fact
to construct a Q-Q plot to assess multivariate normality.
 # Graphical Assessment of Multivariate Normality
 x <- as.matrix(mydata) # n x p numeric matrix
 center <- colMeans(x) # centroid
 n <- nrow(x); p <- ncol(x); cov <- cov(x);
 d <- mahalanobis(x,center,cov) # distances
 qqplot(qchisq(ppoints(n),df=p),d,
 main="QQ Plot Assessing Multivariate Normality",
 ylab="Mahalanobis D2")
 abline(a=0,b=1)
 *Homogeneity of Variances*
 The bartlett.test( ) function provides a parametric K-sample test of
the equality of variances. The fligner.test( ) function provides a
non-parametric test of the same. In the following examples y is a
numeric variable and G is the grouping variable.
 # Bartlett Test of Homogeneity of Variances
 bartlett.test(y~G, data=mydata)
 # Figner-Killeen Test of Homogeneity of Variances
 fligner.test(y~G, data=mydata)
 The plot.hov( ) function in the HH package provides a graphic test of
homogeneity of variances based on Brown-Forsyth. In the following
example, y is numeric and G is a grouping factor. Note that G must be of
type factor.
 # Homogeneity of Variance Plot
 library(HH)
 hov(y~G, data=mydata)
 plot.hov(y~G,data=mydata)
 *Homogeneity of Covariance Matrices*
 MANOVA and LDF assume homogeneity of variance-covariance matrices. The
assumption is usually tested with Box's M. Unfortunately the test is
very sensitive to violations of normality, leading to rejection in most
typical cases. Box's M is not included in R, but code is available.


*** MANOVA

*MANOVA*
 If there is more than one dependent (outcome) variable, you can test
them simultaneously using a multivariate analysis of variance (MANOVA).
In the following example, let Y be a matrix whose columns are the
dependent variables.
 # 2x2 Factorial MANOVA with 3 Dependent Variables.
 Y <- cbind(y1,y2,y3)
 fit <- manova(Y ~ A*B)
 summary(fit, test="Pillai")
 Other test options are "Wilks", "Hotelling-Lawley", and "Roy". Use
summary.aov( ) to get univariate statistics. TukeyHSD( ) and plot( )
will not work with a MANOVA fit. Run each dependent variable separately
to obtain them. Like ANOVA, MANOVA results in R are based on Type I SS.
To obtain Type III SS, vary the order of variables in the model and
rerun the analyses. For example, fit y~A*B for the TypeIII B effect and
y~B*A for the Type III A effect.

** result presentation

*** scale by IRQ

This is done mainly to better compare 2 variables from 2 different
datasets.
 to scale a variable (IE exposure by it's IQR) use something like this:
 #Scale by IRQ
 100*(exp(0.00128*6.49)-1)
 where 0.00128 is the beta coefficient of the variable , 6.49 us the IRQ
range Q3-Q1

*** Percent change in DV per 1 IQR increase in exposure

get IRQ for pollution (exposure) > in our case lag24h of PM

mlag001\_irq<- IQR(mb1$lag24h)

 run model:
 mlag001<-glmmPQL(logicam ~ lag24h +PREDICTED +age + cwtemplag24h +
bmi+as.factor(smk2)+cwhumlag24h + diabete+statin + cos+ sin,
random=~1|id,family=gaussian, data=mb1,na=na.omit)
 Extract BETA,SE,Sig:
 ICAM\_restable$beta[1] <- mlag001$coef$fixed[2] #extract Betas
 ICAM\_restable$se[1] <-(summary(mlag001)$tTable[2,2]) #extract SE
 ICAM\_restable$sig[1] <-(summary(mlag001)$tTable[2,5]) #extract sig
 get Percent change for 1 IQR change if outcome is logged:

ICAM\_restable$pc[1] <- (exp(ICAM\_restable$beta[1]*mlag001\_irq)-1)*100

 # Low CI bound:
 ICAM\_restable$L\_CI[1] <-
(exp((ICAM\_restable$beta[1]-1.96*ICAM\_restable$se[1])*mlag001\_irq)-1)*100
 # High CI bound:
 ICAM\_restable$H\_CI[1] <-
(exp((ICAM\_restable$beta[1]+1.96*ICAM\_restable$se[1])*mlag001\_irq)-1)*100

** Power analysis

*** intro

** Prediction errors

*** RMSE
Root Mean Square Error (RMSE) between sim and obs, in the same units of sim and obs, with treatment of missing values.
RMSE gives the standard deviation of the model prediction error. Asmaller value indicates better model performance.
Methods to calculate RMSPE:

1. use this codes

#+BEGIN_SRC r

#get the RMSE
r.rmse <- sqrt(mean(r$residuals^2))

#objects made by model fitting functions should have a residuals method, making it possible to do this (although some types of models may not)
r.rmse <- sqrt(mean(residuals(r)^2))
#+END_SRC


2) install package hydroGOF
rmse(sim, obs, na.rm=TRUE, ...)
example:
 library(hydroGOF)
 rmse(FALL$predicted, FALL$PM25)
** test the significance of a difference between 2 coefficients 
To test the significance of a difference between 2 coefficients (b2-b1), compute 

s3=sqrt(s1^2 + s2^2) where s1 is the se of b1, s2 the se of b2
and s3 is the se of (b2-b1) that should be a t statistic and you can look up the p value from that ( 2*(1-probt((b2-b1)/s3)))

#+BEGIN_SRC R
#long term
b1=0.00096339
#short term
b2=0.0003317
s3=sqrt(b1^2)+sqrt(b2^2)

t=(b2-b1)/s3 
#+END_SRC

* Regressions
** Intro

In many ways, regression analysis lives at the heart of statistics. It's
a broad term for a set of methodologies used to predict a *response
variable*(also called a *dependent, criterion, or outcome variable*)
from one or more *predictor variables* (also called
 *independent or explanatory variables*). In general, regression
analysis can be used to *identify the explanatory variables that are
related to a response variable, to describe the form of the
relationships involved, and to provide an equation for predicting the
response variable from the explanatory variables.
 [[R_files/img/Image_iJTPwdmOXjrrhFOMoTLQPA_0003.png]]*

** OLS regression

*** Basics

For most of this chapter, we'll be predicting the response variable from
a set of predictor variables (also called "regressing" the response
variable on the predictor variables---hence the name) using OLS. OLS
regression fits models of the form
 [[R_files/img/Image_b9Y2UmqWtBcR2GRsxdIuIw_0005.png]]
 Our goal is to select model parameters (intercept and slopes) that
minimize the difference between actual response values and those
predicted by the model. Specifically, model parameters are selected to
minimize the sum of squared residuals
 [[R_files/img/Image_b9Y2UmqWtBcR2GRsxdIuIw_0006.png]]
 If you violate these assumptions, your statistical significance tests
and confidence intervals may not be accurate. Note that OLS regression
also assumes that the independent variables are fixed and measured
without error, but this assumption is typically relaxed in practice.
 When the regression model contains *one dependent variable and one
independent variable*, we call the approach *simple linear regression*.
When there's *one predictor variable but powers of the variable are
included (for example, X, X2, X3), we call it polynomial regression*.
When there's *more than one predictor variable, you call it multiple
linear regression*. We'll start with an example of a simple linear
regression, then progress to examples of polynomial and multiple linear
regression, and end with an example of multiple regression that includes
an interaction among the predictors.

*** Fitting regression models with lm()

In R, the basic function for fitting a linear model is lm(). The format
is
 *myfit <- lm(formula, data)*
 where formula describes the model to be fit and data is the data frame
containing the data to be used in fitting the model. The resulting
object (myfit in this case) is a list that contains extensive
information about the fitted model. The formula is typically written as
 *Y ~ X1 + X2 + ... + Xk*
 where the ~ separates the response variable on the left from the
predictor variables on the right, and the predictor variables are
separated by + signs. Other symbols can be used to modify the formula in
various ways :
 [[R_files/img/Image_7iZpEAudd4oPPJu8DJtEog_0003.png]]

*** functions that are useful when fitting linear model

[[R_files/img/Image_ncKg8zI85yZXVnURFHXuKA_0003.png]]

** Regression (lm)
*** lm function/components
**** Fitting a Model 

fr more details:

http://data.princeton.edu/R/linearModels.html

To fit an ordinary linear model with fertility change as the response and setting and effort as predictors, try

#+begin_src R
lmfit = lm( change ~ setting + effort )
#+end_src

Note first that lm is a function, and we assign the result to an object that we choose to call lmfit (for linear model fit). This stores the results of the fit for later examination.

The argument to lm is a model formula, which has the response on the left of the tilde ~ (read "is modeled as") and a Wilkinson-Rogers model specification formula on the right. R uses

#+BEGIN_EXAMPLE
use +	to combine elementary terms, as in A+B
use : 	for interactions, as in A:B;
use * for or both main effects and interactions, so A*B = A+B+A:B
#+END_EXAMPLE

A nice feature of R is that it lets you create interactions between categorical variables, between categorical and continuous variables, and even between numeric variables (it just creates the cross-product). 

**** extract R2

for example to extract R2:
 summary(MODEL/FIT\_NAME)$r.squared
 summary(mod\_spatial)$r.squared
 *Other methods:*
 sum.lm1=summary(MODEL)
 sum.lm1$r.squared #this *gives you the R2*
 sum.lm1$coef[2,2] #this giv*es you the SE of the slope*

**** extracting model components

*Extracting Model Components*
 summary(model.object) - can be used to obtain or print a summary of the
results and the function 'anova' to produce an analysis of variance
table.
 weights(model.object) - extracts a vector of weights, one for each case
in the fit (after subsetting and 'na.action').
 formula(model.object) - extracts the formula expression contained in
the model object and returns a character string with the formula.
 coef(model.object) - extracts the regression coefficients contained in
the model object and returns a numeric vector of length p.
 deviance(model.object) - extracts the deviance contained in the model
object and returns a unit numeric vector.

**** mathematical expressions

We need to use *I(expression)* to add a mathematical expression
involving one or more variables as an independent predictor.
 IE


 bp.model2<-lm(log(bp1sys)~age+
 as.factor(race)+ smokpkyr +bmi+ I(bmi\^2) , na.action=na.omit,
 data=hanes1b)



 summary(bp.model2)

**** na.action

to omit missing values use:

#+BEGIN_EXAMPLE
na.action=na.omit
#+END_EXAMPLE

IE:

#+BEGIN_SRC R
out.model_zinc = lme( Znppm ~ CO +nitro+cultiv+iregg, random = ~1 | crop\_type, na.action=na.omit, data = zinc)
#+END_SRC

**** Residuals

*Residuals and REGRESSION DIAGNOSTIC
*
 Extracting Residuals and predicted values
 predict(lm.object,newdata=data,frame) or predict.lm(lm.object) -
calculates predicted values. The arguments include a lm object and a new
data matrix of covariate values with dimensions n' x p. The function
returns a numeric vector of length n' with the predicted values for the
dependent variable.
 fitted(lm.object) - extracts the predicted values on the same scale as
the original dependent variable, that is those contained in
lm.object$fitted.values and returns a numeric vector of length n.
 residuals(lm.object,”type”) - extracts the residuals from the fitted
model. The type argument specifies either "deviance" (the default and
the same as summary.object$deviance.resid), "working" (same as
lm.object$residuals), "pearson" (a weighted version of the working
residuals), or "response" (these are on the same scale as the dependent
variable).
 Fitted values and residuals
 lm.object$fitted.values - a numeric vector of length n that contains
the fitted values on the same scale as the original dependent variable
 lm.object$residuals - a numeric vector of length n containing the
working residuals. The working residuals may also be obtained using a
function call: residual(lm.object, type="working").

**** Subsetting

out.model\_zinc <- lme( Znppm ~ CO , na.action=na.omit,
 random = ~1 | Paircount, *subset=crop\_type==4*,
 data = zinc)
 *example 2:*
 to create a simple regression model for blood pressure among men:
subset=(SEX==0) and women subset=(SEX==1).
 bp.model3<-lm(log(bp1sys)~ age+ smokpkyr+bmi+ as.factor(race)
+as.factor(booze)+I(age\^2), na.action=na.omit, data=hanes1b,
subset=(sex==0))
 summary(bp.model3)
 Now run the regression for women and compare the results for men and
women.
 bp.model4<-lm(log(bp1sys)~ age+ smokpkyr+bmi+ as.factor(race)
+as.factor(booze)+I(age\^2), na.action=na.omit, data=hanes1b,
subset=(sex==1))
 summary(bp.model4)

**** Summarizing model results

Summarizing model results
 Summary Functions
 summary(lm.object) - summarizes all of the regression results contained
in a lm object. The returning values are described below for the summary
lm object.

 summary.aov(lm.object) - summarizes the regression results for a series
of nested models with one covariate removed from the model in the lm
object.
 Summary Object - a recursive object that may be created by applying the
summary function to a lm object. The returning value of the summary
function may be stored as an object or listed as text on the default
output device.
 Summary(object)$call, summary.object$terms, and summary.object$iter -
these are the same as for the similarly named elements of the lm object.
The complete function call and the number of iterations are supplied in
the listed output.
 summary(object)$coefficients - a numeric matrix with dimensions p x 3
that contains the estimate, the standard error of the estimate, and the
t-statistic for each regression coefficient. This matrix is supplied in
the listed output.
 summary(object)$df - a numeric vector of length 2 that contains p and
n-p.
 summary(object)$cov.unscaled - a numeric matrix with dimensions p x p
that contains the unscaled covariance of the regression coefficients.

**** catagorical variables

categorical variables are added to a model differently from numeric
variables
 they are added like this:
 as.factor(VAR NAME)
 the reference will be the lowest variable and will be missing from the
output
 you can also force numerical variables to be treated as categorical
variables by putting them in an as.factor() statement

**** put in catagorical variables

use as.factor for catagorical variables in the regression equation
 Example:
 birthweight ~
lungd+renal+as.factor(apncu)+*as.factor*(new\_race)+*as.factor*(edu\_group),

**** Polynomial regression (quadratic)

use something like this:
 You can fit a quadratic equation using the statement
 fit2 <- lm(weight ~ height + I(height\^2), data=women)
 The new term I(height\^2) requires explanation. height\^2 adds a
height-squared
 term to the prediction equation. The I function treats the contents
within the paren-
 theses as an R regular expression.
 [[R_files/img/Image_98hLbAZ8O2dRyYslr1EP4A_0003.png]]

*** Exploratory Data Analysis (EDA) and Regression

**** VIF

load the 'car' package and issue :
 vif(MODEL)
 Rule of Thumb: VIF > 10 is of concern

**** examine bivariate relationshps (all model variable cor matix plot)

By default, the *scatterplotMatrix()* function provides scatter plots of
the variables with each other in the off-diagonals and superimposes
smoothed (loess) and linear fit lines on these plots. The principal
diagonal contains density and rug plots for each variable.
 *example 1:*
 You can see that murder rate may be bimodal and that each of the
predictor variables is skewed to some extent. Murder rates rise with
population and illiteracy, and fall with higher income levels and frost.
At the same time, colder states have lower illiteracy rates and
population and higher incomes.
 [[R_files/img/Image_eVu0fKLs896kx04gHxanpQ_0003.png]]

*** Regression Diagnostics

**** Intro

R's base installation provides numerous methods for evaluating the
statistical assumptions in a regression analysis. The most common
approach is to apply the *plot()* function to the object returned by the
lm(). Doing so produces four graphs that are useful for evaluating the
model fit. Applying this approach to the simple linear regression
example
 fit <- lm(weight ~ height, data=women)
 par(mfrow=c(2,2))
 plot(fit)
 produces the graphs shown in figure 8.6. The par(mfrow=c(2,2))
statement is used to combine the four plots produced by the plot()
function into one large 2x2 graph.
 [[R_files/img/Image_5KHqMOqckRE1vIOAAVqyIQ_0003.png]]
 To understand these graphs, consider the assumptions of OLS regression:
 *Normality* ---If the dependent variable is normally distributed for a
fixed set of
 predictor values, then the residual values should be normally
distributed with a
 mean of 0. The Normal Q-Q plot (upper right) is a probability plot of
the stan-
 dardized residuals against the values that would be expected under
normality. If you've met the normality assumption, the points on this
graph should fall on the straight 45-degree line. Because they don't,
you've clearly violated the normality assumption.
 *Independence* ---You can't tell if the dependent variable values are
independent
 from these plots. You have to use your understanding of how the data
were col-
 lected. There's no a priori reason to believe that one woman's weight
influences
 another woman's weight. If you found out that the data were sampled
from fami-
 lies, you may have to adjust your assumption of independence.
 *Linearity* ---If the dependent variable is linearly related to the
independent vari-
 ables, there should be no systematic relationship between the residuals
and the
 predicted (that is, fitted) values. In other words, the model should
capture all
 the systematic variance present in the data, leaving nothing but random
noise. In
 the Residuals versus Fitted graph (upper left), you see clear evidence
of a curved
 relationship, which suggests that you may want to add a quadratic term
to the
 regression.
 *Homoscedasticity* ---If you've met the constant variance assumption,
the points in
 the Scale-Location graph (bottom left) should be a random band around a
hori-
 zontal line. You seem to meet this assumption.
 Finally, the Residual versus Leverage graph (bottom right) provides
information on
 individual observations that you may wish to attend to. The graph
identifies outliers,
 high-leverage points, and influential observations. Specifically:
 *An outlier -*is an observation that isn't predicted well by the fitted
regression
 model (that is, has a large positive or negative residual). An
observation with a high leverage value has an unusual combination of
predictor values. That is, it's an outlier in the predictor space. The
dependent variable
 value isn't used to calculate an observation's leverage.
 An influential observation is an observation that has a
disproportionate impact on
 the determination of the model parameters. Influential observations are
identi-
 fied using a statistic called Cook's distance, or Cook's D.

**** r built in functions

***** Normality

The residplot() function generates a histogram of the studentized
residuals and superimposes a normal curve, kernel density curve, and rug
plot.

**** using the car package

***** intro

A set of techniques called regression diagnostics provides you with the
necessary tools for evaluating the appropriateness of the regression
model and can help you to uncover and correct problems.
 The car package provides a number of functions that significantly
enhance your ability to fit and evaluate regression models
 [[R_files/img/Image_vmfGdKdSmyGjdiqlSLiJpA_0003.png]]

***** Normality

The *qqPlot()*function provides a more accurate method of assessing the
normality assumption than provided by the plot() function in the base
package. It plots the studentized residuals (also called studentized
deleted residuals or jackknifed residuals) against a t distribution with
n-p-1 degrees of freedom, where n is the sample size and p is the
 number of regression parameters (including the intercept). The code
follows:
 library(car)
 fit <- lm(Murder ~ Population + Illiteracy + Income + Frost,
data=states)
 qqPlot(fit, labels=row.names(states), id.method="identify",
 simulate=TRUE, main="Q-Q Plot")
 The qqPlot() function generates the probability plot displayed in
figure 8.9. The option id.method="identify" makes the plot
interactive---after the graph is drawn, mouse clicks on points within
the graph will label them with values specified in the labels option of
the function. Hitting the Esc key, selecting Stop from the graph's
drop-down menu, or right-clicking on the graph will turn off this
interactive mode. Here, I identified Nevada. When simulate=TRUE, a 95
percent confidence envelope is produced using a parametric bootstrap.
 [[R_files/img/Image_ssM9O1RKRZys8SwY6cBNFw_0003.png]]
 With the exception of Nevada, all the points fall close to the line and
are within the confidence envelope, suggesting that you've met the
normality assumption fairly well. But you should definitely look at
Nevada. It has a large positive residual (actual-predicted), indicating
that the model underestimates the murder rate in this state.

***** INDEPENDENCE OF ERRORS

As indicated earlier, the best way to assess whether the dependent
variable values (and
 thus the residuals) are independent is from your knowledge of how the
data were col-
 lected. For example, time series data will often display
autocorrelation---observations
 collected closer in time will be more correlated with each other than
with observations
 distant in time. The car package provides a function for the
Durbin--Watson test to
 detect such serially correlated errors. You can apply the
Durbin--Watson test to the
 multiple regression problem with the following code:
 durbinWatsonTest(fit)
 lag Autocorrelation D-W Statistic p-value
 1
 -0.201
 2.32
 0.282
 Alternative hypothesis: rho != 0
 The nonsignificant p-value (p=0.282) suggests a lack of
autocorrelation, and conversely
 an independence of errors. The lag value (1 in this case) indicates
that each observa-
 tion is being compared with the one next to it in the dataset. Although
appropriate
 for time-dependent data, the test is less applicable for data that
isn't clustered in this
 fashion. Note that the durbinWatsonTest() function uses bootstrapping
(see chapter
 12) to derive p-values. Unless you add the option simulate=FALSE,
you'll get a slightly
 different value each time you run the test.

***** Linearity

You can look for evidence of nonlinearity in the relationship between
the dependent
 variable and the independent variables by using component plus residual
plots (also
 known as partial residual plots). The plot is produced by crPlots()
function in the
 car package. You're looking for any systematic departure from the
linear model that
 you've specified.
 The code to produce these plots is as follows:
 library(car)
 crPlots(fit)
 The resulting plots are provided in figure 8.11. Nonlinearity in any of
these plots sug-
 gests that you may not have adequately modeled the functional form of
that predictor
 in the regression. If so, you may need to add curvilinear components
such as polyno-
 mial terms, transform one or more variables (for example, use log(X)
instead of X), or
 abandon linear regression in favor of some other regression variant.

**** Multi-collinearity

Multicollinearity can be detected using a statistic called the variance
inflation factor (VIF). For any predictor variable, the square root of
the VIF indicates the degree to which the confidence interval for that
variable's regression parameter is expanded relative to a model with
uncorrelated predictors (hence the name). VIF values are
 provided by the vif() function in the car package. As a general rule,
vif > 2 indicates a multicollinearity problem. The code is provided in
the following listing.
 The results indicate that multicollinearity isn't a problem with our
predictor variables.
 *example:
*
 # Evaluate Collinearity
 vif(fit) # variance inflation factors
 sqrt(vif(fit)) > 2 # problem?

*** Unusual /Influential Observations

Influential Observations
 # Influential Observations
 # added variable plots
 av.plots(fit, one.page=TRUE, ask=FALSE)
 # Cook's D plot
 # identify D values > 4/(n-k-1)
 cutoff <- 4/((nrow(mtcars)-length(fit$coefficients)-2))
 plot(fit, which=4, cook.levels=cutoff)
 # Influence Plot
 influencePlot(fit, main="Influence Plot",
 sub="Circle size is proportial to Cook's Distance" )
 [[R_files/img/Image_3XrpyMNvBcrmVgNyBLr7kA_0003.jpg]]

**** Outliers

Outliers are observations that aren't predicted well by the model. They
have either unusually large positive or negative residuals
[[R_files/img/Image_DG4NxsIy7Uhco1C12LiMUw_0007.png]] . *Positive
residuals* indicate that the *model is underestimating* the response
value, while *negative residuals* indicate an o*verestimation*.
 You've already seen one way to identify outliers. Points in the Q-Q
plot of figure 8.9
 [[R_files/img/Image_DG4NxsIy7Uhco1C12LiMUw_0008.png]]
 that lie outside the confidence band are considered outliers. A rough
rule of thumb is that standardized residuals that are larger than 2 or
less than --2 are worth attention.
 The car package also provides a statistical test for outliers. The
*outlierTest()* function reports the Bonferroni adjusted p-value for the
largest absolute studentized residual:
 [[R_files/img/Image_DG4NxsIy7Uhco1C12LiMUw_0009.png]]
 Nevada is identified as an outlier (p=0.048). if it isn't significant,
there are no outliers in the dataset. If it is *significant, you must
delete it and rerun the test to see if others are present.*

**** High leverage points

Observations that have high leverage are outliers with regard to the
other predictors. In other words, they have an unusual combination of
predictor values. The response value isn't involved in determining
leverage.
 Observations with high leverage are identified through the hat
statistic. For a given dataset, the average hat value is p/n, where p is
the number of parameters estimated in the model (including the
intercept) and n is the sample size. Roughly speaking, an observation
with a hat value greater than 2 or 3 times the average hat value should
be
 examined.

*** Corrective measures

**** Deleting observations

Deleting outliers can often improve a dataset's fit to the normality
assumption. Influential observations are often deleted as well, because
they have an inordinate impact on the results. *The largest outlier or
influential observation is deleted, and the model is refit. If there are
still outliers or influential observations, the process is repeated
until
 an acceptable fit is obtained.*
 Again, I urge caution when considering the deletion of observations.
Sometimes, you can determine that the observation is an outlier because
of data errors in recording, or because a protocol wasn't followed, or
because a test subject misunderstood instructions. In these cases,
deleting the offending observation seems perfectly reasonable. In other
cases, the unusual observation may be the most interesting thing about
the data you've collected. Uncovering why an observation differs from
the rest can contribute great insight to the topic at hand, and to other
topics you might not have thought of. Some of our greatest advances have
come from the serendipity of noticingthat something doesn't fit our
preconceptions.

**** Transforming variables

*** Comparing Models (ANOVA and AIC)

**** compare with ANOVA using a binomial model

You can compare the fit of two nested models using the *anova()*
function in the base installation. A nested model is one whose terms are
completely included in the other model. In our states multiple
regression model, we found that the regression coefficients for Income
and Frost were nonsignificant. You can test whether a model without
these two variables predicts as well as one that includes them (see the
following listing).
 [[R_files/img/Image_MTZGcp0H7vtYTDVndwsnlA_0003.png]]
 Here, model 1 is nested within model 2. The anova() function provides a
simultaneous test that Income and Frost add to linear prediction above
and beyond Population and Illiteracy. Because the test is nonsignificant
(p = .994), we conclude that they don't add to the linear prediction and
we're justified in dropping them from our model.
 if the anova is sig. it means adding the variables to the 2nd model
improves the model, in the above example it is not and thus adding the
variables in model 2 (the quad term) does not improve the model compared
to model 1

**** Comparing Models with AIC

The Akaike Information Criterion (AIC) provides another method for
comparing models. The index takes into account a model's statistical fit
and the number of parameters needed to achieve this fit. Models with
smaller AIC values indicating adequate fit with fewer parameters---are
preferred. The criterion is provided by the AIC() function
 > fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost,
 data=states)
 > fit2 <- lm(Murder ~ Population + Illiteracy, data=states)
 [[R_files/img/Image_CrEhaUtGEOGG1QYgn2RvlA_0003.png]]

 The AIC values suggest that the model without Income and Frost is the
better model. Note that although the ANOVA approach requires nested
models, the AIC approach doesn't.

*** Stepwise regression

**** basics

In stepwise selection, variables are added to or deleted from a model
one at a time, until some stopping criterion is reached. For example, in
*forward stepwise regression*you *add predictor variables to the model
one at a time*, *stopping when the addition of variables would no longer
improve the model*. In *backward stepwise regression*, you start with a
model that includes all predictor variables, and then *delete them one
at a time until removing variables would degrade the quality of the
model*. In stepwise stepwise regression (usually called stepwise to
avoid sounding silly), you combine the forward and backward stepwise
approaches. Variables are entered one at a time, but at each step, the
variables in the model are reevaluated, and those that don't contribute
to the model are deleted. A predictor variable may be added to, and
deleted from, a model several times before a final solution is reached.
The implementation of stepwise regression methods vary by the criteria
used to enter or remove variables. The *step AIC()* function in the MASS
package performs stepwise model selection (forward, backward, stepwise)
using an exact AIC criterion.In the next listing, we apply backward
stepwise regression to the multiple regression problem.
 [[R_files/img/Image_V644y5O2qGAxch291VNqPg_0005.png]]
 [[R_files/img/Image_V644y5O2qGAxch291VNqPg_0006.png]]
 You start with all four predictors in the model. For each step, the AIC
column provides
 the model AIC resulting from the deletion of the variable listed in
that row. The AIC
 value for <none> is the model AIC if no variables are removed. In the
first step, Frost
 is removed, decreasing the AIC from 97.75 to 95.75. In the second step,
Income is re-
 moved, decreasing the AIC to 93.76. Deleting any more variables would
increase the
 AIC, so the process stops.
 Stepwise regression is controversial. Although it may find a good
model, there's no
 guarantee that it will find the best model. This is because not every
possible model is
 evaluated. An approach that attempts to overcome this limitation is all
subsets regression.

*** Cross Validation

**** intro

Cross-validation is a useful method for evaluating the generalizability
of a regression equation.
 In cross-validation, a *portion of the data is selected as the training
sample and a portion is selected as the hold-out sample*. A regression
equation is *developed on the training sample, and then applied to the
hold-out sample*. Because the hold-out sample wasn't involved in the
selection of the model parameters, the performance on
 this sample is a more accurate estimate of the operating
characteristics of the model with new data.

**** k-folds

In k-fold cross-validation, the sample is divided into k subsamples.
Each of the ksubsamples serves as a hold-out group and the combined
observations from the remaining k-1 subsamples serves as the training
group. The performance for the k prediction equations applied to the k
hold-out samples are recorded and then averaged. (When k equals n, the
total number of observations, this approach is called jackknifing.)
 You can perform k-fold cross-validation using the *crossval()* function
in the bootstrap package. The following listing provides a function
(called shrinkage()) for cross-validating a model's R-square statistic
using k-fold cross-validation.

*** extract coeficents
#+BEGIN_SRC R
summary(crude270.mod)$coefficients[2,1]  #extract Betas
#+END_SRC

*** extract CI 
#+BEGIN_SRC R
confint(MODEL)[2,1]
#+END_SRC

** lme
*** remove missing values from lme model

you can use na.omit:
 library(nlme)
 out.model = lme( Zn.a ~ Zn.e,
 random = ~1|SID,*na.action=na.omit*,
 data = data1)

*** extract standart error in lme

*use the following syntax*
 restable$se[1] <- (summary(DATASETNAME)$tTable[2,2])
 *or if this dosent work or gives error you can use this:*
 tst2 <- (sqrt(mod1$varFix[2:2,2:2]))
 where 2:2 is the position of the covarite needed

*** extract coeeficents and intercept

use something like this
 summary(lm2000s1)$coefficients[1,1]
 summary(lm2000s1)$coefficients[2,1]
 where 1,1 means first row first column (intercept)
 and 2,1 means second row first column (first beta)

*** extract residuals

to extract and then subset residuals:
 #extract residuals
 day2011$resid<-residuals(Dmodel\_T2011)
 #order them
 day2011 <- day2011[order(day2011$resid),]
 summary(day2011$resid)
 #substract to specific range needed
 x.sub1 <- subset(day2011, resid > 20 & resid > -20)
 #export
 write.dbf(x.sub1 ,"c:/Users/ekloog/Documents/tmp/resid11.dbf")

*** specify random slope and intercept in models

to assign a *random intercept* to a model use this:
 random=~1|INTERCEPT VARIABLE,
 example:
 bp.model.lme<-lme(fixed=log(bp1sys)~ sex + as.factor(race) + smokpkyr +
bmi+ as.factor(booze) + age + ser.chol, random=~1|locode, data=hanesna )
 to assign a *random intercept+slope* to a model use this:
 random=~1+Slope\_variable|INTERCEPT VARIABLE,
 *example:
*
 bp.model.lme<-lme(fixed=log(bp1sys)~ sex + as.factor(race) + smokpkyr +
bmi+ as.factor(booze) + age + ser.chol,
 random=~1+age|locode, data=hanesna )

*** define the correlation structure of the model

*DEFINE THE CORRELATION STRUCTURE OF THE MODEL
*
 lmeStruct(reStruct, corStruct, varStruct): a linear mixed effect
structure is a list of model components representing different sets of
parameters in the model. An lmeStruct must contain at least a reStruct,
but may also contain corStruct and varFunc objects.
 reStruct: representing a random effects structrure
 corStruct : representing a correlation structure. Default is NULL.
 varStruct: a varFunc object, representing a variance function
structure. Default is NULL.
 CorClasses: standard classes of correlation structure:
 CorAR1: autoregressive process of order 1
 CorARMA: autoregressive moving average process
 corCAR1:continuos autoregressive process
 corCompSymm: compound symmetry structure corresponding to a
constant correlation. The correlation model does not depend on the
position of the observation but just on the group.
 corExp, corLin, corGaus, corRatio, corSpher: exponential,
linear,Gaussian, rational,spherical spatial correlation
 corSymm: general correlation matrix, with no additional structure
 All of these structures are functions in which the form needs to be
specified. For example:
 CorCompSymm(value, form, fixed)
 this function is a constructor for the corComSymm class. Value is the
correlation between any 2 correlated observations.
 Form is a formula of the form ~t, or ~t|g, specifying a time covariate
and a grouping factor.
 VarCorr(x,sigma,rdig) -- This is a function that calculates the
estimated variances, standard deviations and correlations between the
random-effects terms.
 *example :*
 bp.model.lme3<-lme(fixed= log(bp1sys)~ sex + as.factor(race) + smokpkyr
+ bmi+ as.factor(booze) + age+ ser.chol, random=~1|locode, correlation=
corCompSymm(form=~1|locode),data=hanesna)
 summary(bp.model.lme3)

*** Linear Mixed Effect Object

*me.object$coefficients* - a list with two components, fixed and random,
where the first is a vector containing the estimated coefficients for
the fixed effects and the second is a matrix containing the estimated
coefficients for the random effects. The columns refer to the parameters
in the random formula, and the rows to the cluster levels; the names of
the coefficients are the same as those in the fixed or random formulas.
 *lme.object$fitted* - a data frame containing the population and
cluster fitted values. The population fitted values are for the fixed
effects only. The cluster fitted values are evaluated at the converged
estimates of the fixed effects and the conditional estimates of the
random effects.
 *lme.object$residuals* - a data frame containing the population and
cluster residuals from the fit. The population residuals are the
observed values minus the population fitted values and the cluster
residuals are the observed values minus the cluster fitted values
 *lme.object$varFix*- An approximate covariance matrix of the fixed
effects estimates.
 *lme.object$modelStruct*-- a list of mixed-effects model components,
such as reStruct, corStruct and varFunc objects.
 *getVarCov(lme.object)* : extract the variance-covariance matrix from a
fitted model
 for example:
 #Extract the coefficients for fixed and random components
 bp.model.lme$coef
 bp.model.lme$coef$fixed
 bp.model.lme$coef$random
 #Extract the covariance matrix for fixed effect.
 bp.model.lme$varFix

*** create a table from random coefficents

Use something like this:
 ranlung<- as.data.frame(lung\_new$coef$random )

** lme4
*** extract part of results of lme4 Objects
**** using arm Packages
install arm package and then use:

#+BEGIN_EXAMPLE
se.coef (object, ...)
se.fixef (object)
se.ranef (object)

#  lmer fit
   M3 <- lmer (y1 ~ x + (1 + x |group))
   se.coef (M3)
   se.fixef (M3)
   se.ranef (M3)
#+END_EXAMPLE
**** using lmerTest
after you load the package and run your lme model you can use the summery output to extract the needed info

#+BEGIN_SRC R
bw270.mod <- lmer(birthw ~ IQRfintempmabirth+elev+(1 |FIPS),data =  bd)
bw270.res<- summary(bw270.mod)
#for sig level
bw270.res$coefficients[2,5]
#+END_SRC
 
**** tract manually

#+BEGIN_SRC R
round(fixef(bw270.mod),3)
# and the standard errors with
round(sqrt(diag(vcov(bw270.mod))),3)
#+END_SRC

*** extract random effect from mer (lme4) objects
**** simple print out random effects (all)

#+begin_src R
#extract random eff
ranef(out.m1_2003)

#or

str(ranef(out.m1))
m1ranef <- data.table(ranef(out.m1)[["day"]], keep.rownames = T)
setnames(m1ranef, c("day", "ranint", "ranslope"))
m1ranef[, day := as.Date(day)]
mod1 <- merge(mod1, m1ranef, by = "day", all.x = T)
#+end_src

**** Extract individual componenets

Use the following code example:

#+BEGIN_SRC sh
#run the lme4 model, $Note$ this is has 2 random slopes and a nested region within a day here 
out.m1_2003 = lmer(PM25 ~ aod+tempc+WDSP+NDVI+dist_PE+(1 +aod+tempc|day/region)) ,data =  m1all)
#take a look with str on the structure of the random slopes, $Note$ in the example we 2 random slopes and a nested region within a day here so this
#will result in the sections the first for the nested random part and the second for the overall aod+temp slopes 

str(ranef(out.m1_2003))
# List of 2
#  $ region:day:'data.frame':	1102 obs. of  3 variables:
#   ..$ (Intercept): num [1:1102] 2.01e-06 1.71e-06 4.39e-07 1.73e-06 3.06e-06 ...
#   ..$ aod        : num [1:1102] -3.933 -2.853 -0.616 -2.778 -6.635 ...
#   ..$ tempc      : num [1:1102] 0.0304 0.0465 0.0169 0.0521 0.0186 ...
#  $ day       :'data.frame':	286 obs. of  3 variables:
#   ..$ (Intercept): num [1:286] 2.47 -2.05 2.15 3.77 3.42 ...
#   ..$ aod        : num [1:286] -1.279 0.219 -0.859 -0.835 0.282 ...
#   ..$ tempc      : num [1:286] -0.034727 -0.000428 -0.056537 -0.029459 0.087472 ...
#  - attr(*, "class")= chr "ranef.mer"

#now we can extract these into a data table with the ranef command. we choose the section type we saw above with str
#so the first will extract the day section and the second the region:day section (the nested)
m1ranef <- data.table(ranef(out.m1_2003)[["day"]], keep.rownames = T)
m1nested <- data.table(ranef(out.m1_2003)[["region:day"]], keep.rownames = T)

#finally we can rebanme the collumns
setnames(m1ranef, c("day", "ranint", "ranslope", "ranslopetemp"))
#+END_SRC
**** plot ranef as a function of predictors

#+begin_src R
ggplot(mod1, aes(daymean, ranslope)) + geom_point()
ggplot(mod1, aes(daymean, AOD)) + geom_point()
#+end_src
 
*** improve convergence with optimize method

use the
 *control=lmeControl(opt = "optim")*
 option for models that dont converge
 example:
 smooth\_T2004\_yearly = lme(pred ~ mpm ,
 random = list(guid= ~1 + mpm ),*control=lmeControl(opt = "optim")*,
 data= GAM\_T2004 )
*** calculate confidance intervals
use the 'confit' command which are part of lme4
#+BEGIN_SRC R
confint(bw270.mod, method="Wald",'IQRfintempmabirth', level=0.95)
#+END_SRC

*** create "tTable/table of fixed effects" using broom
use the package `broom` and the `tidy` function in it like this
#+BEGIN_SRC R
coall.2005<-tidy(m1.fit.2005,effects = "fixed")
#+END_SRC

*** exclude missing values
#+BEGIN_SRC R
out.m1 = lmer(m1.formula ,data =  pm10.m1,na.action = na.exclude)
#+END_SRC
*** extract lme4 object to a latex table

use the The memisc package which does lme4 tables
 Pasted from
<[[http://stackoverflow.com/questions/9407239/how-extract-regression-results-from-lme-lmer-glmer-to-latex][http://stackoverflow.com/questions/9407239/how-extract-regression-results-from-lme-lmer-glmer-to-latex]]>
 Example:
 Rresp\_m25x <- (glmer(count~
ns(date,df=35)+pmnew\_l0+pmnewmayear+temp\_f\_l0+medhhin\_wtd+pctnonwht\_wtd+pctbachorhigher\_wtd+pct65upest+
(1|guid), family = poisson, data = allresp\_m25))
 then:
 library(memisc)
 mtable(Rresp\_m25x, summary.stats=FALSE)
 *This can be used tio show multiple models side by side as well*
 Note:
 you can export that table to lateX with this command:
 toLatex(model1)
 Fore more option see the memsic options HERE:

*** override lmer to give sig levels
To override lmer to give sig levels
#+BEGIN_SRC R
library(lmerTest)

summary(lmer(momwt24 ~ pmpreg + momht + primiparous + momage + mombmiat06 + factor(SES_scored) + factor(educlevels) + (1|firstaodid), data = gestpred))
#+END_SRC

*** weights in lme4
#+BEGIN_SRC R
x<-  lmer(m1.formula,data=pm25.m1,weights=normwt)
#+END_SRC

*** re.form
formula for random effects to include:
If `NULL` > include all random effects
if `NA`   > include no random effects

example:

#+BEGIN_SRC R
mod1d_10_s10$predicted <- predict(object=out_90_s10,newdata=mod1d_10_s10,allow.new.levels=TRUE,re.form=NULL )

#+END_SRC

*** specify nesting

use this syntax, using a + after the last fixed variable:

#+BEGIN_SRC R
(1+Slope_variable|INTERCEPT_VAR/NESTED_VAR)
#+END_SRC

here is a complex example using several random statments

#+BEGIN_SRC R
CO+as.factor(year)+iregg+nitro+(1|Paircount)+(1+CO|crop)+(1|crop/Cultivar),na.action=na.omit,
data = zinc)
#+END_SRC

*** Mixed Effects Logistic Regression (glmer)
**** Background
see [[file:KSG.org::*Background%20of%20mixed%20logistic][Background of mixed logistic]]
**** glmer
We use the glmer command to estimate a mixed effects logistic
regression models.

`if you have a numeric 0/1 response, you are predicting 1's and if you use a two level factor, you are predicting the second level of the factor`
example formula

#+begin_src r
m <- glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +
    (1 | DID), data = hdp, family = binomial)

# print the mod results without correlations among fixed effects
print(m, corr = FALSE)
#+end_src

output:
#+begin_src
## Generalized linear mixed model fit by maximum likelihood ['glmerMod']
##  Family: binomial ( logit )
## Formula: remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID)
##    Data: hdp
##
##      AIC      BIC   logLik deviance
##     7397     7461    -3690     7379
##
## Random effects:
##  Groups Name        Variance Std.Dev.
##  DID    (Intercept) 4.06     2.02
## Number of obs: 8525, groups: DID, 407
##
## Fixed effects:
##                Estimate Std. Error z value Pr(>|z|)
## (Intercept)     -2.0546     0.5244   -3.92  8.9e-05 ***
## IL6             -0.0567     0.0114   -4.99  6.0e-07 ***
## CRP             -0.0215     0.0101   -2.14  0.03257 *
## CancerStageII   -0.4142     0.0745   -5.56  2.8e-08 ***
## CancerStageIII  -1.0032     0.0968  -10.36  < 2e-16 ***
## CancerStageIV   -2.3364     0.1569  -14.89  < 2e-16 ***
## LengthofStay    -0.1212     0.0331   -3.66  0.00026 ***
## Experience       0.1202     0.0272    4.42  9.7e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#+end_src

The first part tells us the estimates are based on an adaptive
Gaussian Hermite approximation of the likelihood.

The next section gives us basic information that can be used to
compare models, followed by the random effect estimates. This
represents the estimated variability in the intercept on the logit
scale. Had there been other random effects, such as random slopes,
they would also appear here. The top section concludes with the total
number of observations, and the number of level 2 observations.

The last section is a table of the fixed effects estimates. For many applications, these are what people are primarily interested in. The estimates represent the regression coefficients. These are unstandardized and are on the logit scale. The estimates are followed by their standard errors (SEs). As is common in GLMs, the SEs are obtained by inverting the observed information matrix (negative second derivative matrix). However, for GLMMs, this is again an approximation. The approximations of the coefficient estimates likely stabilize faster than do those for the SEs. Thus if you are using fewer integration points, the estimates may be reasonable, but the approximation of the SEs may be less accurate. The Wald tests, EstimateSE, rely on asymptotic theory, here referring to as the highest level unit size converges to infinity, these tests will be normally distributed, and from that, p values (the probability of obtaining the observed estimate or more extreme, given the true estimate is 0).

It can be nice to get confidence intervals (CIs). We can get rough estimates using the SEs.

#+begin_src r
se <- sqrt(diag(vcov(m)))
# table of estimates with 95% CI
(tab <- cbind(Est = fixef(m), LL = fixef(m) - 1.96 * se, UL = fixef(m) + 1.96 *
    se))
##                     Est       LL        UL
## (Intercept)    -2.05457 -3.08248 -1.026674
## IL6            -0.05675 -0.07903 -0.034469
## CRP            -0.02155 -0.04131 -0.001788
## CancerStageII  -0.41417 -0.56028 -0.268064
## CancerStageIII -1.00318 -1.19293 -0.813435
## CancerStageIV  -2.33645 -2.64404 -2.028862
## LengthofStay   -0.12116 -0.18613 -0.056190
## Experience      0.12018  0.06693  0.173429

##If we wanted odds ratios instead of coefficients on the logit scale, we could exponentiate the estimates and CIs.

exp(tab)
##                    Est      LL     UL
## (Intercept)    0.12815 0.04585 0.3582
## IL6            0.94483 0.92402 0.9661
## CRP            0.97868 0.95954 0.9982
## CancerStageII  0.66089 0.57105 0.7649
## CancerStageIII 0.36671 0.30333 0.4433
## CancerStageIV  0.09667 0.07107 0.1315
## LengthofStay   0.88589 0.83017 0.9454
## Experience     1.12770 1.06922 1.1894

#+end_src

** Generalized Linear Models
*** Intro

*Generalized Linear Models*
 Generalized linear models are fit using the glm( ) function. The form
of the glm function is
 glm(formula, family=familytype(link=linkfunction), data=)
 [[R_files/img/Image_OkkYzf4uBVFDlBVp3o1OVA_0002.jpg]]
 See help(glm) for other modeling options. See help(family) for other
allowable link functions for each family. Three subtypes of generalized
linear models will be covered here: logistic regression, poisson
regression, and survival analysis.

*** Logistic Regression

#+BEGIN_SRC R
fit <- glm(NSGA ~ IQRfintempmabirth,data=bd,family=binomial)
#+END_SRC

`if you have a numeric 0/1 response, you are predicting 1's and if you use a two level factor, you are predicting the second level of the factor`

#+BEGIN_EXAMPLE
summary(fit) # display results
confint(fit) # 95% CI for the coefficients
exp(coef(fit)) # exponentiated coefficients
exp(confint(fit)) # 95% CI for exponentiated coefficients
predict(fit, type="response") # predicted values
residuals(fit, type="deviance") # residuals
#+END_EXAMPLE

You can use anova(fit1,fit2, test="Chisq") to compare nested models.
Additionally, cdplot(F~x, data=mydata) will display the conditional
density plot of the binary outcome F on the continuous x variable.

A typical predictor has the form:

#+BEGIN_EXAMPLE
response ~ terms
#+END_EXAMPLE

where response is the (numeric) response vector and terms is a series of terms which specifies a linear predictor for response.
For binomial and quasibinomial families the response can also be specified as a factor (when the first level denotes failure and all others success) or as a two-column matrix with the columns giving the numbers of successes and failures, thus if you have a numeric 0/1 response, you are predicting 1's and if you use a two level factor, you are predicting the second level of the factor



*** Poisson Regression

*Poisson Regression*
 Poisson regression is useful when predicting an outcome variable
representing counts from a set of continuous predictor variables.
 # Poisson Regression
 # where count is a count and
 # x1-x3 are continuous predictors
 fit <- glm(count ~ x1+x2+x3, data=mydata, family=poisson())
 summary(fit) display results
 If you have overdispersion (see if residual deviance is much larger
than degrees of freedom), you may want to use quasipoisson() instead of
poisson().

*** Survival Analysis
*Survival Analysis*
Survival analysis (also called event history analysis or reliability
analysis) covers a set of techniques for modeling the time to an event.
Data may be right censored - the event may not have occured by the end
of the study or we may have incomplete information on an observation but
know that up to a certain time the event had not occured (e.g. the
participant dropped out of study in week 10 but was alive at that time).
 While generalized linear models are typically analyzed using the glm( )
function, survival analyis is typically carried out using functions from
the survival package . The survival package can handle one and two
sample problems, parametric accelerated failure models, and the Cox
proportional hazards model.
 Data are typically entered in the format start time, stop time, and
status (1=event occured, 0=event did not occur). Alternatively, the data
may be in the format time to event and status (1=event occured, 0=event
did not occur). A status=0 indicates that the observation is right
cencored. Data are bundled into a Surv object via the Surv( ) function
prior to further analyses.
 survfit( ) is used to estimate a survival distribution for one or more
groups.
 survdiff( ) tests for differences in survival distributions between two
or more groups.
 coxph( ) models the hazard function on a set of predictor variables.
 # Mayo Clinic Lung Cancer Data
 library(survival)
 # learn about the dataset
 help(lung)
 # create a Surv object
 survobj <- with(lung, Surv(time,status))
 # Plot survival distribution of the total sample
 # Kaplan-Meier estimator
 fit0 <- survfit(survobj, data=lung)
 summary(fit0)
 plot(fit, xlab="Survival Time in Days",
 ylab="% Surviving", yscale=100,
 main="Survival Distribution (Overall)")
 # Compare the survival distributions of men and women
 fit1 <- survfit(survobj~sex,data=lung)
 # plot the survival distributions by sex
 plot(fit1, xlab="Survival Time in Days",
 ylab="% Surviving", yscale=100, col=c("red","blue"),
 main="Survival Distributions by Gender")
 legend("topright", title="Gender", c("Male", "Female"),
 fill=c("red", "blue"))
 # test for difference between male and female
 # survival curves (logrank test)
 survdiff(survobj~sex, data=lung)
 # predict male survival from age and medical scores
 MaleMod <- coxph(survobj~age+ph.ecog+ph.karno+pat.karno,
 data=lung, subset=sex==1)
 # display results
 MaleMod
 # evaluate the proportional hazards assumption
 cox.zph(MaleMod)
 [[R_files/img/Image_HWJvoZQ7B01N4Wyljn6c4A_0005.jpg]]
 [[R_files/img/Image_HWJvoZQ7B01N4Wyljn6c4A_0006.jpg]]

*** predictions

**** find the predicted values

we could find the predicted values, using the fitted.values() function.

-----------------------------------------------------------------------------------------------------------------------
 glm.linear.preds <- fitted.values(MODEL\_NAME)

-----------------------------------------------------------------------------------------------------------------------
 glm.linear.preds <- fitted.values(glm.linear)

**** predict in a binomial model

To get predictions in a binary logisitc model you need to add the
type-repsonse option since its not a linear prediction
 IE:
 #get probability prediction , note that its a binary logisitc and thus
the type-repsonse option
 T2001\_weight$prob <- predict(w1,type = c("response"))

*** basic GLM regression

The basic regression command in R is GLM (general, or generalised,
linear model). We use the command:

-----------------------------------------------------------------------------------------------------------------------
 glm(outcome(DV) ~ predictor1 + predictor2 + predictor3, data= NAME )

-----------------------------------------------------------------------------------------------------------------------
 Example:
 the analysis we want to do is:
 glm(ANX ~ HASSLES)
 And R outputs:

-----------------------------------------------------------------------------------------------------------------------
 Call: glm(formula = ANX ~ HASSLES)
 Coefficients:
 (Intercept) HASSLES
 5.4226 0.2526
 Degrees of Freedom: 39 Total (i.e. Null); 38 Residual
 Null Deviance: 4627
 Residual Deviance: 2159 AIC: 279

-----------------------------------------------------------------------------------------------------------------------
 But this is now gone - we have the estimates, but no more. We need to
have the output stored somewhere, so we can do something with it. To do
this, we use the assignment operator, as before:

-----------------------------------------------------------------------------------------------------------------------
 glm.linear <- glm(ANX ~ HASSLES)

-----------------------------------------------------------------------------------------------------------------------
 We are naming the output of our regression glm.linear - in R language,
we are creating an object, called glm.linear . If you are used to almost
any other program, the result will seem a little strange, because there
isn't any. If you want to know what happened, you have to ask.
 One way to ask is to just type the name of the object, and the object
will be give to you. (Just like when we typed the name of the dataset,
the dataset was output).

-----------------------------------------------------------------------------------------------------------------------
 > glm.linear
 Call: glm(formula = ANX ~ HASSLES)
 Coefficients:
 (Intercept) HASSLES
 5.4226 0.2526
 Degrees of Freedom: 39 Total (i.e. Null); 38 Residual
 Null Deviance: 4627
 Residual Deviance: 2159 AIC: 279

-----------------------------------------------------------------------------------------------------------------------
 But we can do other things with the object. The function summary(), for
example, when applied to a variable, gave a certain kind of result. When
applied to a dataset, it gave a different kind of result. When applied
to a glm object, it gives a different kind of result:

-----------------------------------------------------------------------------------------------------------------------
 > summary(glm.linear)
 Call:
 glm(formula = ANX ~ HASSLES)
 Deviance Residuals:
 Min 1Q Median 3Q Max
 -13.3153 -5.0549 -0.3794 4.5765 17.5913
 Coefficients:
 Estimate Std. Error t value Pr(>|t|)
 (Intercept) 5.42265 2.46541 2.199 0.034 *
 HASSLES 0.25259 0.03832 6.592 8.81e-08 ***
 ---
 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 '' 1
 (Dispersion parameter for gaussian family taken to be 56.80561)
 Null deviance: 4627.1 on 39 degrees of freedom
 Residual deviance: 2158.6 on 38 degrees of freedom
 AIC: 279.05
 Number of Fisher Scoring iterations: 2

-----------------------------------------------------------------------------------------------------------------------
 And that is what we would consider to be our answer from the
regression.
*** weights

to add a weight use:
 *weights=WEIGHTVARIABLE*
 IE:
 glm (tot\_cigarettes\_tr ~ sex + age + relig + educ +resistance +
discrimination + alienation ,weights=weight, data=roni\_r2, dist =
"negbin", na.action=na.omit)

*** extract items from objects (coefs..etc)
**** extract coeficents
use the coef option. you can specify name and/or position

#+BEGIN_SRC R
coef(pt270.res)[2,"Estimate"]
coef(pt270.res)[2,"Std. Error"]
coef(pt270.res)[2,3]
#+END_SRC

 
* Advanced Statistics
** calculate odds-ratio
*** calculate odds-ratio and CI

Extract the coefficients from the final model and compute odds ratio.
 Estimate the effect of a 10 mmHg change in systolic blood pressure
 beta<- bp.model.gamm1$lme$coef$fixed[10]
 se<- sqrt (bp.model.gamm1$lme$varFix[10,10])
 OR.BP<-exp(beta*10)
 lo95ci.BP<- exp((beta-1.96*se)*10)
 hi95ci.BP<- exp((beta+1.96*se)*10)

 another example:
 Extract the coefficients from the final model and compute odds ratio.
Estimate the effect of a 10 mmHg change in systolic blood pressure
 beta<-model6$coef[11]
 se<-sqrt(summary(model6)$cov.unscaled[11:11,11:11]) #extracts the
covariance matrix
 OR.BP<-exp(beta*10)
 lo95ci.BP<- exp((beta-1.96*se)*10)
 hi95ci.BP<- exp((beta+1.96*se)*10)
 For a Binary variable:
 Extract the coefficient for sex (men 0; women 1) from the final model
and compute odds ratio
 beta<-model6$coef[2]
 se<-sqrt(summary(model6) $cov.unscaled[2:2,2:2])
 OR.female<-exp(beta)
 lo95ci.female<- exp((beta-1.96*se))
 hi95ci.female<- exp((beta+1.96*se))
 ##for men (sex=0)
 or.men<-exp(-beta)
 lo95ci.male<- exp((-beta-1.96*se))
 hi95ci.male<- exp((-beta+1.96*se))

** Mixed models
*** GAM Mixed modles
**** basics

Another function which can be used to estimate mixed effect model is the
function GAMM, from the package mgcv.
 This function estimates generalized additive mixed models and allow
using penalized splines.
 The limitation is that only GCV can be used to select the degrees of
freedom with penalized spline.
 This function is composed of two objects: the lme part gives you the
mixed model results, and the gam part which gives you the gam part with
the smoothing results.
 We can then use the same model above for blood pressure, with family
Gaussian, without spline
 to assign a *random intercept* to a model use this:
 random=list(VARIABLE=~1)
 bp.model.gamm<-gamm(log(bp1sys)~ sex + as.factor(race) + smokpkyr +
bmi+ as.factor(booze) + age+ ser.chol, random=list(locode=~1),
family=gaussian(), data=hanesna)
 summary(bp.model.gamm$lme)

to assign a random intercept and slope to a model use this:

#+BEGIN_EXAMPLE
random=list(Intercept_VARIABLE=~1+Slope_variable)
#+END_EXAMPLE

example:
#+BEGIN_SRC R
bp.model.gamm<-gamm(log(bp1sys)~ sex + as.factor(race) + smokpkyr + bmi+ as.factor(booze) + age+ ser.chol, random=list(locode=~1+age),
family=gaussian(), data=hanesna)
 summary(bp.model.gamm$lme)
#+END_SRC

**** Extract objects

to extract the random and fixed effects coefficients:
 bp.model.gamm$lme$coef
 to extract the random coefficient:
 bp.model.gamm$lme$coef$random
 to extract the fixed effect coefficient:
 bp.model.gamm$lme$coef$fixed
 to extract the covariance matrix of the fixed effects:
 bp.model.gamm$lme$varFix
 Extract the coefficient and the standard error for cholesterol:
 bp.model.gamm$lme$coef$fixed[10]
 sqrt(bp.model.gamm$lme$varFix[10,10])

***** extract standart errors

use this sytnax:
 sqrt(summary(try)$cov.scaled[2:2,2:2])

**** Include a different correlation structure

 Include a different correlation structure such as a compound symmetry
or an autoregressive of order 1.
 bp.model.gamm1<-gamm(d.total~ sex + as.factor(race) + smokpkyr +
s(bmi)+ as.factor(booze) + age+ ser.chol+bp1sys, random=list(locode=~1),
family=binomial(), data=hanesna, correlation= corCompSymm(form=~1
| locode)) |
 summary(bp.model.gamm1$lme)
 bp.model.gamm1<-gamm(d.total~ sex + as.factor(race) + smokpkyr +
s(bmi)+ as.factor(booze) + age+ ser.chol+bp1sys, random=list(locode=~1),
family=binomial(), data=hanesna, correlation= corAR1(form=~1 |locode))
 summary(bp.model.gamm1$lme)

**** run a logistic (binomal) mixed model

#+BEGIN_SRC R
p.model.gamm1<-gamm(d.total~ sex + as.factor(race) + smokpkyr + s(bmi,bs='cr')+ as.factor(booze) + s(age,bs='cr')+ s(ser.chol,bs='cr')+s(bp1sys,bs='cr'), random=list(locode=~1), family=binomial(), data=hanesna) 
#+END_SRC

*** gamm4
**** run a mixed model with nested regions

#+BEGIN_SRC R
try<-gamm4(PM2_5MEA~s(POP00_SQ) +s(ELEV_M) +s(MJRRDDEN) +
            s(AOD1_MEA,TEMPC)  +s(NDVI) + as.factor(WSS),random=~ (1|date/region),
          data=basicregion)
#+END_SRC

*** posion
**** extract the standart error for an interaction

To extract the SE for the interaction use this code:
 STDERROR=sqrt(try$varFix[46,46]+try$varFix[2,2]+2*try$varFix[46,2])

** Splines
*** Gam Splines
**** natural spline

#Or with natural spline, here fx=TRUE and k=number of df+1

#+BEGIN_SRC R
bp.model.gamm<-gamm(log(bp1sys)~ sex + as.factor(race) + smokpkyr +
bmi+ as.factor(booze) +s(age,k=4,fx=TRUE) + ser.chol,
random=list(locode=~1), family=gaussian(), data=hanesna)
#+END_SRC

**** penalized spline, with GCV choosing the df

penalized spline, with GCV choosing the df:

#+BEGIN_SRC sh
bp.model.gamm<-gamm(log(bp1sys)~ sex + as.facto1r(race) + smokpkyr +
bmi+ as.factor(booze) + *s(age,bs='cr')*+ ser.chol,
random=list(locode=~1), family=gaussian(), data=hanesna)
#+END_SRC
**** plot gam output
#+BEGIN_SRC R
par(mfrow=c(2,2)) 
plot(bp.model.ps, main = "2003 model")
#+END_SRC
**** penelized Vs natural
fx=false is for penelized splines
fx=True is for natural splines
bs='cr' stands for cubic splines
in natural splines k=X where X is the df-1

*** Intro

**** b splines

bs(x, df = NULL, knots = NULL, degree = 3, intercept = FALSE,...)
 generate the B-spline basis matrix for a polynomial spline. df:
degrees of freedom; one can specify 'df' rather than 'knots'; 'bs()'
then chooses 'df-degree-1' knots at suitable quantiles of 'x'.

**** natural splines

ns(x, df = NULL, knots = NULL, intercept = FALSE,...)
 generate the B-spline basis matrix for a natural cubic spline. df:
degrees of freedom. One can supply 'df' rather than knots; 'ns()' then
chooses 'df - 1 - intercept' knots at suitably chosen quantiles of 'x'.

**** Penalized splines

*Penalized splines*in R have been developed by Simon Wood and can be
estimated in a generalized additive model through the Multiple
Generalized Cross Validation or "mgcv" package.
 The s() function is used in definition of smooth terms within the
'gam' model formulae.
 s(x,y, k=-1,fx=FALSE,bs="tp",m=0,by=NA)
 It estimates an un-penalized regression spline or a penalized
regression spline. It can estimate both a single or 2 covariate spline
x, y: a list of variables that are the covariates that this smooth is a
function of. k is the dimension of the basis used to represent the
smooth term, that is the number of knots. fx indicates whether the term
is a fixed df cubic regression spline ('TRUE') or a penalized regression
spline ('FALSE'). bs: this can be '"cr"' for a cubic regression spline
or '"tp" for a thin plate regression spline.
 Penalized spline can be fit using Generalized Additive Models (gam).
These models will be introduced later; when the formula is linear model,
and the family is gaussian (default) then the fit is exactly as the
linear model.
 When an un-penalized regression spline is fit, then k (the number of
knots) is equal to the number of degrees of freedom+1. Therefore if you
specify k=6 then it estimate a 5 df regression spline. In a penalized
regression spline k should be bigger than the number of degrees of
freedom and these will be estimated by mgcv using Generalized Cross
Validation (GCV) or the Unbiased Risk Estimator (UBRE) criteria. If
instead you want to fix the number of degrees of freedom, it is possible
to use the sp option by defining the smoothing parameters which
correspond to the desired degrees of freedom.
 sp: A vector of smoothing parameters for each term can be provided
here. Smoothing parameters must be supplied in the order that the smooth
terms appear in the model formula.
 The number of df is in inverse relation to the span, therefore if the
estimated df by the model is large, you'll need to increase the span to
decrease the number of df
 [[R_files/img/Image_3v76gDRdZMXU0OFe50DJWA_0002.jpg]]

**** summary of models and spline in r

[[R_files/img/Image_fyGNhYVcuYYmOTuqmuBmPQ_0002.jpg]]

*** export spline variable to other stat apps

create the spline for the variable needed and use the *data.frame*
option:
 test<- data.frame(ns(ts0004lag$date,df=45))
 this will create variables per degrees of freedom (so in the above
example 45 new variables in 'test')
 then cbind it back to the original dataset
 test2<-cbind(ts0004lag,test)
 finally export it to the format of choice (csv etc)

* Plots/Graphs
** Basics
*** create a new windows for each graph

Creating a new graph will typically overwrite a previous graph. How can
you create more than one graph and still have access to each? There are
several methods. First, you can open a new graph window before creating
a new graph:
 dev.new()
 statements to create graph 1
 dev.new()
 statements to create a graph 2
 Each new graph will appear in the most recently opened window.
 you can use the functions dev.new(), dev.next(), dev.prev(), dev.set(),
and dev.off() to have multiple graph windows open at one time and choose
which output are sent to which windows. This approach works on any
platform. See help(dev.cur) for details on this approach.

*** Parameters for specifying symbols and lines

[[R_files/img/Image_XFetaHYysTPWBYkSn8Fqxg_0005.png]]
 example:
 plot(VAR1, VAR2, type="b", lty=3, lwd=3, pch=15, cex=2)
 [[R_files/img/Image_XFetaHYysTPWBYkSn8Fqxg_0006.png]]

*** Colors

[[R_files/img/Image_2qSsatbpSVQy5JuxXMeSDQ_0005.png]]
 You can specify colors in R by index, name, hexadecimal, RGB, or HSV.
For example, col=1, col="white", col="#FFFFFF", col=rgb(1,1,1), and
col=hsv(0,0,1) are equivalent ways of specifying the color white.
 [[R_files/img/Image_2qSsatbpSVQy5JuxXMeSDQ_0006.png]]

*** Text characteristics

[[R_files/img/Image_VKB3cctneYeyzax9FxriaQ_0005.png]]
 For example, all graphs created after the statement
 par(font.lab=3, cex.lab=1.5, font.main=4, cex.main=2)
 will have italic axis labels that are 1.5 times the default text size,
and bold italic titles that are twice the default text size.
 [[R_files/img/Image_VKB3cctneYeyzax9FxriaQ_0006.png]]

*** Graph and margin dimensions

Finally, you can control the plot dimensions and margin sizes using
parameters
 [[R_files/img/Image_7yXssVDVIQstNlmcKnKPCQ_0003.png]]
 IE:
 par(pin=c(4,3), mai=c(1,.5, 1, .2))

** Manipulate graphs

*** Axes and Text

**** Axes

*Axes*
 You can create custom axes using the axis( ) function.
 axis(side, at=, labels=, pos=, lty=, col=, las=, tck=, ...)
 where
 option description
 side an integer indicating the side of the graph to draw the axis
(1=bottom, 2=left, 3=top, 4=right)
 at a numeric vector indicating where tic marks should be drawn
 labels a character vector of labels to be placed at the tickmarks
 (if NULL, the at values will be used)
 pos the coordinate at which the axis line is to be drawn.
 (i.e., the value on the other axis where it crosses)
 lty line type
 col the line and tick mark color
 las labels are parallel (=0) or perpendicular(=2) to axis
 tck length of tick mark as fraction of plotting region (negative
number is outside graph, positive number is inside, 0 suppresses ticks,
1 creates gridlines) default is -0.01
 (...) other graphical parameters
 If you are going to create a custom axis, you should suppress the axis
automatically generated by your high level plotting function. The option
axes=FALSE suppresses both x and y axes. xaxt="n" and yaxt="n" suppress
the x and y axis respectively. Here is a (somewhat overblown) example.
 # A Silly Axis Example
 # specify the data
 x <- c(1:10); y <- x; z <- 10/x
 # create extra margin room on the right for an axis
 par(mar=c(5, 4, 4, 8) + 0.1)
 # plot x vs. y
 plot(x, y,type="b", pch=21, col="red",
 yaxt="n", lty=3, xlab="", ylab="")
 # add x vs. 1/x
 lines(x, z, type="b", pch=22, col="blue", lty=2)
 # draw an axis on the left
 axis(2, at=x,labels=x, col.axis="red", las=2)
 # draw an axis on the right, with smaller text and ticks
 axis(4, at=z,labels=round(z,digits=2),
 col.axis="blue", las=2, cex.axis=0.7, tck=-.01)
 # add a title for the right axis
 mtext("y=1/x", side=4, line=3, cex.lab=1,las=2, col="blue")
 # add a main title and bottom and left axis labels
 title("An Example of Creative Axes", xlab="X values",
 ylab="Y=X")

**** general

*Axes and Text*
 Many high level plotting functions (plot, hist, boxplot, etc.) allow
you to include axis and text options (as well as other graphical
paramters). For example
 # Specify axis options within plot()
 plot(x, y, main="title", sub="subtitle",
 xlab="X-axis label", ylab="y-axix label",
 xlim=c(xmin, xmax), ylim=c(ymin, ymax))
 For finer control or for modularization, you can use the functions
described below.
 Titles
 Use the title( ) function to add labels to a plot.
 title(main="main title", sub="sub-title",
 xlab="x-axis label", ylab="y-axis label")
 Many other graphical parameters (such as text size, font, rotation, and
color) can also be specified in the title( ) function.
 # Add a red title and a blue subtitle. Make x and y
 # labels 25% smaller than the default and green.
 title(main="My Title", col.main="red",
 sub="My Sub-title", col.sub="blue",
 xlab="My X label", ylab="My Y label",
 col.lab="green", cex.lab=0.75)

**** Legend

*Legend
*
 Add a legend with the legend() function.
 legend(location, title, legend, ...)
 Common options are described below.
 option description
 location There are several ways to indicate the location of the
legend. You can give an x,y coordinate for the upper left hand corner of
the legend. You can use locator(1), in which case you use the mouse to
indicate the location of the legend. You can also use the keywords
"bottom", "bottomleft", "left", "topleft", "top", "topright", "right",
"bottomright", or "center". If you use a keyword, you may want to use
inset= to specify an amount to move the legend into the graph (as
fraction of plot region).
 title A character string for the legend title (optional)
 legend A character vector with the labels
 ... Other options. If the legend labels colored lines, specify col=
and a vector of colors. If the legend labels point symbols, specify pch=
and a vector of point symbols. If the legend labels line width or line
style, use lwd= or lty= and a vector of widths or styles. To create
colored boxes for the legend (common in bar, box, or pie charts), use
fill= and a vector of colors.
 Other common legend options include bty for box type, bg for background
color, cex for size, and text.col for text color. Setting horiz=TRUE
sets the legend horizontally rather than vertically.
 # Legend Example
 attach(mtcars)
 boxplot(mpg~cyl, main="Milage by Car Weight",
 yaxt="n", xlab="Milage", horizontal=TRUE,
 col=terrain.colors(3))
 legend("topright", inset=.05, title="Number of Cylinders",
 c("4","6","8"), fill=terrain.colors(3), horiz=TRU
 [[R_files/img/Image_n5EUBvQxmxdxTWKXqwyTaA_0003.jpg]]

**** Reference Lines

*Reference Lines
*Add reference lines to a graph using the abline( ) function.
 abline(h=yvalues, v=xvalues)
 Other graphical parameters (such as line type, color, and width) can
also be specified in the abline( ) function.
 # add solid horizontal lines at y=1,5,7
 abline(h=c(1,5,7))
 # add dashed blue verical lines at x = 1,3,5,7,9
 abline(v=seq(1,10,2),lty=2,col="blue")
 Note: You can also use the grid( ) function to add reference lines.

**** Text Annotations

*Text Annotations*
 Text can be added to graphs using the text( ) and mtext( ) functions.
text( ) places text within the graph while mtext( ) places text in one
of the four margins.
 text(location, "text to place", pos, ...)
 mtext("text to place", side, line=n, ...)
 Common options are described below.
 option description
 location location can be an x,y coordinate. Alternatively, the text
can be placed interactively via mouse by specifying location as
locator(1).
 pos position relative to location. 1=below, 2=left, 3=above,
4=right. If you specify pos, you can specify offset= in percent of
character width.
 side which margin to place text. 1=bottom, 2=left, 3=top, 4=right.
you can specify line= to indicate the line in the margin starting with 0
and moving out. you can also specify adj=0 for left/bottom alignment or
adj=1 for top/right alignment.
 Other common options are cex, col, and font (for size, color, and font
style respectively).
 Labeling points
 You can use the text( ) function (see above) for labeling point as well
as for adding other text annotations. Specify location as a set of x, y
coordinates and specify the text to place as a vector of labels. The x,
y, and label vectors should all be the same length.
 # Example of labeling points
 attach(mtcars)
 plot(wt, mpg, main="Milage vs. Car Weight",
 xlab="Weight", ylab="Mileage", pch=18, col="blue")
 text(wt, mpg, row.names(mtcars), cex=0.6, pos=4, col="red")
 labeling points click to view
 Math Annotations
 You can add mathematically formulas to a graph using TEX-like rules.
See help(plotmath) for details and examples.

*** Bar plots

*Bar Plots*
 Create barplots with the barplot(height) function, where height is a
vector or matrix. If height is a vector, the values determine the
heights of the bars in the plot. If height is a matrix and the option
beside=FALSE then each bar of the plot corresponds to a column of
height, with the values in the column giving the heights of stacked
"sub-bars". If height is a matrix and beside=TRUE, then the values in
each column are juxtaposed rather than stacked. Include option
names.arg=(character vector) to label the bars. The option horiz=TRUE to
createa a horizontal barplot.
 *Simple Bar Plot*
 # Simple Bar Plot
 counts <- table(mtcars$gear)
 barplot(counts, main="Car Distribution",
 xlab="Number of Gears")
 [[R_files/img/Image_FRtHwRZpeQuyiUe78TP0PQ_0008.jpg]]
 # Simple Horizontal Bar Plot with Added Labels
 counts <- table(mtcars$gear)
 barplot(counts, main="Car Distribution", horiz=TRUE,
 names.arg=c("3 Gears", "4 Gears", "5 Gears"))
 [[R_files/img/Image_FRtHwRZpeQuyiUe78TP0PQ_0009.jpg]]
 *Stacked Bar Plot*
 # Stacked Bar Plot with Colors and Legend
 counts <- table(mtcars$vs, mtcars$gear)
 barplot(counts, main="Car Distribution by Gears and VS",
 xlab="Number of Gears", col=c("darkblue","red"),
 legend = rownames(counts))
 [[R_files/img/Image_FRtHwRZpeQuyiUe78TP0PQ_0010.jpg]]
 *Grouped Bar Plot*
 # Grouped Bar Plot
 counts <- table(mtcars$vs, mtcars$gear)
 barplot(counts, main="Car Distribution by Gears and VS",
 xlab="Number of Gears", col=c("darkblue","red"),
 legend = rownames(counts),
beside=TRUE[[R_files/img/Image_FRtHwRZpeQuyiUe78TP0PQ_0011.jpg]])
 [[R_files/img/Image_FRtHwRZpeQuyiUe78TP0PQ_0012.jpg]]
 *
 Note*s
 Bar plots need not be based on counts or frequencies. You can create
bar plots that represent means, medians, standard deviations, etc. Use
the aggregate( ) function and pass the results to the barplot( )
function.
 By default, the categorical axis line is suppressed. Include the option
axis.lty=1 to draw it.
 With many bars, bar labels may start to overlap. You can decrease the
font size using the cex.names = option. Values smaller than one will
shrink the size of the label. Additionally, you can use graphical
parameters such as the following to help text spacing:
 # Fitting Labels
 par(las=2) # make label text perpendicular to axis
 par(mar=c(5,8,4,2)) # increase y-axis margin.
 counts <- table(mtcars$gear)
 barplot(counts, main="Car Distribution", horiz=TRUE, names.arg=c("3
Gears", "4 Gears", "5 Gears"), cex.names=0.8)
 Barplot with graphic parameters click to view

*** Boxplots

Boxplots
 Boxplots can be created for individual variables or for variables by
group. The format is boxplot(x, data=), where x is a formula and data=
denotes the dataframe providing the data. An example of a formula is
y~group where a separate boxplot for numeric variable y is generated for
each value of group. Add varwidth=TRUE to make boxplot widths
proportional to the square root of the samples sizes. Add
horizontal=TRUE to reverse the axis orientation.
 # Boxplot of MPG by Car Cylinders
 boxplot(mpg~cyl,data=mtcars, main="Car Milage Data",
 xlab="Number of Cylinders", ylab="Miles Per Gallon")
 [[R_files/img/Image_4ffTKWxmNpsunnSoEem3ZA_0003.jpg]]
 # Notched Boxplot of Tooth Growth Against 2 Crossed Factors
 # boxes colored for ease of interpretation
 boxplot(len~supp*dose, data=ToothGrowth, notch=TRUE,
 col=(c("gold","darkgreen")),
 main="Tooth Growth", xlab="Suppliment and Dose")
 [[R_files/img/Image_4ffTKWxmNpsunnSoEem3ZA_0004.jpg]]
 In the notched boxplot, if two boxes' notches do not overlap this is
'strong evidence' their medians differ (Chambers et al., 1983, p. 62).
 Colors recycle. In the example above, if I had listed 6 colors, each
box would have its own color. Earl F. Glynn has created an easy to use
list of colors is PDF format.

*** Graphical Parameters

**** colors

 colors
 Options that specify colors include the following.
 option description
 col Default plotting color. Some functions (e.g. lines) accept a
vector of values that are recycled.
 col.axis color for axis annotation
 col.lab color for x and y labels
 col.main color for titles
 col.sub color for subtitles
 fg plot foreground color (axes, boxes - also sets col= to same)
 bg plot background color
 You can specify colors in R by index, name, hexadecimal, or RGB.
 For example col=1, col="white", and col="#FFFFFF" are equivalent.
 The following chart was produced with code developed by Earl F. Glynn.
See his Color Chart for all the details you would ever need about using
colors in R. There are wonderful color schemes at graphviz.
 You can also create a vector of n contiguous colors using the functions
rainbow(n), heat.colors(n), terrain.colors(n), topo.colors(n), and
cm.colors(n).
 colors() returns all available color names.

**** Combining Plots

*Combining Plots*
 R makes it easy to combine multiple plots into one overall graph, using
either the
 par( ) or layout( ) function.
 With the par( ) function, you can include the option mfrow=c(nrows,
ncols) to create a matrix of nrows x ncols plots that are filled in by
row. mfcol=c(nrows, ncols) fills in the matrix by columns.
 # 4 figures arranged in 2 rows and 2 columns
 attach(mtcars)
 par(mfrow=c(2,2))
 plot(wt,mpg, main="Scatterplot of wt vs. mpg")
 plot(wt,disp, main="Scatterplot of wt vs disp")
 hist(wt, main="Histogram of wt")
 boxplot(wt, main="Boxplot of wt")
 # 3 figures arranged in 3 rows and 1 column
 attach(mtcars)
 par(mfrow=c(3,1))
 hist(wt)
 hist(mpg)
 hist(disp)
 The layout( ) function has the form layout(mat) where
 mat is a matrix object specifying the location of the N figures to
plot.
 # One figure in row 1 and two figures in row 2
 attach(mtcars)
 layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
 hist(wt)
 hist(mpg)
 hist(disp)
 Optionally, you can include widths= and heights= options in the layout(
) function to control the size of each figure more precisely. These
options have the form
 widths= a vector of values for the widths of columns
 heights= a vector of values for the heights of rows.
 Relative widths are specified with numeric values. Absolute widths (in
centimetres) are specified with the lcm() function.
 # One figure in row 1 and two figures in row 2
 # row 1 is 1/3 the height of row 2
 # column 2 is 1/4 the width of the column 1
 attach(mtcars)
 layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE),
 widths=c(3,1), heights=c(1,2))
 hist(wt)
 hist(mpg)
 hist(disp)
 See help(layout) for more details.
 creating a figure arrangement with fine control
 In the following example, two box plots are added to scatterplot to
create an enhanced graph.
 # Add boxplots to a scatterplot
 par(fig=c(0,0.8,0,0.8), new=TRUE)
 plot(mtcars$wt, mtcars$mpg, xlab="Miles Per Gallon",
 ylab="Car Weight")
 par(fig=c(0,0.8,0.55,1), new=TRUE)
 boxplot(mtcars$wt, horizontal=TRUE, axes=FALSE)
 par(fig=c(0.65,1,0,0.8),new=TRUE)
 boxplot(mtcars$mpg, axes=FALSE)
 mtext("Enhanced Scatterplot", side=3, outer=TRUE, line=-3)
 To understand this graph, think of the full graph area as going from
(0,0) in the lower left corner to (1,1) in the upper right corner. The
format of the fig= parameter is a numerical vector of the form c(x1, x2,
y1, y2). The first fig= sets up the scatterplot going from 0 to 0.8 on
the x axis and 0 to 0.8 on the y axis. The top boxplot goes from 0 to
0.8 on the x axis and 0.55 to 1 on the y axis. I chose 0.55 rather than
0.8 so that the top figure will be pulled closer to the scatter plot.
The right hand boxplot goes from 0.65 to 1 on the x axis and 0 to 0.8 on
the y axis. Again, I chose a value to pull the right hand boxplot closer
to the scatterplot. You have to experiment to get it just right.
 fig= starts a new plot, so to add to an existing plot use new=TRUE.
 You can use this to combine several plots in any arrangement into one
graph.

**** fonts

fonts
 You can easily set font size and style, but font family is a bit more
complicated.
 option description
 font Integer specifying font to use for text.
 1=plain, 2=bold, 3=italic, 4=bold italic, 5=symbol
 font.axis font for axis annotation
 font.lab font for x and y labels
 font.main font for titles
 font.sub font for subtitles
 ps font point size (roughly 1/72 inch)
 text size=ps*cex
 family font family for drawing text. Standard values are "serif",
"sans", "mono", "symbol". Mapping is device dependent.
 In windows, mono is mapped to "TT Courier New", serif is mapped to"TT
Times New Roman", sans is mapped to "TT Arial", mono is mapped to "TT
Courier New", and symbol is mapped to "TT Symbol" (TT=True Type). You
can add your own mappings.
 # Type family examples - creating new mappings
 plot(1:10,1:10,type="n")
 windowsFonts(
 A=windowsFont("Arial Black"),
 B=windowsFont("Bookman Old Style"),
 C=windowsFont("Comic Sans MS"),
 D=windowsFont("Symbol")
 )
 text(3,3,"Hello World Default")
 text(4,4,family="A","Hello World from Arial Black")
 text(5,5,family="B","Hello World from Bookman Old Style")
 text(6,6,family="C","Hello World from Comic Sans MS")
 text(7,7,family="D", "Hello World from Symbol")
 [[R_files/img/Image_WQphYM1MxJwk8pfFrcfwTA_0003.jpg]]

**** general

Graphical Parameters
 You can customize many features of your graphs (fonts, colors, axes,
titles) through graphic options.
 One way is to specify these options in through the par( ) function. If
you set parameter values here, the changes will be in effect for the
rest of the session or until you change them again. The format is
par(optionname=value, optionname=value, ...)
 # Set a graphical parameter using par()
 par() # view current settings
 opar <- par() # make a copy of current settings
 par(col.lab="red") # red x and y labels
 hist(mtcars$mpg) # create a plot with these new settings
 par(opar) # restore original settings
 A second way to specify graphical parameters is by providing the
optionname=value pairs directly to a high level plotting function. In
this case, the options are only in effect for that specific graph.
 # Set a graphical parameter within the plotting function
 hist(mtcars$mpg, col.lab="red")
 See the help for a specific high level plotting function (e.g. plot,
hist, boxplot) to determine which graphical parameters can be set this
way.
 The remainder of this section describes some of the more important
graphical parameters that you can set.

**** Lines

Lines
 You can change lines using the following options. This is particularly
useful for reference lines, axes, and fit lines.
 option description
 lty line type. see the chart below.
 lwd line width relative to the default (default=1). 2 is twice as
wide.
 [[R_files/img/Image_lbYjt7sx4wblU4GUgIyy9g_0003.jpg]]

**** Margins and Graph Size

You can control the margin size using the following parameters.
 option description
 mar numerical vector indicating margin size c(bottom, left, top,
right) in lines. default = c(5, 4, 4, 2) + 0.1
 mai numerical vector indicating margin size c(bottom, left, top,
right) in inches
 pin plot dimensions (width, height) in inches
 For complete information on margins, see Earl F. Glynn's margin
tutorial.

**** Plotting Symbols

Plotting Symbols
 Use the pch= option to specify symbols to use when plotting points. For
symbols 21 through 25, specify border color (col=) and fill color (bg=).
 plotting symbols

 [[R_files/img/Image_efZC4mV78ZEbPclB6WKEXQ_0002.jpg]]

**** Text and Symbol Size

Text and Symbol Size
 The following options can be used to control text and symbol size in
graphs.
 option description
 cex number indicating the amount by which plotting text and symbols
should be scaled relative to the default. 1=default, 1.5 is 50% larger,
0.5 is 50% smaller, etc.
 cex.axis magnification of axis annotation relative to cex
 cex.lab magnification of x and y labels relative to cex
 cex.main magnification of titles relative to cex
 cex.sub magnification of subtitles relative to cex

*** Histograms

Histograms
 You can create histograms with the function hist(x) where x is a
numeric vector of values to be plotted. The option freq=FALSE plots
probability densities instead of frequencies. The option breaks=
controls the number of bins.
 # Simple Histogram
 hist(mtcars$mpg)
 [[R_files/img/Image_knZFfPE6YV7Iui4hSKLHow_0004.jpg]]
 # Colored Histogram with Different Number of Bins
 hist(mtcars$mpg, breaks=12, col="red")
 [[R_files/img/Image_knZFfPE6YV7Iui4hSKLHow_0005.jpg]]
 # Add a Normal Curve
 x <- mtcars$mpg
 h<-hist(x, breaks=10, col="red", xlab="Miles Per Gallon",
 main="Histogram with Normal Curve")
 xfit<-seq(min(x),max(x),length=40)
 yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
 yfit <- yfit*diff(h$mids[1:2])*length(x)
 lines(xfit, yfit, col="blue", lwd=2)
 [[R_files/img/Image_knZFfPE6YV7Iui4hSKLHow_0006.jpg]]
 Histograms can be a poor method for determining the shape of a
distribution because it is so strongly affected by the number of bins
used.

*** Kernel Density Plots

Kernal density plots are usually a much more effective way to view the
distribution of a variable. Create the plot using plot(density(x)) where
x is a numeric vector.
 # Kernel Density Plot
 d <- density(mtcars$mpg) # returns the density data
 plot(d) # plots the results
 [[R_files/img/Image_gECg8kOucndaZkhGA5ocFA_0004.jpg]]
 # Filled Density Plot
 d <- density(mtcars$mpg)
 plot(d, main="Kernel Density of Miles Per Gallon")
 polygon(d, col="red", border="blue")
 [[R_files/img/Image_gECg8kOucndaZkhGA5ocFA_0005.jpg]]
 *Comparing Groups VIA Kernal Density*
 The sm.density.compare( ) function in the sm package allows you to
superimpose the kernal density plots of two or more groups. The format
is sm.density.compare(x, factor) where x is a numeric vector and factor
is the grouping variable.
 # Compare MPG distributions for cars with
 # 4,6, or 8 cylinders
 library(sm)
 attach(mtcars)
 # create value labels
 cyl.f <- factor(cyl, levels= c(4,6,8),
 labels = c("4 cylinder", "6 cylinder", "8 cylinder"))
 # plot densities
 sm.density.compare(mpg, cyl, xlab="Miles Per Gallon")
 title(main="MPG Distribution by Car Cylinders")
 # add legend via mouse click
 colfill<-c(2:(2+length(levels(cyl.f))))
 legend(locator(1), levels(cyl.f), fill=colfill)
 [[R_files/img/Image_gECg8kOucndaZkhGA5ocFA_0006.jpg]]

*** Pie Charts

Pie Charts
 Pie charts are not recommended in the R documentation, and their
features are somewhat limited. The authors recommend bar or dot plots
over pie charts because people are able to judge length more accurately
than volume. Pie charts are created with the function pie(x, labels=)
where x is a non-negative numeric vector indicating the area of each
slice and labels= notes a character vector of names for the slices.
 * 
 Simple Pie Chart*
 # Simple Pie Chart
 slices <- c(10, 12,4, 16, 8)
 lbls <- c("US", "UK", "Australia", "Germany", "France")
 pie(slices, labels = lbls, main="Pie Chart of Countries")
 *Pie Chart with Annotated Percentages*
 # Pie Chart with Percentages
 slices <- c(10, 12, 4, 16, 8)
 lbls <- c("US", "UK", "Australia", "Germany", "France")
 pct <- round(slices/sum(slices)*100)
 lbls <- paste(lbls, pct) # add percents to labels
 lbls <- paste(lbls,"%",sep="") # ad % to labels
 pie(slices,labels = lbls, col=rainbow(length(lbls)),
 main="Pie Chart of Countries")
 *3D Pie Chart*
 The pie3D( ) function in the plotrix package provides 3D exploded pie
charts.
 # 3D Exploded Pie Chart
 library(plotrix)
 slices <- c(10, 12, 4, 16, 8)
 lbls <- c("US", "UK", "Australia", "Germany", "France")
 pie3D(slices,labels=lbls,explode=0.1,
 main="Pie Chart of Countries")
 *Creating Annotated Pies from a Dataframe*
 # Pie Chart from Dataframe with Appended Sample Sizes
 mytable <- table(iris$Species)
 lbls <- paste(names(mytable), "\n", mytable, sep="")
 pie(mytable, labels = lbls,
 main="Pie Chart of Species\n (with sample sizes)")

*** Saving Graphs

You can save the graph in a variety of formats from the menu
 File -> Save As.
 You can also save the graph via code using one of the following
functions.
 *Function  Output to
*
 pdf("mygraph.pdf") pdf file
 win.metafile("mygraph.wmf") windows metafile
 png("mygraph.png") png file
 jpeg("mygraph.jpg") jpeg file
 bmp("mygraph.bmp") bmp file
 postscript("mygraph.ps") postscript file

*** scatter and smooth line

3.To have a graphical look at the data, the univariate smoothing can be
plot on the scatterplot with the scatter.smooth function.
 Changing the smoothing parameter will change the smoothness of the fit.
 scatter.smooth(VAR1~VAR2)
 IE:
 scatter.smooth(bp1sys~age,col=2)
 scatter.smooth(bp1sys~age,span=0.1,col=2)

*** Simple Scatterplot

*Simple Scatterplot*
 There are many ways to create a scatterplot in R. The basic function is
plot(x, y), where x and y are numeric vectors denoting the (x,y) points
to plot.
 # Simple Scatterplot
 attach(mtcars)
 plot(wt, mpg, main="Scatterplot Example",
 xlab="Car Weight", ylab="Miles Per Gallon", pch=19)
 [[R_files/img/Image_groFKu0LqQUsnRswtl3Egg_0004.jpg]]
 # Add fit lines
 abline(lm(mpg~wt), col="red") # regression line (y~x)
 lines(lowess(wt,mpg), col="blue") # lowess line (x,y)
 [[R_files/img/Image_groFKu0LqQUsnRswtl3Egg_0005.jpg]]
 *car package*
 The scatterplot( ) function in the car package offers many enhanced
features, including fit lines, marginal box plots, conditioning on a
factor, and interactive point identification. Each of these features is
optional.
 # Enhanced Scatterplot of MPG vs. Weight
 # by Number of Car Cylinders
 library(car)
 scatterplot(mpg ~ wt | cyl, data=mtcars,
 xlab="Weight of Car", ylab="Miles Per Gallon",
 main="Enhanced Scatter Plot",
 labels=row.names(mtcars))
 [[R_files/img/Image_groFKu0LqQUsnRswtl3Egg_0006.jpg]]

*** Simple scatter plot

To create a simple scatter plot between the predicted and the observed
values issue:
 plot(data\_10$mfvalue,data\_10$predicted)
 where
 plot is the command
 data\_10 is the data name
 $mfvalue is the variable name

** Output
*** output to Html

The R2HTML package lets you output text, tables, and graphs in HTML
format. Here is a sample session, followed by an explanation.
 # Sample Session
 library(R2HTML)
 HTMLStart(outdir="c:/mydir", file="myreport",
 extension="html", echo=FALSE, HTMLframe=TRUE)
 HTML.title("My Report", HR=1)
 HTML.title("Description of my data", HR=3)
 summary(mydata)
 HTMLhr()
 HTML.title("X Y Scatter Plot", HR=2)
 plot(mydata$y~mydata$x)
 HTMLplot()
 HTMLStop()
 Once you invoke HTMLStart( ), the prompt will change to HTML> until you
end with HTMLStop().
 The echo=TRUE option copies commands to the same file as the output.
 HTMLframe=TRUE creates framed output, with commands in the left frame,
linked to output in the right frame. By default, a CSS file named
R2HTML.css controlling page look and feel is output to the same
directory. Optionally, you can include a CSSFile= option to use your own
formatting file.
 Use HTML.title() to annotate the output. The HR option refers to HTML
title types (H1, H2, H3, etc.) . The default is HR=2.
 HTMLhr() creates a horizontal rule.
 Since several interactive commands may be necessary to create a
finished graph, invoke the HTMLplot() function when each graph is ready
to output.
 The RNews article The R2HTML Package has more complex examples using
titles, annotations, header and footer files, and cascading style
sheets.

*** output to pdf

use this code:
 pdf("bla.pdf")
 plot commands...
 dev.off()
 IE:

pdf("c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.1.Projetcs/3.1.3.TEMP\_MODELS/3.1.1.4.Work/4.results/histograms/bla.pdf")
 par(mfrow=c(1,1))
 hist(F\_T2003\_All$TMIN)
 hist(F\_T2003\_All$st\_faren)
 hist(F\_T2003\_All$ndvi)
 dev.off()
 *NOTE: you can open these pdf's with Inkscape*

*** output to svg

for the SVG export, either use rkward (works internally with GUI) or
when using 'R':
 download the 'RSvgDevice' package and then use the first two lines of
code and the last line of code here:
 library("RSvgDevice")
 devSVG(file = "P:/Dissertation/Aim1/JULY2010/gstmplot.svg")
 plot(XXXXXXXXXXXXX)
 dev.off()

** R function to use to construct CI

R function to use to construct CI:
 For those who are wondering, the R function to use to construct CI is
"segments". I have written a simple example that works :
 x0<-2
 y0<-3
 plot(x0,y0, ylim=c(0,10))
 segments(x0, y0, y1=c(2,4))

** construct time sereis 

#plot 2 y scales

x=pm25.m1[c == 2004 & A_T==1 & stn == "ANT"]$day
y1=pm25.m1[c == 2004 & A_T==1 & stn == "ANT"]$aod;summary(y1)
y2=pm25.m1[c == 2004 & A_T==1 & stn == "ANT" ]$PM25;summary(y2)
length(x)

## add extra space to right margin of plot within frame
par(mar=c(5, 4, 4, 6) + 0.1)
## Plot first set of data and draw its axis
plot(x, y1, pch=19, cex=1.3,axes=FALSE, ylim=c(0,1.1), xlab="", ylab="", 
     type="p",col="black")
axis(2, ylim=c(0,1.1),col="black",las=1)  ## las=1 makes horizontal labels
mtext("aod",side=2,line=2.5)
box()

## Allow a second plot on the same graph
par(new=TRUE)
## Plot the second plot and put axis scale on right
plot(x, y2, pch=1, cex=1, xlab="", ylab="", ylim=c(0,530), 
     axes=FALSE, type="p", col="red")
## a little farther out (line=4) to make room for labels
mtext("PM",side=4,col="red",line=2) 
axis(4, ylim=c(0,530), col="red",col.axis="red",las=1)

## Draw the x axis
 #plot PM time series
  plot(data$day,data$PM10,type="p",axes=FALSE,xlab="",ylab="PM10 [microgram'm^3']")
axis(2, ,col="black",las=1)  ## las=1 makes horizontal labels
axis.Date(1, at = seq(x[1], x[9439], length.out=12),
          labels = seq(x[1], x[9439], length.out=12),
          format= "%m/%d/%Y", las = 2)
 
#x axis label
axis(1,xlim=c(1:12),col="black")
mtext("Month",side=1,col="black",line=2.5) 


## Add Legend
legend("topleft",legend=c("PM2.5","DB550"),
       text.col=c("black","red"),pch=c(16,15),col=c("black","red"))






** ggmap
*** use google API key to generate a map
#+begin_src R
library(ggmap)

api_key <- 'AIzaSyDooP7J5EmH77toX7PbEgNu3uqTT6XTdg8'
map <- get_map(location = 'united states', zoom = 4, source = 'stamen')
ggmap(map, fullpage = TRUE)

#+end_src

* Tips/apps
** extract coefficants

to extract the fixed effect cooeficent:
 cofix <- Final\_pred\_2001$coef$fixed
 to extract the random effect cooeficent:
 coran <- Final\_pred\_2001$coef$random
** Coding tips
*** turn on and off section quickly
you can achieve that with a true/false statement at start and then put
the relevant sections in special 'if brackets':

#+begin_src r
if(showplots){
}
#+end_src

in example:

#+begin_src r
showplots <- T #use True (T) or False (F) at  start of script to define weather these sections will run


if(showplots){
  ggplot(pm[,list(alldaymean = mean(daymean,na.rm = T), precipdaymm = head(precipdaymm,1)), by=day], aes(precipdaymm, alldaymean)) +
    geom_point(alpha = 0.25) + geom_smooth() + theme_bw(12)
}

#+end_src

** use a formula in models

#+begin_src R
#model formula
m1.formula <- as.formula(PM25 ~ aod+tempc+WDSP+NDVI+dist_PE+pcturb_1km+logroad+SumOfEMISS+pop_sqkm+elev_m+ah_gm3+visib+aod*pbl+pbl)
#base model
out.m1_2003 = lme(m1.formula , random = ~1 + aod+tempc|date/region,data =  m1_2003)
#+end_src

** Trimming objects using negative indices

Consider the following vector:
 x <- c(4,1,7,2,3,7,3,4,5,154,1,8,3,4)
 Now we calculate the mean:
 mean(x)
 [1] 14.71429
 It seems as the value 154 is somewhat special. It is much bigger than
the other values so we want to omit it from further calculations. That
is done with negative subscripts. First, we use the function which to
find out which index the value 154 has:
 which(x==154)
 [1] 10
 So it's the tenth value in x. Now we calculate the mean again but use
negative indices to omit the value from the calculations:
 mean(x[-10])
 [1] 4
 The mean is much lower when the value 154 is omitted. You can use the
concatenation function c to omit multiple values but you can't mix
positive and negative indices!

** rkward

*** rkward under windoez

to have a clean start remember to delete the .kde folder under
application data in the USER folder (its a hidden folder)

** rstudio

*** install on linux

make sure fortran is installed
 in arch:
 gcc-fortran

** get rid of glmPQL long output

use
 summary(OBJECT)$tTable
 to get just the t table and not all the iterations

** R_VIM
*** install on windows

In addition to having R installed in your system, this plugin requires
users to install external dependencies:
 1) Vim's version must be >= 7.3
 2) Python:
[[http://www.python.org/download/][http://www.python.org/download/]]
 Note: The official Vim is 32 bit and, thus, *Python must be 32
bit too*.
 However, Vim and R run as independent processes, that is, you may
have
 32 bit Vim sending commands to 64 bit R.
 *Note: Be careful to download the correct Python version because
Vim
 needs a specific version of Python DLL.* For example,*the
official
 Vim 7.3 for Windows needs either Python 2.7 or 3.1*. If Python
was
 not installed or was not found, the Vim-R-plugin will output
 information about what Python version Vim was compiled against.
Do
 the following if you want to discover this information manually:
 1. Type :version in Vim (normal mode).
 2. Look for a string like -DDYNAMIC\_PYTHON\_DLL="python27.dll".
 3. Install the Python version which corresponds to the version
 which Vim was linked against. In the example of step 2
 (python27.dll) the required Python version is 2.7.x.
 3) pywin32:
[[http://sourceforge.net/projects/pywin32/][http://sourceforge.net/projects/pywin32/]]
 Note: The default download may not match the Python version Vim
was
 linked against. You have to "View all files" on the download page
to
 find the file that matches exactly the version of Python that you
 installed.
 *INSTALL THE 32 BIT PYTHON VERSION OF WHAT VIM IS COMPILED AGAINST*
 4) vimcom >= 0.9-2: An R package. You can install it with the R
command
 install.packages("vimcom"). The vimcom package creates a server
on R to
 allow the communication with Vim.
 5) put this in your R profile (use both the *Rprofile.site* file
first in the *C:\Program Files\R\R-n.n.n\etc\* directory and the
.Rporfile in ~)
 if(interactive()){
 library(vimcom)
 }
 6) Create your .vimfiles directory if you do not have it yet. Its path
will be similar to one of the following:
 *C:\Documents and Settings\yourlogin\vimfiles*
 OR
 *C:\Users\yourlogin\vimfiles*
 7) Uncompress the archive. Right click on the plugin's zip file and
choose
 "Extract all". Then choose ~/vimfiles as the destination directory.
 8)Start Vim and build the tags file for this document (and others that
eventually
 are in the same directory):
 *:helptags ~\vimfiles\doc*
 9)Start editing an R file with GVim and try to send some lines to R
Console. You
 may have to adjust the value of |vimrplugin\_sleeptime|.
** show all variables and position in data frame
*** using built in menu function
use the menu command
#+begin_src R
menu(names(mod2))
#with a gui add grpahics=true
menu(names(mod2),graphics=TRUE)
#+end_src

*** using a snippet
#+begin_src R
l=seq(names(tst));names(l)=names(tst);l
#+end_src
** import only if dosent exist code
#+begin_src R
if(!exists("DATA")){
    COMMAND
  }
#+end_src

₆In example₆ 

#+begin_src R
if(!exists("m2_agg")){
  m2_agg<-readRDS("/home/zeltak/smb4k/ZUNISYN/ZUraid/Uni/Projects/P031_MIAC_PM/3.Work/2.Gather_data/FN008_model_prep/m2_agg_2006.rds")
  }
#+end_src
** time the running of a piece of code 
use this

#+begin_src R

# Start the clock!
ptm <- proc.time()
# start code
CODE GOES HERE
# Stop the clock
proc.time() - ptm
#+end_src
$Note-the time is the last column (elapsed) and is in seconds$
** test if dataframes are the same
#+begin_src R
all.equal(DF1, DF2)
[1] TRUE
#+end_src
** list all variables from a formula

#+BEGIN_SRC R
#base model for stage 1
m1.formula <- as.formula(PM25 ~ aod+tempc+WDSP+NDVI+dist_PE+pcturb_1km+logroad+nei05nonpntcntypm25+pop_sqkm+elev_m+ah_gm3+visib+aod*pbl+pbl+NOXsum+PM10sum+SO2sum+pctmd_1km + pctld_1km+pctop_1km+  pctdf_1km+pctmf_1km+pctev_1km+  pctcr_1km+pctpa_1km+pctsh_1km+  pctgr_1km+  pm25stge30_15k  +  pm25stlt30_3k+pm10stge30_15k   + pm10stlt30_3k   +noxstge30_15k+noxstlt30_3k+ so2stge30_15k+so2stlt30_3k+ (1 +aod+tempc|day/region))
all.vars(testformula)
#+END_SRC

** create sine and cosine
#+BEGIN_SRC R
bwfull.s[, costime := cos(2*pi*jday/365.25)]
bwfull.s[, sintime := sin(2*pi*jday/365.25)]
#+END_SRC

** HDF4 under R
*** dependencies
make sure your system has the hdf4 and gdal dependencies installed:
under debian:

#+BEGIN_SRC sh
sudo apt-get install libhdf4-0 gdal
#+END_SRC

*** look at the hdf info

$Note- you will need to cd into the directory that holds the hdf files$ 

you can use the gdalinfo command to get info on HDF files

#+BEGIN_SRC sh
gdalinfo MAIACAOT.h00v00.20013050915.hdf 
#+END_SRC

#+BEGIN_EXAMPLE
Driver: HDF4/Hierarchical Data Format Release 4
Files: MAIACAOT.h00v00.20013050915.hdf
Size is 512, 512
Coordinate System is `'
Metadata:
  HDFEOSVersion=HDFEOS_V2.12
Subdatasets:
  SUBDATASET_1_NAME=HDF4_EOS:EOS_GRID:"MAIACAOT.h00v00.20013050915.hdf":grid1km:Optical_Depth_Land
  SUBDATASET_1_DESC=[400x400] Optical_Depth_Land grid1km (16-bit integer)
  SUBDATASET_2_NAME=HDF4_EOS:EOS_GRID:"MAIACAOT.h00v00.20013050915.hdf":grid1km:Angstrom_Para
  SUBDATASET_2_DESC=[400x400] Angstrom_Para grid1km (16-bit integer)
  SUBDATASET_3_NAME=HDF4_EOS:EOS_GRID:"MAIACAOT.h00v00.20013050915.hdf":grid1km:Column_WV
  SUBDATASET_3_DESC=[400x400] Column_WV grid1km (16-bit integer)
  SUBDATASET_4_NAME=HDF4_EOS:EOS_GRID:"MAIACAOT.h00v00.20013050915.hdf":grid1km:AOT_QA
  SUBDATASET_4_DESC=[400x400] AOT_QA grid1km (8-bit unsigned integer)
Corner Coordinates:
Upper Left  (    0.0,    0.0)
Lower Left  (    0.0,  512.0)
Upper Right (  512.0,    0.0)
Lower Right (  512.0,  512.0)
Center      (  256.0,  256.0)
#+END_EXAMPLE

also you can look at specific hdf data subsets

#+NAME: 
#+BEGIN_SRC sh
gdalinfo HDF4_EOS:EOS_GRID:"MAIACAOT.h00v00.20013050915.hdf":grid1km:Optical_Depth_Land 
#+END_SRC

#+BEGIN_EXAMPLE
Driver: HDF4Image/HDF4 Dataset
Files: MAIACAOT.h00v00.20013050915.hdf
Size is 400, 400
Coordinate System is:
PROJCS["unnamed",
    GEOGCS["Unknown datum based upon the custom spheroid",
        DATUM["Not specified (based on custom spheroid)",
            SPHEROID["Custom spheroid",6371007.181,0]],
        PRIMEM["Greenwich",0],
        UNIT["degree",0.0174532925199433]],
    PROJECTION["Sinusoidal"],
    PARAMETER["longitude_of_center",0],
    PARAMETER["false_easting",0],
    PARAMETER["false_northing",0],
    UNIT["Meter",1]]
Origin = (3200000.000000000000000,3700000.000000000000000)
Pixel Size = (1000.000000000000000,-1000.000000000000000)
Metadata:
  add_offset=0
  HDFEOSVersion=HDFEOS_V2.12
  long_name=AOT at 0.47 micron for land
  scale_factor=0.001
  unit=None
  valid_range=-100, 5000
  _FillValue=-28672
Corner Coordinates:
Upper Left  ( 3200000.000, 3700000.000) ( 34d25'18.38"E, 33d16'29.50"N)
Lower Left  ( 3200000.000, 3300000.000) ( 33d 7'23.45"E, 29d40'39.29"N)
Upper Right ( 3600000.000, 3700000.000) ( 38d43'28.18"E, 33d16'29.50"N)
Lower Right ( 3600000.000, 3300000.000) ( 37d15'48.88"E, 29d40'39.29"N)
Center      ( 3400000.000, 3500000.000) ( 35d51' 8.35"E, 31d28'34.39"N)
Band 1 Block=400x400 Type=Int16, ColorInterp=Gray
  Description = AOT at 0.47 micron for land
  NoData Value=-28672
  Offset: 0,   Scale:0.001
#+END_EXAMPLE

*** convert hdf to Rdata

#+BEGIN_SRC sh
gdal_translate -of R HDF4_EOS:EOS_GRID:"MAIACAOT.h00v00.20013050915.hdf":grid1km:Optical_Depth_Land testout.RData            
#+END_SRC

** estimate separate slope for each day
#+BEGIN_SRC R
for(i in 1:length(moddays)){
subdat <- mod1[mod1$date == moddays[i], ]
day.mod1 <- try(lm(model.formula, data =  subdat, na.action = na.omit) )
moddayslm$ith[i] <- i
moddayslm$intercept[i]<-coef(day.mod1)[1]  #Is the intercept (a+uj) or uj ? fixed+random? Or only random?
moddayslm$beta[i] <- coef(day.mod1)[2]   #Is the slope (β1+Vj) or Vj ? fixed+random? Or only random?
moddayslm$nrows[i] <- nrow(subdat)
}
#+END_SRC

** join aod to pm by day 
#+BEGIN_SRC R
aodpm5k<-fread("/media/NAS/Uni/Projects/P046_Israel_MAIAC/3.Work/2.Gather_data/FN007_Key_tables/aod_pm_5k.csv")
aqua.se <- aqua[aqua$aodid %in% aodpm5k$aodid, ] 
aqua.se<-aqua.se[, c(8:24) := NULL]


setkey(aodpm5k,aodid)
setkey(aqua.se,aodid)
aqua.se <- merge(aqua.se,aodpm5k[,list(dist,aodid,stn)], all.x = T)

setkey(PM25,day,stn)
setkey(aqua.se,day,stn)
m1.s1 <- merge(PM25,aqua.se,all.x = T)

#to leave only THE 1 closest sat data point to station in each day
setkey(m1.s1,stn,day,dist)
#take first ocurance by day per STN (its sorted by dist so the shortest one)
x<-m1.s1[unique(m1.s1[, list(stn, day)]), mult = "first"]

#+END_SRC

** LOOCV

Here is some code for leave one (or many >1) CV method 
`xout` below defines how many stations to leave out

#+BEGIN_SRC R
# Cross-validation
#get the unique names of stations
stns <- unique(m1.2007$stn)
# cross-validation and model building
# repeated leave x stnitors out CV
#number of iterations you want the model to run
n.iter <- 20
xout <- 5 # number of stations to hold out
# how many combinations if we pull out xout stns
ncol(combn(stns, 2))
# list to store scheme
cvscheme <- list()
cvout <- list()
system.time(for(i in 1:n.iter){
  stns.test <- stns[sample(length(stns), xout)]
  cvscheme[[i]] <- stns.test
  test <- m1.2007[stn %in% stns.test, ]
  train<- m1.2007[!stn %in% stns.test, ]
  print(paste("iteration #", i, "testing set is stnitor", paste(unique(test$stn), collapse = ","), ",", nrow(test), "records from", paste(format(range(test$day), "%Y-%m-%d"), collapse = " to ")))
  print(paste("training on", nrow(train), "records"))
  trainmod <-  lmer(m1.formula, data =  train)
  test$predcv <- predict(object=trainmod,newdata=test,allow.new.levels=TRUE,re.form=NULL )
  test$itercv <- i  
  # export these results
  cvout[[i]] <- test[, list(day, stn, PM25, predcv, itercv)]
}# end of cross-validation loop
)
alltest <- rbindlist(cvout)
head(alltest, 2)
summary(lm(PM25 ~ predcv, data = alltest))
# compute root mean squared error
alltest[, sqrt(mean((PM25 - predcv)^2))]
#+END_SRC

%Tip- you can also plot these results with ggplot%    

#+BEGIN_SRC R
ggplot(alltest, aes(predcv, PM25)) + 
  geom_abline(linetype = "dashed") + 
  geom_point(aes(color = factor(itercv))) + geom_smooth() + 
  facet_wrap(~stn) + coord_equal() + 
  theme_bw(12)
#+END_SRC



** check max value of a variable
#+BEGIN_SRC R
alltest[which.max(PM25)]
#+END_SRC
   
* Packages specific
** Hemisc
*** mtable
[[R_files/attach/memisc.pdf][Attachment #06 (memisc.pdf)]]

mtable produces a table of estimates for several models:
 Usage:
 mtable(...,coef.style=getOption("coef.style"), #
 summary.stats=TRUE,
 factor.style=getOption("factor.style"),
 getSummary=eval.parent(quote(getSummary)),
 float.style=getOption("float.style"),
 digits=min(3,getOption("digits")),
 drop=TRUE
 important options:
 *See attached PDF for full options*
 digits - number of significant digits if not specified by the template
returned from getCoefTemplate
 getSummaryTemplate:
 model1=mtable(Rresp\_m25x ,
summary.stats=FALSE,digits=min(3,getOption("digits")))
** plyr
*** sort data
use arrange

#+begin_src R
df <- arrange(df, date, mh)
#+end_src

*** run multiple model with by 
#+BEGIN_SRC R
modelList <- dlply(pm25.m1.c[A_T==1], "stn", function(x) lm(m1.formula, data=x))
lapply(modelList, function(x) summary(x)$r.squared)
#+END_SRC

*** Alternative way>> run models on subset with plyr
#+BEGIN_SRC R
# easy way to run on subsets
# first we define the function we want on each subset
runandsummarizemod1 <- function(df){
  out.m1.plyr = lmer(m1.formula, data = df)
  df$predicted <- predict(out.m1.plyr)
  data.frame(nobs = nrow(df), rsq = round(summary(lm(daymean~predicted, data = df))$r.squared, 3))  
}
# then we call ddply
mod1subsets <- ddply(mod1, .(yr), runandsummarizemod1)
mod1subsets
ggplot(mod1subsets, aes(yr, rsq)) + geom_point() + geom_line()

#+END_SRC
    

** closestbyday  (allan KNN function)
*** k nearest
knearest=100 is roughly 5.7 km's
100/pi= 32
sqrt(32)=5.7

** broom

https://github.com/dgrtwo/broom
*** lme4
#+BEGIN_SRC R
lmm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
tidy(lmm1)
tidy(lmm1, effects = "fixed")
head(augment(lmm1, sleepstudy))
glance(lmm1)
#+END_SRC
    
** compare
compare two dataframes
http://cran.r-project.org/web/packages/compare/vignettes/compare-intro.pdf

example
#+BEGIN_SRC R
comparison <- compare(aodf.2003.tmp.s9.m2,aodf.2004.tmp.s9.m2,allowAll=TRUE)
comparison
#+END_SRC

$Note$ this may also be done to some extent with native R code
#+BEGIN_SRC R
all.equal(lapply(aodf.2003.tmp.s9.m2,class),lapply(aodf.2004.tmp.s9.m2,class))
#+END_SRC

** nearestbyday

to calculate the mean use nearestmean= TRUE. if set to False it will only get knreaset and get closest one avilable by distance

knreast: number of nearest points it considers,if or if not they have data. assesment of how near something is , is done by pointmatrix. if you have alot of points with no data you might end up with no data. its the maximum number of points that gets considered.
the higher the kneareast the bigger the Dataset needs to be- has RAM/CPU implications
will only consider max number based in max number of monitors/sites in the joining dataset

maxdistance (optional)- defualts to NA-you can add second restriction that the points that are considred have to be within maxdist (will use units of cords being used- meters, dec degree)
if you set knearest if 5 and max dist of 20000 (20km) it will look for 5 closest points up to 20km.

please note knearest runs before maxdist, the maxdist is an additional screen

#+BEGIN_SRC R
nearestbyday <- function(matrix_target, matrix_join, dt_target, dt_join, dt_target_varname, dt2_join_varname, 
                         closestname = "closestmet", varstoget = "avewsp", 
                         knearest = 5, maxdistance = NA, nearestmean = FALSE){
#+END_SRC

dt-target_varname-the string of the variable name for the units in the target dt
dt2_join_varname-the string of the variable name for the units in the join dt
closestname = "closestmet"-the string to be given to derived variable- a string prefix
varstoget = "avewsp"-isnt used ..here be dragons?- ignore it for now and just put target variable

2nd column (named based on the named in join dataset)-data point closest from unit
closest-which unit from the join was the closest one (which was used)
closestknn-which of the closest knn (k nearest) was used
closestnobs-total number of observations that met the knearst and maxdist criteria--WITH DATA!! if no data was avilable it would ne NA
closestmean- mean of all the points if nearstmean=True, if False then you dont get it.

** zoo 
*** fill in/impute missing days from day before 
sort by day and use the following syntax

#+BEGIN_EXAMPLE
#DT[,FULLVAR := na.loc(VAR,na.rm=F)]
#+END_EXAMPLE

#+BEGIN_SRC R
# impute missing Temperature
# to start with - use last observation carried forward
library(zoo)
#set the key by day 
setkey(Temp, day)
Temp[, Temp.im := na.locf(Temp, na.rm = F)]
describe(Temp[, list(Temp, Temp.im)])
#+END_SRC

* data.table
*** basics
**** into
We can easily convert existing data.frame objects to data.table.
CARS = data.table(cars)
head(CARS)
It is often useful to see a list of all data.tables in memory:
tables()

NAME NROW MB COLS KEY
[1,] CARS 50 1 speed,dist
[2,] DT 5 1 x,v
Total: 2MB
The MB column is useful to quickly assess memory use and to spot if any
redundant tables can
be removed to free up memory. Just like data.frames, data.tables must t inside RAM.

To see the*column types* :
sapply(DT,class)
x v
"character" "numeric"
**** data.table examples
example(data.table) 
**** show all data.table tables in ws info
#+begin_src R
tables()
#+end_src
**** summarize a single data.table
#+begin_src R
 tables(silent = T)[NAME == "dat"]
#+end_src
**** query with string
Q:what does the with=F mean in dt?
A:that you query with strings
*** fread import (import txt/csv into data.tbale format)
**** basic import
#+begin_src r
library(data.table)
DATA.TABLE.FILE <- fread("/PATH/TO/FILE")
IE:
dat <- fread("f:/Uni/Projects/P031_MIAC_MEXICO/3.Work/Archive/mod2.csv")
#+end_src
**** import only specific rows
add the nrow option
 #+begin_src r
library(data.table)
dat <- fread("f:/Uni/Projects/P031_MIAC_MEXICO/3.Work/Archive/mod2.csv", nrow=1000)
#+end_src

will import only the first 1000 rows

**** subset columns in import
fread's drop, select and NULL in colClasses are implemented. To drop or select columns by name or by number. See examples in ?fread.

The examples in ?fread are

#+begin_src R
data = "A,B,C,D\n1,3,5,7\n2,4,6,8\n"

# colClasses    
fread(data, colClasses=c(B="character",C="character",D="character"))
fread(data, colClasses=list(character=c("B","C","D")))    # saves typing
fread(data, colClasses=list(character=2:4))     # same using column numbers

# drop
fread(data, colClasses=c("B"="NULL","C"="NULL"))   # as read.csv
fread(data, colClasses=list(NULL=c("B","C")))      # same
fread(data, drop=c("B","C"))      # same but less typing, easier to read
fread(data, drop=2:3)             # same using column numbers

# select
# (in read.csv you need to work out which to drop)
fread(data, select=c("A","D"))    # less typing, easier to read
fread(data, select=c(1,4))        # same using column numbers
#+end_src

**** force column type when importing
#+BEGIN_SRC R
loc<-fread("/media/NAS/Uni/Projects/P011.BirthW_NE/3.1.11.4.Work/3.Analysis/2.R_analysis/bw_diab37.csv",colClasses=c(FIPS="character",tract="character"))
#+END_SRC
*** Merge/Join
**** Normal join
#+begin_src R
reg<-fread("y:/EAST_USA_MAIAC/data/FN007_Key_tables/region_guid.csv")
#add new region
setkey(m1_2003,guid)
setkey(reg,guid)
m1_2003 <- merge(m1_2003, reg, all.x = T)
table(m1_2003$region)
#+end_src
 
**** Binary join
#+begin_src r
# setting key before binary join
setkey(datkurt, DATE)
setkey(datm1, DATE)
# binary not join
datm1[!datkurt[kurthigh == T,]]
#+end_src
**** merge but keep only some variables

#+begin_src R
setkey(clippedaod, aodid)
setkey(clipgrid, aodid)
zclippedaod <- merge(clippedaod, clipgrid[,list(aodid,x_aod_ITM, y_aod_ITM)], all.x = T)
#+end_src
this will keep only 3 variables from the clipgrid dataset
**** Complex merge and renaming multiple variables
***** example 1

#+BEGIN_SRC R
xgestlong <- merge(gestlong, gestlong_cli[,list(birthyear_clin=birthdate, id)], by = "id")
#+END_SRC

***** example 2

#+begin_src R
#complex merge and renaming
gestpred <- merge(gestpred,
                  participants[etapa == "2T", list(folio,
                                                   SES_scored, #SES
                                                   hospital,
                                                   numlivingchildren = t21, # living children
                                                   primiparous = v68, # primiparous
                                                   parity = v69, # parity
                                                   previoussmallbaby = v70 > 0, # previous small babies
                                                   previousearlybaby = v71 > 0, # previous early babies
                                                   deliverytype = v121, # delivery type
                                                   pregcomplication = v124, # pregnancy complication
                                                   pregcomptext = v124e, # text of preg complication
                                                   anypregcomplication = !is.na(v124e)
                  )],
               all.x = T, by = "folio")

#+end_src
**** use the by instead of setkey
#+begin_src R
mod1 <- merge(mod1, filterpm25[, list(day,mon,filterpm25)], all.x= T, by = c("day", "mon"))
#+end_src
 
**** cartesean join						  :CANCELLED:
a cartesean join is a join of every row of one table to every row in another table
under data.table it looks like this:

#+BEGIN_SRC R
test4.se<- merge(test3.se,cases,allow.cartesian=TRUE)
#+END_SRC

**** leave only row based on sort (mult=)
#+BEGIN_SRC R
#to leave only THE 1 closest sat data point to station in each day
setkey(m1.s1,stn,day,dist)
#take first ocurance by day per STN (its sorted by dist so the shortest one)
x<-m1.s1[unique(m1.s1[, list(stn, day)]), mult = "first"]

#+END_SRC

**** join and leave specific columns based on position
#+BEGIN_SRC R
setkey(m2,aodid)
setkey(wlu,aodid)
x2<-merge(m2,wlu[,c(1,18:ncol(wlu)), with = F],all.x = T)
#+END_SRC
     
*** Dates
**** extract dates in data.table
#+begin_src r
dat[, day:=as.Date(strptime(DATE, "%m/%d/%y"))]
#+end_src
    
**** recode data months to bimon using integer division by 2
#+begin_src R
mpmg[bimon := (m + 1) %/% 2]
#+end_src

**** Create seasons

#+BEGIN_SRC R
#Seasons
library(car)
mod1d$month <- as.numeric(format(mod1d$date, "%m"))
#1-winter, 2-spring,3-summer,4-autum
mod1d$season<-recode(mod1d$month,"1=1;2=1;3=2;4=2;5=2;6=3;7=3;8=3;9=4;10=4;11=4;12=1")
#1-winter, 2-summer
mod1d$seasonSW<-as.character(recode(mod1d$month,"1=1;2=1;3=1;4=2;5=2;6=2;7=2;8=2;9=2;10=1;11=1;12=1"))
#+END_SRC
**** create year
#+BEGIN_SRC R
WD[, c := as.numeric(format(day, "%Y")) ]
#+END_SRC

**** drop specific dates from a dataset
#+BEGIN_SRC R
# drop new years and newyears eve
mod1 <- mod1[!dayofyr %in% c(1,365,366),]
# drop christmas
mod1 <- mod1[!day  %in% as.Date(paste0(2004:2014, "-12-25")), ]
#+END_SRC
     
     
*** delete columns data.table
Either of the following will remove column foo from the data.table df3:

#+BEGIN_SRC sh
# Method 1 (and preferred as it takes 0.00s even on a 20GB data.table)
df3[,foo:=NULL]

#EXAMPLE FOR MULTIPLE COLUMNS
# remove old regions
m1_2003[, c("region", "reg") := NULL]
xaqua.se<-aqua.se[, c(8:24) := NULL]


# Method 2
df3 <- df3[,-grep("foo", colnames(df3)), with=FALSE]
# Method 3 -- Safer than Method 2 -- see Joshua Ulrich's comments above and below
df3 <- df3[, which(!grepl("foo", colnames(df3))), with=FALSE]
# Method 3b: 
df3 <- df3[, !grepl("foo", colnames(df3)), with=FALSE]
# Method 4:
df3 <- df3[, !"foo", with=FALSE]  


#+END_SRC

Note: that if foo is not found, these last two methods will yield an empty data.table. The methods are better suited for interactive analysis, where one might want to display the data.table without the named column (without necessarily assigning it). For programming purposes, Method 1 is really the best option.

*** subsetting data
**** select columns to keep
  
#+begin_src R
m1<-m1[,c(6,44,5),with=FALSE]
#will keep in m1 rows 6,44,55
#by name
m1<-aodf.2003.tmp.s9.m2[,c( "x_aod_ITM", "y_aod_ITM", "aodid"),with=FALSE]
#+end_src

**** select values to keep
#+begin_src R
d1 <- mod1[!mon %in% c("SAG", "PER", "XAL"), ]
#+end_src
**** select columns to drop
#+BEGIN_SRC R
WS[,c("Year","Month","Day","date"):=NULL]
#or
m2<-m2[, c(10:12) := NULL]
#+END_SRC
 
**** select values to keep by matching in vector/dataset
#+begin_src R
mod3grid.se <- mod3grid [mod3grid$guid %in% m2_agg$guid, ] 
#to exclude by vector
xtst <- terra[!(terra$aodid %in% pag$aodid), ] 

#+end_src
**** subset and rename
#+BEGIN_SRC R
age4subset <- age4[, list(folio, age4date = fecha_vis, weight_m_mean, 
                          height_m_mean, momfatmass = M_body_m)]
#+END_SRC
**** subset data in a model
@you have to sort the subset variable by setley@

#+begin_src R
setkey(m1all,seasonSW)
out.m1_2003 = lmer(m1.formula ,data =  m1all[seasonSW==1,])

#or
m2.smooth = lme(predicted.m2 ~ mpm*as.factor(bimon),random = list(aodid= ~1 + mpm),
                control=lmeControl(opt = "optim"),data= mod2[.(2011),])
#+end_src
**** select missing values (NA's)
#+begin_src R
try2 <- midmod3grid[is.na(mpmid)]
#+end_src
**** leave X obsv out 
See Here:

[[*subset%20a%20dataset%20based%20on%20a%20sample%20of%20values][subset a dataset based on a sample of values]]
**** subset by date
#+begin_src R
add5 <- add4[daystart <= day & dayend > day, ]
#example 2
add5 <- met[Date == "2000-12-27"]
#example 3
tss6<-allbestpred[day >= "2006-01-01" & day < "2007-01-01"]
#+end_src
**** susbet by year
#+begin_src 
dat2013 <- clippedaod[yr == "2013"]
#+end_src
**** subset to remove NAs
#+begin_src R
  temp <- temp[x_s_ITM != 'NA']
  temp <- temp[x_s_ITM != 'NA']           #
#+end_src

**** subset by value
#+begin_src R
pm25 <- pm25[PM25>= 0]
#+end_src

**** subset rows
#+BEGIN_SRC R
f2<-f1[1:1000,]
#+END_SRC
y
**** subset by lat and long
#+BEGIN_SRC R
p2008 <- p2008[long_aod > -74 & long_aod < -69 & lat_aod < 44 & lat_aod > 41, ]
#+END_SRC
*** reorder variables in data.table
#+begin_src R
x <- data.table(a = 1:3, b = 3:1, c = runif(3))
setcolorder(x, c("c", "b", "a"))
#+end_src
*** rbind in data.table
#+begin_src R
require(data.table)
DT.1 <- data.table(x = letters[1:5], y = 6:10)
DT.2 <- data.table(x = LETTERS[1:5], y = 11:15)
# works fine
rbindlist(list(DT.1, DT.2))

#$Note$ - you can add DT's with different column numbers with fill=TRUE
m1.all <- rbindlist(list(m1.2003,m1.2004,m1.2005,m1.2006,m1.2007,m1.2008,m1.2009,m1.2010,m1.2011,m1.2012,m1.2013), fill=TRUE)

#+end_src
*** Aggregate
**** aggregate on 1 key variable and 1 aggregated variable
#+BEGIN_SRC R
# Declare which variable you want to group on (optional).
# List the name of the data table first, then the name of the field(s).
setkey(stulevel, grade)

# Average ability by grade
stulevel_agg_2 <- as.data.frame(stulevel[, mean(ability, na.rm = TRUE),by = grade])
#+END_SRC
**** aggregate on 2 key variable and 1 aggregated variable
#+BEGIN_SRC R
# Average ability by grade
stulevel_agg_3 <- as.data.frame(stulevel[, j=list(mean(ability, na.rm = TRUE),mean(attday, na.rm = TRUE)),by = grade])

#+END_SRC
**** aggregate on 2 key variable and 2 aggregated variable
#+BEGIN_SRC R
# Average ability by grade
stulevel_agg_4 <- as.data.frame(stulevel[, j=list(mean(ability, na.rm = TRUE),mean(attday, na.rm = TRUE)),by = list(year,grade)])

#+END_SRC
**** aggregate and keep associated variables (₆In Example₆ lat/long)
#+begin_src R
m3d_agg <- (mod3[, list(LTPM =mean(pm_mod3, na.rm = TRUE), 
                        x_aod_utm = x_aod_utm[1], #use the first long and lat (by aodid)
                        y_aod_utm = y_aod_utm[1]),by = aodid])	
#+end_src
 
*** Rename
**** simple rename
#+begin_src R
setnames(DT,"MyName.1","MyNewName")
#+end_src
**** rename multiple variables/columns at once
#+begin_src R
setnames(DT,"b","B")               # by name; no match() needed
setnames(DT,3,"C")                 # by position
setnames(DT,2:3,c("D","E"))        # multiple
setnames(DT,c("a","E"),c("A","F")) # multiple by name
setnames(DT,c("X","Y","Z"))        # replace all
#+end_src
*** rounding up data
**** round up lat/long
#+begin_src R
aodidlur[, aodid := paste(round(long_aod, 4), round(lat_aod, 4), sep = "_")]
#+end_src
*** Dates
**** convert to R dates
#+BEGIN_SRC R
SO2[, day:=as.Date(strptime(date, "%d/%m/%Y"))]
#+END_SRC

**** extract part of a date

#+begin_src R
#for numeric
am2.lu[, m := as.numeric(format(date, "%m")) ]
#as a factor with levels
am2.lu[, m:=factor(format(date, "%m"), levels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"))]

#for years
SO2[, c := as.numeric(format(day, "%Y")) ]
#+end_src
**** subset by date range

#+BEGIN_SRC R
mod1[day >= as.Date("2007-01-01") & day < as.Date("2008-01-01")]
#+END_SRC

*** Create unique dataset
#+begin_src R
tryu<-try2[,unique(guid)]
#+end_src
*** duplicate a data.table
make sure to use copy other wise it will change var names in both tables

#+begin_src R
new<-copy(old)
#+end_src
****
*** order (sort) in data.table
#+begin_src R
dt[order(VAR]
#+end_src

you can also sort by multiple variables and rank ₆In example₆

#+begin_src R
You can use DT[order(-rank(x), y)]
#+end_src

reslults:
#+BEGIN_EXAMPLE
   x y v
1: c 1 7
2: c 3 8
3: c 6 9
4: b 1 1
5: b 3 2
6: b 6 3
7: a 1 4
8: a 3 5
9: a 6 6
#+END_EXAMPLE
*** recode data.table
**** recode missing to 0
#+begin_src R
pm10all[is.na(Dust), Dust:= 0]
#+end_src
**** recode value to Value
#+BEGIN_SRC R
x<- mod3[predicted.m3  < 0 , predicted.m3  := 0.01]
#+END_SRC

*** choose based on best avilable
#+begin_src R
# store the best available
mod3best <- mod3[, list(aodid, elev, x_aod_utm, y_aod_utm, day, pm_mod3)]
setkey(mod3best, day, aodid)
setkey(mod2, day, aodid)
mod3best <- merge(mod3best, mod2[,list(aodid, day, predicted.m2)], all.x = T)
setkey(mod1,day,aodid)
mod3best <- merge(mod3best, mod1[,list(aodid, day, predicted.m1 = predicted)], all.x = T)
head(mod3best,1)
mod3best[,bestpred := pm_mod3]
mod3best[!is.na(predicted.m2),bestpred := predicted.m2]
mod3best[!is.na(predicted.m1),bestpred := predicted.m1]
#+end_src
*** Sampling
**** subset a dataset based on a sample of values
#+begin_src R
mlist<-(mod1$SiteCode)
mod1.cb <- mod1[!SiteCode %in% sample(mlist, 3), ]
#+end_src
*** create a loop to import files
#+begin_src R
allbestpredlist <- list()
path.data<-"/home/zeltak/smb4k/ZUNISYN/ZUraid/Uni/Projects/P031_MIAC_PM/3.Work/2.Gather_data/FN008_model_prep/"

for(i in 2003:2004){
  allbestpredlist[[paste0("year_", i)]] <- readRDS(paste0(path.data, "mod3best_", i, ".rds"))
  print(i)
} 
allbestpred <- rbindlist(allbestpredlist)
rm(allbestpredlist)
#+end_src
 
*** Regression Tips and tricks
**** run regression with a by statment
#+begin_src R
############################################
# data.table - run and store regression with grouping variables
DF <- read.table(text="Brand    Day     Rev     RVP              
A      1        2535.00  195.00 
B      1        1785.45  43.55 
C      1        1730.87  32.66 
A      2        920.00   230.00
B      2        248.22   48.99 
C      3        16466.00 189.00      
A      1        2535.00  195.00 
B      3        1785.45  43.55 
C      3        1730.87  32.66 
A      4        920.00   230.00
B      5        248.22   48.99 
C      4        16466.00 189.00", header=TRUE)
DF
library(data.table)

DT <- data.table(DF)
Mod.tbl<-DT[, list(mod=list(lm(Rev~Day))), by=list(Brand)]
# pull out the coefficients for Day
Mod.tbl[ , coef(mod[[1]])["Day"], by= Brand]
# residuals
Mod.tbl[ , resid(mod[[1]]), by= Brand]
# rsq
Mod.tbl[ , summary(mod[[1]])$r.squared, by= Brand]
# save out brand-specific predictions
Mod.tbl[ , predict(mod[[1]]), by= Brand]
# just do one brand for comparison
predict(lm(Rev~Day, data = DT[Brand == "A"]))

DT[, summary(lm(Rev~predbrand))$r.squared]
# note this is different than prediction from a single model
#+end_src

**** regression by monitor
#+begin_src R
 # by mon
 monsummary <- ddply(pm1[yr > 2006], .(mon), function(df) {
   res.mod1 <- lm(daymean ~ AOD,data =  df, na.action = na.omit)
   data.frame(nobs = nrow(df), adj.r.sq = round(as.numeric(summary(res.mod1)$adj.r.squared), 3),
              long_aod = df$long_aod[1], lat_aod = df$lat_aod[1],
              pm_x = df$pm_x[1], pm_y = df$pm_y[1], daymeanmon = mean(df$daymean))
   #print(summary(res.mod1))
 })
 monsummary <- monsummary[order(monsummary$adj.r.sq),]
 monsummary
 dput(head(monsummary[order(monsummary$adj.r.sq, decreasing = T),"mon"], 12))
#+end_src
*** Discriptives
**** count number of obvs per ID 
this works when there isnt NAs in the DATASET
#+BEGIN_SRC emacs-lisp :results none
pm2009[, .N, by=c("guid")]
# for multiple vars
pm2009[, .N, by=c("guid","year")]
#+END_SRC
**** aggregate observations by station with missing
#+begin_src R
#temp readings by station
temp[,length(na.omit(Temp)),by=list(stn)]
#temp readings by station and year
temp[,length(na.omit(Temp)),by=list(stn,year)]
#@NOTE-gives missing in each stn each year= 0 means no data 365 means full TS@
#+end_src
@NOTE-gives missing in each stn each year= 0 means no data 365 means full TS@
*** by statments

#+begin_src R
#single
,by=day
#multiple
by=list(c,stn)
#+end_src
     
*** Tips
**** construct a dataset based on means, SD etc
#+BEGIN_SRC R
loclongsummary <- loclong[, list(pmpreg = mean(.SD[day<birthdate,bestpred], na.rm = T), 
                  pm1styrpp = mean(.SD[day>=birthdate & day<(birthdate + 365),bestpred]),
                  pm2ndyrpp = mean(.SD[day>=(birthdate + 365) & day<(birthdate +365*2),bestpred]),
                  pm3rdyrpp = mean(.SD[day>=(birthdate + 365*2) & day<(birthdate + 365*3),bestpred]),
                 pm3yrspp = mean(.SD[day>=birthdate & day<(birthdate + 365*3),bestpred]),
                pmbirthtonow = mean(.SD[day>=birthdate,bestpred]),
                ndaysbirthtonow = nrow(.SD[day>=birthdate])),by=folio]

#+END_SRC

* SQLdf
** intro
*** General
SQL is not case-sensitive!
*** Logical operators
*** Null
In SQL NULL means missing value.  Confusingly R also has NULL but the equivalent of SQL NULL is NA in R.
*** Quotes
In SQL single quotes are used to delimit character strings.  A single quote inside a string is given with two single quotes in a row. 
Some implementations allow you to specify the delimiter.
** Data managment
*** Subsetting data
**** simple subset (Subsetting columns)
Columns in SQL are also called “fields”.  In R it is rather common for columns to be called “variables”.

In SQL the subset of columns is determined by select.  Here we want to get the Type and conc columns:

#+BEGIN_SRC R
DF2 <- sqldf("select VAR1, VAR2 from DF1")
#example
s01 <- sqldf("select Type, conc from myCO2")
#for 1 column
s03 <- sqldf("select Type from myCO2")
#+END_SRC
**** subset all colums
All columns

An asterisk is used in SQL to indicate that you want all columns:

#+BEGIN_SRC R
DF2 <- sqldf("select * from DF1") 
#example
s02 <- sqldf("select * from myCO2") 
#+END_SRC
**** Subset Rows
***** simple row subset
n SQL a common synonym for “row” is “record”.  In R a common synonym is “observation”.

Conditions-The common way of getting a subset of rows in SQL is with the 'where' command:

#+BEGIN_SRC R
DF2 <- sqldf("select * from DF1 where STATEMTN XX")
#example
s05 <- sqldf("select * from myCO2 where uptake < 20")
#+END_SRC
***** with logical operators 
Logical comparisons in SQL are combined with AND and OR:

#+BEGIN_SRC R
s06 <- sqldf("select * from myCO2 where uptake < 20 and Type='Quebec'")
#+END_SRC

$Note that testing equality is with =. $
**** limiting subset
***** with the limit command
First few

The limit command in SQL limits the number of rows that are given:

#+BEGIN_SRC sh
s07 <- sqldf("select * from myC02 limit 6")
#+END_SRC

%Tip a way to see just the column names is to limit the number of rows to zero%
**** selecting non missing
#+begin_src R
sqldf("select age, count(*) from titanic3 where age is not null group by age")
#+end_src
*** Merge/Join
**** SQL Join Types:

from: http://www.tutorialspoint.com/sql/sql-using-joins.htm

There are different types of joins available in SQL:

INNER JOIN: returns rows when there is a match in both tables.

LEFT JOIN: returns all rows from the left table, even if there are no matches in the right table.

RIGHT JOIN: returns all rows from the right table, even if there are no matches in the left table.

FULL JOIN: returns rows when there is a match in one of the tables.

SELF JOIN: is used to join a table to itself as if the table were two tables, temporarily renaming at least one table in the SQL statement.

CARTESIAN JOIN: returns the Cartesian product of the sets of records from the two or more joined tables.
**** Left Join

The SQL LEFT JOIN returns all rows from the left table, even if there are no matches in the right table. This means that if the ON clause matches 0 (zero) records in right table, the join will still return a row in the result, but with NULL in each column from right table.

This means that a left join returns all the values from the left table, plus matched values from the right table or NULL in case of no matching join predicate.
Syntax:

The basic syntax of LEFT JOIN is as follows:

#+BEGIN_EXAMPLE
SELECT table1.column1, table2.column2...
FROM table1
LEFT JOIN table2
ON table1.common_filed = table2.common_field;
#+END_EXAMPLE

Here given condition could be any given expression based on your requirement.
Example:

Consider the following two tables, (a) CUSTOMERS table is as follows:

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+

(b) Another table is ORDERS as follows:

+-----+---------------------+-------------+--------+
| OID | DATE | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+

Now, let us join these two tables using LEFT JOIN as follows:

#+BEGIN_SRC sql
SELECT  ID, NAME, AMOUNT, DATE
  FROM CUSTOMERS
  LEFT JOIN ORDERS
  ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID;
#+END_SRC

This would produce the following result:

+----+----------+--------+---------------------+
| ID | NAME     | AMOUNT | DATE                |
+----+----------+--------+---------------------+
|  1 | Ramesh   |   NULL | NULL                |
|  2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|  3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|  3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|  4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
|  5 | Hardik   |   NULL | NULL                |
|  6 | Komal    |   NULL | NULL                |
|  7 | Muffy    |   NULL | NULL                |
+----+----------+--------+---------------------+
**** Inner join

The most frequently used and important of the joins is the INNER JOIN. They are also referred to as an EQUIJOIN.

The INNER JOIN creates a new result table by combining column values of two tables (table1 and table2) based upon the join-predicate. The query compares each row of table1 with each row of table2 to find all pairs of rows which satisfy the join-predicate. When the join-predicate is satisfied, column values for each matched pair of rows of A and B are combined into a result row.
Syntax:

The basic syntax of INNER JOIN is as follows:

#+BEGIN_EXAMPLE
SELECT table1.column1, table2.column2...
FROM table1
INNER JOIN table2
ON table1.common_filed = table2.common_field;
#+END_EXAMPLE

Example:

Consider the following two tables, (a) CUSTOMERS table is as follows:

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+

(b) Another table is ORDERS as follows:

+-----+---------------------+-------------+--------+
| OID | DATE                |          ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+

Now, let us join these two tables using INNER JOIN as follows:

SQL> SELECT  ID, NAME, AMOUNT, DATE
     FROM CUSTOMERS
     INNER JOIN ORDERS
     ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID;

This would produce the following result:

+----+----------+--------+---------------------+
| ID | NAME     | AMOUNT | DATE                |
+----+----------+--------+---------------------+
|  3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|  3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|  2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|  4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
+----+----------+--------+---------------------+
* ggplot
*** basic
The simplest approach for creating graphs in ggplot2 is through the
qplot() or quick plot function. The format is:
#+BEGIN_EXAMPLE
qplot(x, y, data=, color=, shape=, size=, alpha=, geom=, method=,
formula=, facets=, xlim=, ylim=, xlab=, ylab=, main=, sub=)
#+END_EXAMPLE
where the parameters/options are defined
*** Histogram
**** basics
#+begin_src R
ggplot(p2006, aes(x=PM25)) + geom_histogram(binwidth=5, colour="black", fill="white")
#+end_src

*** qplot
**** basic manual

[[R_files/attach/qplot.pdf][Attachment #04 (qplot.pdf)]]
 [[R_files/attach/qplot2.pdf][Attachment #05 (qplot2.pdf)]]
 See attahced

**** A simple histogram in ggplot2

*A simple histogram in ggplot2
*
 For a histogram, all we need to tell qplot()is which dataframe to look
in and which variable is on the x axis. I also added in a plot title
with the "main=" argument. Pretty easy!
 qplot(data=myData,x=BM,main="Histogram of BodyMass")
 [[R_files/img/Image_xMFhU97XAGzh0lbj7N2wmQ_0002.jpg]]
 Pasted from
<[[http://www.r-bloggers.com/basic-introduction-to-ggplot2/][http://www.r-bloggers.com/basic-introduction-to-ggplot2/]]>[[http://1.bp.blogspot.com/-83J_6bvtaNI/Ts--12sI61I/AAAAAAAAIJs/avUtn-8Y17M/s1600/BMHistogram.jpg][]]

**** A basic scatterplot

*A basic scatterplot
*Lets say I want to plot a variable against body mass, color coded by
taxonomic tribe. qplot() works just like regular plot() only much
smarter. For instance, you tell qplot to do the color coding with a
single argument "color=Tribe".
 Note also that I indicate I want to log both variables with the
log="xy" argument.
 Note further: the legend is handled AUTOMATICALLY BY DEFAULT
 (data=myData,x=BM,y=var1,log="xy",color=Tribe)
 [[R_files/img/Image_yHcX7yQJRCX3DDlDrQbY6Q_0002.jpg]]
 Pasted from
<[[http://www.r-bloggers.com/basic-introduction-to-ggplot2/][http://www.r-bloggers.com/basic-introduction-to-ggplot2/]]>[[http://2.bp.blogspot.com/-ko7p70PJfQI/T14nSB9L97I/AAAAAAAAItg/gZ5tjxW0TZM/s1600/var1.jpg][]]

**** Trend lines

*Trend lines - adding statistical transformation layers.
*
 We probably want to add a trend line to each plot as well. In ggplot2,
you think of a plot as made up of different layers. A trendline is a
statistical transformation layer that is overlaid on the graph. The
easiest way to do this is to use qplot() to recreate the exact plot we
just made, only this time instead of plotting it, we save it in a
variable to do further stuff to it.

 myGG<-qplot(data=myData,x=BM,y=var1,color=Tribe,facets=~Tribe)
 Then we can ADD A LAYER to this object in which we add a smoothing
statistic using "method='lm'" to add an ordinary least squares
trendline. We could do other smoothing functions like LOESS, among
others. The function stat\_smooth() creates a statistical
transformation layer that can be added to our existing plot. To add it
we....literally.....just add it.
 myGG<- myGG + stat\_smooth(method="lm")
 Now, to plot a ggplot2 object all you have to do is type its name.
 myGG
 By default, it plots itself to the graphics device. It is kind of hard
to see here, but by default all ggplot trend lines show confidence
intervals as gray areas surrounding the line.

 *QUICKER WAY:*
 you can instead add the smoother inside the plot itself for quicker
operation:
 *qplot(X,Y,data=DATASET, xlab="LABELX", ylab="LABELY",geom = c("point",
"smooth") )*
 IE:
 qplot(predicted,tac,data=F\_T2003\_All, xlab="Predicted temperature",
ylab="Air temperature",geom = c("point", "smooth") )

**** saving graphs to file is a snap with ggsave()

*saving graphs to file is a snap with ggsave()
*
 ggplot2 comes with a great function called *ggsave()* that takes all
the headache out of exporting graphics from R. The only required
parameter is a filename, like soL
 ggsave("LIBMFacetsWithTrend.jpg")
 BY DEFAULT, it detects the desired format based on the file extension
you give it and handles everything silently and efficiently.
 The default behavior is to save the last ggplot object that you plotted
to the graphic device, but of course you can pass it the name of any
saved ggplot object. You can also of course change things like output
dimensions and DPI, etc. Just have a look at *?ggsave*
 Pasted from
<[[http://www.r-bloggers.com/basic-introduction-to-ggplot2/][http://www.r-bloggers.com/basic-introduction-to-ggplot2/]]>
*** scatter plot
**** define point shape and size
geom_point(size=2,shape=19)

the default solid circles (shape #16) 
hollow ones (#21)
An alternative circle is (shape #19), which is also a solid circle, but comes out smooth in more cases
**** Grouping Data Points by a Variable Using Shape or Color

#+BEGIN_SRC R
ggplot(heightweight, aes(x=ageYear, y=heightIn, colour=sex)) + geom_point()
ggplot(heightweight, aes(x=ageYear, y=heightIn, shape=sex)) + geom_point()
#+END_SRC

$Note The grouping variable must be categorical—in other words, a factor or character vector. If it is stored as a vector of numeric values, it should be converted to a factor before it is used as a grouping variable $

It is possible to map a variable to both shape and colour, or, if you have multiple
grouping variables, to map different variables to them

#+BEGIN_SRC sh
ggplot(heightweight, aes(x=ageYear, y=heightIn, shape=sex, colour=sex)) + geom_point()
#+END_SRC

This will set different shapes and colors for the grouping varibles:
#+BEGIN_SRC sh
ggplot(heightweight, aes(x=ageYear, y=heightIn, shape=sex, colour=sex)) +
geom_point() +
smcale_shape_manual(values=c(1,2)) +
scale_colour_brewer(palette="Set1")
#+END_SRC
**** Adding Fitted Regression Model Lines

$Note: The linear regression line is not the only way of fitting a model to the data—in fact, it’s not even the default. If you add stat_smooth() without specifying the method, it will use a loess (locally weighted polynomial) curve$

To add a linear regression line to a scatter plot, add stat_smooth() and tell it to use
method=lm. First we’ll save the base plot object in sp, then we’ll add different components to it:

#+BEGIN_SRC sh
# The base plot
sp <- ggplot(heightweight, aes(x=ageYear, y=heightIn))
sp + geom_point() + stat_smooth(method=lm)
#+END_SRC

By default, stat_smooth() also adds a 95% confidence region for the regression fit. The confidence interval can be changed by setting level, or it can be disabled with se=FALSE

#+BEGIN_SRC sh
# 99% confidence region
sp + geom_point() + stat_smooth(method=lm, level=0.99)
# No confidence region
sp + geom_point() + stat_smooth(method=lm, se=FALSE)
#+END_SRC

The default color of the fit line is blue. This can be change by setting colour. As with any other line, the attributes linetype and size can also be set. To emphasize the line, you can make the dots less prominent by setting colour:
#+BEGIN_SRC sh
sp + geom_point(colour="grey60") + stat_smooth(method=lm, se=FALSE, colour="black")
#+END_SRC
**** Adding Annotations with Model Coefficients
you can manually add the text using annotate() 

#+BEGIN_SRC sh
sp + annotate("text", label="r^2=0.42", x=16.5, y=52)
#+END_SRC

%Tip-the X,Y are the positions to set the annotation and is relative to the actuall grid values%

Instead of using a plain text string, it’s also possible to enter formulas using R’s math
expression syntax, by setting parse=TRUE:

#+BEGIN_SRC sh
sp + annotate("text", label="r^2 == 0.42", parse = TRUE, x=16.5, y=52)
#+END_SRC
**** Annotation: automatically extract values from the model object and build an expression using those values
It’s possible to automatically extract values from the model object and build an expression using those values. In this example, we’ll create a string that, when parsed, returns
a valid expression:

#+BEGIN_SRC sh
eqn <- as.character(as.expression(
substitute(italic(y) == a + b * italic(x) * "," ~~ italic(r)^2 ~ "=" ~ r2,
list(a = format(coef(model)[1], digits=3),
b = format(coef(model)[2], digits=3),
r2 = format(summary(model)$r.squared, digits=2)
))))
eqn
"italic(y) == \"37.4\" + \"1.75\" * italic(x) * \",\" ~ ~italic(r)^2 ~ \"=\" ~
\"0.42\""
parse(text=eqn)
 # Parsing turns it into an expression
expression(italic(y) == "37.4" + "1.75" * italic(x) * "," ~ ~italic(r)^2 ~ "=" ~
"0.42")
#+END_SRC

Now that we have the expression string, we can add it to the plot. In this example we’ll
put the text in the bottom-right corner, by setting x=Inf and y=-Inf and using horizontal
and vertical adjustments so that the text all fits inside the plotting area (Figure 5-27):
sp + annotate("text", label=eqn, parse=TRUE, x=Inf, y=-Inf, hjust=1.1, vjust=-.5)
*** Controlling the Overall Appearance of Graphs
**** Changing the Appearance of Text (title, legends, and axes)

To set the appearance of theme items such as the title, axis labels, and axis tick marks, use theme() and set the item with element_text().

For example, 'axis.title.x' controls the appearance of the x-axis label and 'plot.title' controls the appearance of the title text:

#+BEGIN_EXAMPLE
#pBase plot
sp <- ggplot(heightweight,aes(x=ageYear, y=heightIn)) + geom_point()

sp+ theme(axis.title.x=element_text(size=16, lineheight=.9, family="Times",
                                  face="bold.italic", colour="red"))
#+END_EXAMPLE

also:

#+BEGIN_EXAMPLE
sp+ theme(axis.title=element_text(size=16, lineheight=.9, face="bold", colour="black"))+theme(plot.title=element_text(size=16, lineheight=.9, face="bold", colour="black")) 
#+END_EXAMPLE
**** set the appearance of text geoms (text inside plots)
To set the appearance of text geoms (text that’s in the plot itself, with geom_text() or
annotate()), set the text properties:

$Note For text geoms, font size is in mm$

#+BEGIN_EXAMPLE
p + annotate("text", x=15, y=53, label="Some text", size = 7, family="Times",
fontface="bold.italic", colour="red")
p + geom_text(aes(label=weightLb), size=4, family="Times", colour="red")
#+END_EXAMPLE

**** Combining Both 
example
#+BEGIN_EXAMPLE
sp + annotate("text", label="R^2==0.42",  parse = TRUE, x=0.09, y=69, size = 9, fontface="bold.italic", colour="grey")+ theme(axis.title=element_text(size=16, lineheight=.9, face="bold", colour="black"))+theme(plot.title=element_text(size=16, lineheight=.9, face="bold", colour="black"))
#+END_EXAMPLE
**** change the size and poistion of axis text
use the theme(axis.text.x = element_text(angle=0,size=16, vjust=1)) command
₆In example₆ 

#+begin_src R
temp+ theme(axis.title=element_text(size=16, lineheight=.9, face="bold", colour="black"))+theme(plot.title=element_text(size=16, lineheight=.9, face="bold", colour="black"))+theme(axis.text.x = element_text(angle=0,size=16, vjust=1)) 
#+end_src

**** Change scale of y or x axis
use the  scale_y_continuous option
where the numbers are the start and end of the scales

#+begin_src R
p2 <- ggplot(fin, aes(bid, bestpred)) + geom_boxplot(outlier.shape = NA) 
p2<-p2+ scale_y_continuous(limits=c(10.3, 10.6)) 
p2 
#+end_src

you can also use the short hand functions xlim and ylim

#+begin_src R
m + ylim(0, 5000)
m + ylim(1000, 10000)
m + xlim(7, 8) 
#+end_src

*** bar plot
#+BEGIN_SRC r
ggplot(data=test, aes(x=date, y=minutes)) + geom_bar(stat="identity")
#+END_SRC
*** box plot
**** simple box plot
#+BEGIN_SRC R
qplot(bid, bestpred, data = fin, geom="boxplot")
#+END_SRC
**** remove outliers in boxplot
#+begin_src R
qplot(bid, bestpred, data = fin, geom="boxplot")+geom_boxplot(outlier.shape = NA)
#+end_src
**** color the outliers
#+begin_src R
qplot(bid, bestpred, data = fin, geom="boxplot")+geom_boxplot(outlier.shape = NA)
p + geom_boxplot(outlier.colour = "green", outlier.size = 3)
#+end_src
**** Fill the boxplots with color
#+begin_src R
#fill with colors
p + geom_boxplot(aes(fill = factor(bid)))
#+end_src

*** Density plot
**** simple density plots

#+begin_src R
# The expand_limits() increases the y range to include the value 0
ggplot(faithful, aes(x=waiting)) + geom_line(stat="density") +
expand_limits(y=0)
#+end_src

*** identify outliers

#+begin_src r
# identify outliers
ggplot(mod1[,], aes(predicted, daymean)) + geom_abline(linetype = "dotted") +
  geom_point(alpha = 0.2) +
  geom_smooth() +
  coord_equal() +
  facet_wrap(~mon) + theme_bw(16)
#+end_src

*** Raster plots
raster plot by day:

#+begin_src r
# raster plot by day
library(data.table)
library(ggplot2)
# pull out a single day
datatable <- dat[day == as.Date("2009-02-26"),c("AOD", "lat_aod", "long_aod"), with = F]

# plot with colored solid square points
ggplot(datatable, aes(long_aod, lat_aod, color = AOD)) +
  geom_point(size = 4.15, shape = 15)


# function to plot by day
plotdaymex <- function(datatable, datestring,
                       zvar = "AOD", longvar = "long_aod", latvar = "lat_aod") {
  myvars <- c(zvar, longvar, latvar)
  daytitle <- datestring
  # check if flag variable exists
  if("flag" %in% colnames(datatable)){
    myvars <- c(myvars, "daymode3val", "daymode3prop", "flag")
  }
  daydat <- datatable[day == as.Date(datestring), myvars, with = F]
  if("flag" %in% colnames(datatable)) {
    daytitle <- paste0("AOD for ", daytitle, "; ", format(as.Date(datestring), "%Y%j"), "\nAOD mode: ", daydat[1,daymode3val],
                       ", ", daydat[1,100 * round(daymode3prop, 2)], "%; flag = ", daydat[1,flag])
  }
  ggplot(daydat,
         aes_string(x = longvar, y = latvar, color = zvar)) +
    geom_point(size = 4.15, shape = 15) +
    scale_colour_gradient(low = "#FFFFD4", high = "#8C2D04") +
    ggtitle(daytitle) +
    theme_bw(18) + theme(plot.title = element_text(size = 18))
 # ggsave(paste0("mexplot_", format(as.Date(datestring), "%Y%j"), ".pdf"))
 }

plotdaymex(dat, "2009-02-26")
plotdaymex(dat, "2009-02-27")
plotdaymex(dat, "2012-02-28")
plotdaymex(dat, "2012-05-06")
plotdaymex(dat, "2012-10-15")
plotdaymex(dat, "2012-10-16")
plotdaymex(dat, "2008-02-28")# a non-flagged day

pause <- function(){
  cat("Pause. Press <Enter> to continue, q to stop...")
  endplots <- readline()
  invisible(endplots)
}

start <- as.Date("2005-01-05")
for(i in 1:100){
  try(print(plotdaymex(dat, start + i)))
  endplots <- pause()
  if(endplots == "q") stop("end of plotting")
}

#+end_src
*** Saving
**** save last plot
ggsave currently recognises the extensions eps/ps, tex (pictex), pdf, jpeg, tiff, png, bmp, svg and wmf (windows only).

ggsave(file="length-hist.pdf") 

#+begin_src R
m2_agg<- ggplot(m2_agg, aes(long_aod,lat_aod, color = LTPM.m2)) + 
  geom_point(size = 4.15, shape = 15) + theme_bw()
ggsave(file="/home/zeltak/smb4k/ZUNISYN/ZUraid/Uni/Projects/P031_MIAC_PM/3.Work/2.Gather_data/FN000_RWORKDIR/LTPM.m2.png")
#+end_src
#+END_EXAMPLE
*** ggmap
**** create map rectangle (bounding box) by markingpoints

#+BEGIN_EXAMPLE
maintriangle <- gglocator(4)
#+END_EXAMPLE

grep a position of a variable in table based on name

#+BEGIN_SRC sh
grep("precip", names(mod1))
#+END_SRC
**** create a google map with data points
#+BEGIN_SRC R
library(ggmap)
MxC_Map_df <- get_map(location = 'massachusetts', maptype = "hybrid", zoom = 9)
str(MxC_Map_df)
P4 <- ggmap(MxC_Map_df, darken = c(0.5, "white"))
P4 + 
  geom_point(data = bw.o1 ,
             aes(-longdd, latdd, color = pmpreg, size = pmpreg)) + 
  theme_bw(10) + 
  ggtitle("Predictions over pregnancy"
#+END_SRC

* ESS
** starting ESS process  
*** to start a new process
ESS allows you to run more than one ESS process simultaneously in the same session. Each process has a name and a number
the initial process (process 1) is simply named ‘R+3:1’.
The name of the process is shown in the mode line in square brackets (for example, ‘[R+3:2]’)
this is useful if the process buffer is renamed.

₆In example₆ the buffers will look like `*R*` then `*R:2*` etc

Without a prefix argument ~M-x R~ starts a new ESS process, using the first available process number.
With a prefix argument (for R) ~C-u M-x R~  allows for the specification of command line options.

*** You can switch to any active ESS process
use the command `M-x ess-request-a-process`
Just enter the name of the process you require; completion is provided over the names of all running S processes. This is a good command to consider binding to a global key.
*** Changing the startup actions
If you do not wish ESS to prompt for a starting directory when starting a new process, set the variable ess-ask-for-ess-directory to nil. In this case, the starting directory will be set using one of the following methods:

If the variable ess-directory-function stores the name of a function, the value returned by this function is used. The default for this variable is nil.
Otherwise, if the variable ess-directory stores the name of a directory (ending in a slash), this value is used. The default for this variable is nil.
Otherwise, the working directory of the current buffer is used.

₆In example₆ 

#+BEGIN_SRC emacs-lisp :results none
(setq ess-ask-for-ess-directory nil)
;define deault ess dir
(setq ess-directory "/home/zeltak/ZH_tmp/")
#+END_SRC

*** ask for a filename for the transcript before the ESS process starts
    
** Useful keyboard shortcuts and tips for ESS/R
*** Control and up/down arrow keys to search history with matching what you've already typed:
#+BEGIN_SRC R
(define-key comint-mode-map [C-up] 'comint-previous-matching-input-from-input)
(define-key comint-mode-map [C-down] 'comint-next-matching-input-from-input)
#+END_SRC

*** Comment-uncomment a selected region with C-d or C-maj-d
 #+BEGIN_SRC R
(defun uncomment-region (beg end)
  "Like `comment-region' invoked with a C-u prefix arg."
  (interactive "r")
  (comment-region beg end -1))

(define-key ess-mode-map (kbd "C-d") 'comment-region)
(define-key ess-mode-map (kbd "C-S-d") 'uncomment-region)
 #+END_SRC
*** Handy commands
Command: ess-handy-commands
Request and execute a command from ess-handy-commands list.

 
* org mode and r
** basics shortcuts
?C-c ' ?- open code block for editing in ESS
** Use ESS to step through evaluation line-by-line
Use ?C-c '? to visit the edit buffer for your code block
Use ess-eval-line-and-step to evaluate each line in turn

In addition to ess-eval-line-and-step, there are several other ESS functions with names beginning ess-eval-*. They evaluate lines and regions in different ways; it's worth looking at their descriptions (C-h f).

** code block output
*** types of output
the normal output is in org mode tables. to get a classic R console output use:

#+BEGIN_EXAMPLE
#+begin_src R :results output
#+END_EXAMPLE

$Note- changing the :results argument to `value` is the same as omitting the argument completely, since 'value' is the default )$

*** Passing data between code blocks (sessions)			        
Often in R, we will define functions or objects in one code block
and want to use these objects in subsequent code blocks. However,
each time we submit a code block using C-c C-c, org-mode is firing up
an R session, submitting the code, obtaining the return values, and
closing down R. So, by default, our R objects aren't persistent!
That's an important point. Fortunately, there is an easy way to tell
org-mode to submit our code blocks to a running R process in Emacs,
just like we do with R files in ESS.

You simply use the :session argument to the org-mode source block.   

#+begin_src R :session :results output
  square <- function(x) {
    x * x
  }
  x <- 1:10 
#+end_src 

%tip- it is helpful to name the session, ₆In example₆ use :session R1%

** Grphical output
*** Inserting R graphical output 

Here is a really cool feature of evaluating source code in
org-mode. We can insert images generated by R code blocks inline in
our Emacs buffer! To enable this functionality, we need to evaluate a
bit of Emacs lisp code. If this feature is something you want every
time you use org-mode, consider placing the code in your Emacs
initialization file. Either way, evaluate it with C-c C-c.

#+begin_src emacs-lisp :results silent :exports code
  (add-hook 'org-babel-after-execute-hook 'org-display-inline-images)   
  (add-hook 'org-mode-hook 'org-display-inline-images)   
#+end_src

The following R code generates some graphical output. There are
several things to notice.

1) =:results output graphics= is specified. The 'graphics' value is
   one we have not seen yet, and lets org-mode know that our code
   block will be producing a figure of some sort. We need to specify
   the 'output' value to the :results argument since we are generating
   a figure with ggplot2, which is a grid-based graphical system.

2) We use a new source code block argument, :file. This argument will
   capture output (a graphic in this case) from the source block and
   generate a file with the given name. Then, the results section
   becomes an org-mode link to the newly created file. In the example
   below, the file generated is called diamonds.png.

   Finally, If you have defined the Emacs lisp code for inline-image
   support above, an overlay of the file will be inserted inline in
   the actual org-mode document! Run the following source code block
   to see how it works.

#+begin_src R :results output graphics :file diamonds.png :bg "transparent"
  library(ggplot2)
  data(diamonds)
  dsmall <-diamonds[sample(nrow(diamonds), 100), ] 
  p <- qplot(carat, price, data = dsmall)
  
  plot.rrg <- function(...) roundrectGrob(gp = gpar(fill = "skyblue1", col = NA),
                                          r = unit(0.06, "npc"))
  
  panel.rrg <- function(...) roundrectGrob(gp = gpar(fill = "grey80", col = NA),
                                          r = unit(0.06, "npc"))
  
  p + opts(plot.background = plot.rrg) + opts(panel.background = panel.rrg)
  
#+end_src

** chain multiple babel blocks (execute multiple R code buffer)
use 'org-babel-execute-buffer' or  'org-babel-execute-subtree' 
** tangeling
*** intro
*** Tangling the document for selective code blocks 
Now that we have seen how to instruct org-mode how to produce source code files from our org-mode document, how do we actually tangle the document?

We simply have to call the 'org-babel-tangle' function, bound by default to ?C-c C-v C-t?

to do this you will need to have a the tangle switch in each code block 
#+BEGIN_SRC emacs-lisp :results none
:tangle yes

;;examples
:tangle no
;The default. The code block is not included in the tangled output.
:tangle yes
;Include the code block in the tangled output. The output file name is the name of the org file with the extension ‘.org’ replaced by the extension for the block language.
:tangle filename
;Include the code block in the tangled output to file ‘filename’. 
#+END_SRC								    
								  
*** Tangeling the full document

use a property drwaer in the top level header (*)

like this:

#+BEGIN_SRC emacs-lisp :results none
`*` Toplevel
    :PROPERTIES:
    :comments: org
    :tangle:   comments.r
    :END:
#+END_SRC

you can specify which type of comments are exported:

#+BEGIN_EXAMPLE

no
    retains its behavior of not tangling any comments 
yes
    retains its behavior of wrapping the code in links back to the original org-mode file 
link
    is synonymous with "yes" 
org
    does not wrap the code in links back to the original org file, but does include preceding text from the org-mode file as a comment before the code block 
both
    turns on both the "link" and "org" options 
#+END_EXAMPLE

and the output file in the ':tangle' part

* dplyr
** @warning-plyr will screw with dplyr results so be careful when loading both@
in general you should load plyr first and then dplyr
to be specific one can call dplyr specifacly with dplyr::COMMAND like:
#+BEGIN_SRC R
x<-pm25.m1.c %>%
    group_by(stn) %>%
    dplyr::summarise (avg_delay = mean(PM25, na.rm=TRUE))
#+END_SRC

** glimpse (dplyr str alternative)
#+BEGIN_SRC R
# dplyr approach: better formatting, and adapts to your screen width
glimpse(flights)
#+END_SRC

** subset data
*** filter command basics   
Base R approach to filtering forces you to repeat the data frame’s name
dplyr approach is simpler to write and read
Command structure (for all dplyr verbs):
first argument is a data frame
return value is a data frame
nothing is modified in place
$Note$ dplyr generally does not preserve row names
*** simple subset ("filter")
#+BEGIN_SRC R
# dplyr approach
# note: you can use comma or ampersand to represent AND condition
filter(flights, Month==1, DayofMonth==1)
#+END_SRC
*** using and/OR
#+BEGIN_SRC R
# use pipe for OR condition
filter(flights, UniqueCarrier=="AA" | UniqueCarrier=="UA")
#+END_SRC
*** using with %in% (select values to keep by matching in vector/dataset) 
#+BEGIN_SRC R
# you can also use %in% operator
filter(flights, UniqueCarrier %in% c("AA", "UA"))
#+END_SRC

*** select: Pick columns by name
Base R approach is awkward to type and to read
    dplyr approach uses similar syntax to filter
    Like a SELECT in SQL
use colon to select multiple contiguous columns, and use `contains` to match columns by name
$Note$  `starts_with`, `ends_with`, and `matches` (for regular expressions) can also be used to match columns by name

*** select columns
$Note$ the first word is for the DF
#+BEGIN_SRC R
# dplyr approach
#select(DF, VAR1, VAR2, VAR3)
select(flights, DepTime, ArrTime, FlightNum)
#+END_SRC
*** advanced select columns
you can select a range using the : symbol
#+BEGIN_SRC R
# use colon to select multiple contiguous columns, and use `contains` to match columns by name
# note: `starts_with`, `ends_with`, and `matches` (for regular expressions) can also be used to match columns by name
select(flights, Year:DayofMonth, contains("Taxi"), contains("Delay"))
#+END_SRC
*** select columns by range
#+BEGIN_SRC R
#select(DF, colXname:colYname)
select(flights, Year:DayofMonth)
#+END_SRC
** Chaining
*** intro
$Note- this is not dplyr sepcific$
Usual way to perform multiple operations in one line is by nesting
Can write commands in a natural order by using the %>% infix operator (which can be pronounced as “then”)
Chaining increases readability significantly when there are many commands
Operator is automatically imported from the magrittr package
Can be used to replace nesting in R commands outside of dplyr

*** base example
#+BEGIN_SRC R
# chaining method
flights %>%
    select(UniqueCarrier, DepDelay) %>%y
    filter(DepDelay >60)
#+END_SRC
this will select the DF 'flight'
then ( %>%) select columns
then ( %>%) filter deepelay >60 

-another example

#+BEGIN_SRC R
# chaining method
(x1-x2)^2 %>% sum() %>% sqrt()
#+END_SRC

** sort								  :CANCELLED:
*** base sort 
#+BEGIN_SRC R
# dplyr approach
flights %>%
    select(UniqueCarrier, DepDelay) %>%
    arrange(DepDelay)
#+END_SRC
select from 'fight' DF 
from columns UniqueCarrier, DepDelay
then sort by DepD

for descending:

#+BEGIN_SRC R
# use `desc` for descending
flights %>%
    select(UniqueCarrier, DepDelay) %>%
    arrange(desc(DepDelay))
#+END_SRC
 
** mutate (Add new variables)
*** base
$Note- you don't have to select columns as in the below example$  
#+BEGIN_SRC R
# dplyr approach (prints the new variable but does not store it)
flights %>%
    select(Distance, AirTime) %>%
    mutate(Speed = Distance/AirTime*60)
# Alternativly to store the new variable
flights <- flights %>% mutate(Speed = Distance/AirTime*60)
#+END_SRC

*** create a new variable based in conditions
**** using dplyr extra
$Note$ you need to install the package first:
%Tip% for debian install libcurl4-gnutls-dev beforehand
#+BEGIN_SRC R
library(devtools);
install_github(repo="dplyrExtras", username="skranz")
#+END_SRC

then issues this

#+BEGIN_SRC R
mutate_if(bwfull.ptvl,birthyear==2003|birthyear==2004,yearb=2004)
#+END_SRC
where for DT bwfull.ptvl, for years 2003 and 2004 rows the variable yearb will get 2004



** aggregate/summary
*** base
summarise: Reduce variables to values

Primarily useful with data that has been grouped by one or more variables
group_by creates the groups that will be operated on
summarise uses the provided aggregation function to summarise each group

*** simple summary 
#+BEGIN_SRC R
# dplyr approach: create a table grouped by Dest, and then summarise each group by taking the mean of ArrDelay
flights %>%
    group_by(Dest) %>%
    summarise(avg_delay = mean(ArrDelay, na.rm=TRUE))
#+END_SRC

select from DF 'flights'
then group by summary variable 'Dest'
then run the process creating a mean of 'ArrDelay' by 'Dest' into a variable called avg_delay

aggregate XY by station
#+BEGIN_SRC R
#aggregate station xy
stnxy<-pm25.m1.c %>%
  group_by(stn) %>%
  dplyr::summarise(x = mean(x_stn_ITM, na.rm=TRUE),y = mean(y_stn_ITM, na.rm=TRUE) )
#+END_SRC

*** count number of obsv per station

#+BEGIN_SRC R
tst<-met2007 %>%
    group_by(stn) %>%
    summarise(data = n())
#+END_SRC

*** aggrtegate by XY
#+BEGIN_SRC R
ugrid <-data.m3 %>%
    group_by(aodid) %>%
    summarise(lat_aod = mean(lat_aod, na.rm=TRUE),  long_aod = mean(long_aod, na.rm=TRUE),x_aod_ITM = mean(x_aod_ITM, na.rm=TRUE),  y_aod_ITM = mean(y_aod_ITM, na.rm=TRUE))

#aggregate and write to csv
write.csv (x <-m7.m2 %>%
    group_by(aodid) %>%
    summarise(lat_aod = mean(lat_aod, na.rm=TRUE),  long_aod = mean(long_aod, na.rm=TRUE),x_aod_ITM = mean(x_aod_ITM, na.rm=TRUE),  y_aod_ITM = mean(y_aod_ITM, na.rm=TRUE)) , "/home/zeltak/ZH_tmp/m7tst.csv")
 
#+END_SRC

*** summary by 2 grouped variables
#+BEGIN_SRC R
flights %>%
  group_by(Dest,Month) %>%
  summarise(avg_delay = mean(ArrDelay, na.rm=TRUE))
#+END_SRC

*** summarise each command (summary for 2 or more variable at the same time)
summarise_each allows you to apply the same summary function to multiple columns at once
$Note$ mutate_each is also available

BASE example:

#+BEGIN_SRC R
#summurize all variables in data set (only works for continous)
AYLTPM<- LTPM.ALL %>%
    group_by(aodid) %>%
    summarise_each(funs(mean))
#+END_SRC

second example:

#+BEGIN_SRC R
#for each carrier, calculate the percentage of flights cancelled or diverted
flights %>%
    group_by(UniqueCarrier) %>%
    summarise_each(funs(mean), Cancelled, Diverted)
#+END_SRC

here we get the mean of the 2 variables canceled and diverted (by Uniquecarrier)
#+BEGIN_EXAMPLE
## Source: local data frame [15 x 3]
## 
##    UniqueCarrier Cancelled Diverted
## 1             AA  0.018496 0.001850
## 2             AS  0.000000 0.002740
## 3             B6  0.025899 0.005755
## 4             CO  0.006783 0.002627
## 5             DL  0.015903 0.003029
## 6             EV  0.034483 0.003176
## 7             F9  0.007160 0.000000
## 8             FL  0.009818 0.003273
## 9             MQ  0.029045 0.001936
## 10            OO  0.013947 0.003487
## 11            UA  0.016409 0.002413
## 12            US  0.011269 0.001470
## 13            WN  0.015504 0.002294
## 14            XE  0.015496 0.003450
#+END_EXAMPLE

A more complex example

#+BEGIN_SRC R
flights %>%
    group_by(UniqueCarrier) %>%
    summarise_each(funs(min(., na.rm=TRUE), max(., na.rm=TRUE)), matches("Delay"))
#+END_SRC
here we get min and max summary that matches ANY variable that has delay in it
$Note- here because there are NA's you have to tell it to remove NAs$ 
the '.' is a placeholder for the data your placing in 

#+BEGIN_EXAMPLE
## Source: local data frame [15 x 5]
## 
##    UniqueCarrier ArrDelay_min DepDelay_min ArrDelay_max DepDelay_max
## 1             AA          -39          -15          978          970
## 2             AS          -43          -15          183          172
## 3             B6          -44          -14          335          310
## 4             CO          -55          -18          957          981
## 5             DL          -32          -17          701          730
## 6             EV          -40          -18          469          479
## 7             F9          -24          -15          277          275
## 8             FL          -30          -14          500          507
## 9             MQ          -38          -23          918          931
## 10            OO          -57          -33          380          360
## 11            UA          -47          -11          861          869
## 12            US          -42          -17          433          425
## 13            WN          -44          -10          499          548
## 14            XE          -70          -19          634          628
## 15            YV          -32          -11           72           54
#+END_EXAMPLE

₆In example₆ 
*** create single aod point per aodid per day
#+BEGIN_SRC R
aqua <-aqua %>%
    group_by(aodid,day) %>%
    summarise_each(funs(mean),lat_aod,aod,aod,UN,WV,day,x_aod_ITM, y_aod_ITM ,MaskAdjacency)
#+END_SRC

*** counts the number of rows in a group: n()
Helper function n() counts the number of rows in a group
#+BEGIN_SRC R
# for each day of the year, count the total number of flights and sort in descending order
flights %>%
    group_by(Month, DayofMonth) %>%
    summarise(flight_count = n()) %>%
    arrange(desc(flight_count))
#+END_SRC

this gives you the number(using n() ) of obsv per group criteria

#+BEGIN_EXAMPLE
## Source: local data frame [365 x 3]
## Groups: Month
## 
##    Month DayofMonth flight_count
## 1      8          4          706
## 2      8         11          706
## 3      8         12          706
## 4      8          5          705
## 5      8          3          704
## 6      8         10          704
## 7      1          3          702
## 8      7          7          702
## 9      7         14          702
## 10     7         28          701
## ..   ...        ...          ...
#+END_EXAMPLE

%Tip% you can use 'tally' as a simpler coding alternative
#+BEGIN_SRC R
# rewrite more simply with the `tally` function
flights %>%
    group_by(Month, DayofMonth) %>%
    tally(sort = TRUE)
#+END_SRC

#+BEGIN_EXAMPLE
## Source: local data frame [365 x 3]
## Groups: Month
## 
##    Month DayofMonth   n
## 1      8          4 706
## 2      8         11 706
## 3      8         12 706
## 4      8          5 705
## 5      8          3 704
## 6      8         10 704
## 7      1          3 702
## 8      7          7 702
## 9      7         14 702
## 10     7         28 701
## ..   ...        ... ...
#+END_EXAMPLE

*** counting and including distinct (unique) counts: n_distinct
Helper function n_distinct(vector) counts the number of unique items in that vector
#+BEGIN_SRC R
# for each destination, count the total number of flights and the number of distinct planes that flew there
ights %>%
    group_by(Dest) %>%
    summarise(flight_count = n(), plane_count = n_distinct(TailNum))
#+END_SRC

#+BEGIN_EXAMPLE
## Source: local data frame [116 x 3]
## 
##    Dest flight_count plane_count
## 1   ABQ         2812         716
## 2   AEX          724         215
## 3   AGS            1           1
## 4   AMA         1297         158
## 5   ANC          125          38
## 6   ASE          125          60
## 7   ATL         7886         983
## 8   AUS         5022        1015
## 9   AVL          350         142
## 10  BFL          504          70
## ..  ...          ...         ...
#+END_EXAMPLE

*** grouping without summary 
Grouping can sometimes be useful without summarising

#+BEGIN_SRC R
# for each destination, show the number of cancelled and not cancelled flights
flights %>%
    group_by(Dest) %>%
    select(Cancelled) %>%
    table() %>%
    head()
#+END_SRC


#+BEGIN_EXAMPLE
##      Cancelled
## Dest     0  1
##   ABQ 2787 25
##   AEX  712 12
##   AGS    1  0
##   AMA 1265 32
##   ANC  125  0
##   ASE  120  5
#+END_EXAMPLE
** Window Functions
*** intro
Aggregation function (like mean) takes n inputs and returns 1 value
Window function takes n inputs and returns n values
Includes ranking and ordering functions (like min_rank), offset functions (lead and lag), and cumulative aggregates (like cummean).
*** Group rank and order example
for each carrier, calculate which two days of the year they had their longest departure delays
$Note-  smallest (not largest) value is ranked as 1, so you have to use `desc` to rank by largest value$

#+BEGIN_SRC R
flights %>%
    group_by(UniqueCarrier) %>%
    select(Month, DayofMonth, DepDelay) %>%
    filter(min_rank(desc(DepDelay)) <= 2) %>%
    arrange(UniqueCarrier, desc(DepDelay))
#+END_SRC
we grouped by carrier then select the columns we want to show 
then we filter the rows by minimum rank of depdelay (descending)

#+BEGIN_EXAMPLE
## Source: local data frame [30 x 4]
## Groups: UniqueCarrier
## 
##    UniqueCarrier Month DayofMonth DepDelay
## 1             AA    12         12      970
## 2             AA    11         19      677
## 3             AS     2         28      172
## 4             AS     7          6      138
## 5             B6    10         29      310
## 6             B6     8         19      283
## 7             CO     8          1      981
## 8             CO     1         20      780
## 9             DL    10         25      730
## 10            DL     4          5      497
## 11            EV     6         25      479
## 12            EV     1          5      465
## 13            F9     5         12      275
## 14            F9     5         20      240
## 15            FL     2         19      507
## 16            FL     3         14      493
## 17            MQ    11          8      931
## 18            MQ     6          9      814
## 19            OO     2         27      360
## 20            OO     4          4      343
## 21            UA     6         21      869
## 22            UA     9         18      588
## 23            US     4         19      425
## 24            US     8         26      277
## 25            WN     4          8      548
## 26            WN     9         29      503
## 27            XE    12         29      628
## 28            XE    12         29      511
## 29            YV     4         22       54
## 30            YV     4         30       46
#+END_EXAMPLE

$Note- you can simplify the code with the top_n command$ 
#+BEGIN_SRC R
flights %>%
    group_by(UniqueCarrier) %>%
    select(Month, DayofMonth, DepDelay) %>%
    top_n(2) %>%
    arrange(UniqueCarrier, desc(DepDelay))
#+END_SRC

*** example of creating a variable (mutate) with a lag function
#+BEGIN_SRC R
# for each month, calculate the number of flights and the change from the previous month
flights %>%
    group_by(Month) %>%
    summarise(flight_count = n()) %>%
    mutate(change = flight_count - lag(flight_count))
#+END_SRC

here we create a summarise variable called 'flight_count' (by month) then
create a variable 'change' based on the diff between 'flight_count' and the lag of 'flight_count (change from prev month)
%Tip lag- earlier value // lead next value %

#+BEGIN_EXAMPLE
## Source: local data frame [12 x 3]
## 
##    Month flight_count change
## 1      1        18910     NA
## 2      2        17128  -1782
## 3      3        19470   2342
## 4      4        18593   -877
## 5      5        19172    579
## 6      6        19600    428
## 7      7        20548    948
## 8      8        20176   -372
## 9      9        18065  -2111
## 10    10        18696    631
## 11    11        18021   -675
## 12    12        19117   1096
#+END_EXAMPLE

simplify with tally

#+BEGIN_SRC R
# rewrite more simply with the `tally` function
flights %>%
    group_by(Month) %>%
    tally() %>%
    mutate(change = n - lag(n))
#+END_SRC
** sampling
*** randomly sample a fixed number of rows, without replacement (sample_N)
#+BEGIN_SRC R
# randomly sample a fixed number of rows, without replacement
flights %>% sample_n(5)
#+END_SRC

*** randomly sample a fraction of rows, with replacement (sample_frac)
#+BEGIN_SRC R
# randomly sample a fraction of rows, with replacement
flights %>% sample_frac(0.25, replace=TRUE)
#+END_SRC

** do()
*** base
As well as the specialised operations described above, dplyr also provides the generic do() function which applies any R function to each group of the data.

do() also has an automatic progress bar. It appears if the computation takes longer than 5 seconds and lets you know (approximately) how much longer the job will take to complete.

$Note the use of the '.' pronoun to refer to the data in the current group.$

#+BEGIN_SRC R
library(dplyr)
models <- mtcars %>% group_by(cyl) %>% do(lm = lm(mpg ~ wt, data = .))
models %>% summarise(rsq = summary(lm)$r.squared)
#+END_SRC

*** Run lmer models using dplyer do()
**** Run models by a grouping varialbles
#+begin_src R
library(dplyr)

#####example 1 
#base model for stage 1
m1.formula <- as.formula(PM10~ aod+ (1+aod|day))
t1<- mod1 %>% group_by(c) %>% do(function(df){lmer(m1.formula,data=df)})

####example 2 with summary and rename
seas2007<- mod1PM25_2007 %>% group_by(season) %>% do(function(df){summary(lm(m1.formula,data=df))})
names(seas2007)<-c("1-winter", "2-spring","3-summer","4-autum")
seas2007[["1-winter"]][8]
seas2007[["2-spring"]][8]
seas2007[["3-summer"]][8]
seas2007[["4-autum"]][8]


#+end_src
**** run models with 2 grouping variables
#+begin_src r
pmbyc<- mod1pm25 %>%
group_by(c,season) %>%
do(function(df){summary(lm(m1.formula,data=df))})
#+end_src

$Note-results will output by order of first the second by statment$  
₆In example₆ in above example the first 4 results will be for 2002 and then each season, then 2003 and each 4 seasons etc..
** Connecting to Databases
*** base
dplyr can connect to a database as if the data was loaded into a data frame
Use the same syntax for local data frames and databases
Only generates SELECT statements
Currently supports SQLite, PostgreSQL/Redshift, MySQL/MariaDB, BigQuery, MonetDB
Example below is based upon an SQLite database containing the hflights data

#+BEGIN_SRC R
# connect to an SQLite database containing the hflights data
my_db <- src_sqlite("my_db.sqlite3")
#+END_SRC

#+BEGIN_EXAMPLE
## Loading required package: RSQLite
## Loading required package: DBI
## Loading required package: RSQLite.extfuns
#+END_EXAMPLE

#+BEGIN_SRC R
# connect to the "hflights" table in that database
flights_tbl <- tbl(my_db, "hflights")

# example query with our data frame
flights %>%
    select(UniqueCarrier, DepDelay) %>%
    arrange(desc(DepDelay))
#+END_SRC

#+BEGIN_EXAMPLE
## Source: local data frame [227,496 x 2]
## 
##    UniqueCarrier DepDelay
## 1             CO      981
## 2             AA      970
## 3             MQ      931
## 4             UA      869
## 5             MQ      814
## 6             MQ      803
## 7             CO      780
## 8             CO      758
## 9             DL      730
## 10            MQ      691
## ..           ...      ...
#+END_EXAMPLE

#+BEGIN_SRC R
# identical query using the database
flights_tbl %>%
    select(UniqueCarrier, DepDelay) %>%
    arrange(desc(DepDelay))
#+END_SRC

#+BEGIN_EXAMPLE
## Source: sqlite 3.7.17 [my_db.sqlite3]
## From: hflights [227,496 x 2]
## Arrange: desc(DepDelay) 
## 
##    UniqueCarrier DepDelay
## 1             CO      981
## 2             AA      970
## 3             MQ      931
## 4             UA      869
## 5             MQ      814
## 6             MQ      803
## 7             CO      780
## 8             CO      758
## 9             DL      730
## 10            MQ      691
## ..           ...      ...

#+END_EXAMPLE

You can write the SQL commands yourself
dplyr can tell you the SQL it plans to run and the query execution plan

#+BEGIN_SRC R
# send SQL commands to the database
tbl(my_db, sql("SELECT * FROM hflights LIMIT 100"))
#+END_SRC

#+BEGIN_EXAMPLE
## Source: sqlite 3.7.17 [my_db.sqlite3]
## From: <derived table> [?? x 21]
## 
##    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum
## 1  2011     1          1         6    1400    1500            AA       428
## 2  2011     1          2         7    1401    1501            AA       428
## 3  2011     1          3         1    1352    1502            AA       428
## 4  2011     1          4         2    1403    1513            AA       428
## 5  2011     1          5         3    1405    1507            AA       428
## 6  2011     1          6         4    1359    1503            AA       428
## 7  2011     1          7         5    1359    1509            AA       428
## 8  2011     1          8         6    1355    1454            AA       428
## 9  2011     1          9         7    1443    1554            AA       428
## 10 2011     1         10         1    1443    1553            AA       428
## ..  ...   ...        ...       ...     ...     ...           ...       ...
## Variables not shown: TailNum (chr), ActualElapsedTime (int), AirTime
##   (int), ArrDelay (int), DepDelay (int), Origin (chr), Dest (chr),
##   Distance (int), TaxiIn (int), TaxiOut (int), Cancelled (int),
##   CancellationCode (chr), Diverted (int)
#+END_EXAMPLE

#+BEGIN_SRC R
# ask dplyr for the SQL commands
flights_tbl %>%
    select(UniqueCarrier, DepDelay) %>%
    arrange(desc(DepDelay)) %>%
    explain()
#+END_SRC

#+BEGIN_EXAMPLE
## <SQL>
## SELECT "UniqueCarrier" AS "UniqueCarrier", "DepDelay" AS "DepDelay"
## FROM "hflights"
## ORDER BY "DepDelay" DESC
## 
## <PLAN>
##   selectid order from                             detail
## 1        0     0    0 SCAN TABLE hflights (~227496 rows)
## 2        0     0    0       USE TEMP B-TREE FOR ORDER BY
#+END_EXAMPLE
** Joins
*** Type of Joins
The package offers four different joins:

inner_join (similar to merge with all.x=F and all.y=F)
left_join (similar to merge with all.x=T and all.y=F)
semi_join (not really an equivalent in merge() unless y only includes join fields)
anti_join (no equivalent in merge(), this is all x without a match in y)

#+BEGIN_SRC R
#inner
inner_join(df1, df2)

#left outer
left_join(df1, df2)

#right outer (just reverse argument order)
left_join(df2, df1)
#+END_SRC

*** Join examples
**** left join by 2 key variables
#+BEGIN_SRC R
left_join(test_data, kantrowitz, by = c("first_name" = "name"))
#+END_SRC
    
* Testing grounds 
** create horizontal column exposure days per case
# export from 60 days before a period of 390 days (for Leon for P50)
#+BEGIN_SRC R
setkey(gestlong, folio, day)
gestlong390 <- gestlong[, .SD[1:390,list(day, bestpred, dayindex = paste0("dayindex", sprintf("%0.3i", .I)))],by=folio]
gestwide <-  dcast.data.table(gestlong390[, list(folio, dayindex, bestpred)],folio~dayindex)
#by folio pull out a subset follio take out first 390 days of that
gestwide <- merge(gestlong390[unique(gestlong390[, list(folio)]), list(folio, startday = day), mult = "first"],
                  gestwide, by = "folio")
gestwide[1,1:10, with = F]
#+END_SRC



 
